{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debiasLerp(a, b, p, nBatches):\n",
    "    return torch.lerp(a, b/torch.tensor(p).pow(nBatches), p)\n",
    "    \n",
    "class BatchNorm(HelpfulModule):\n",
    "    def __init__(self, inputSize, mom=0.1, eps=0.01):\n",
    "        super().__init__()\n",
    "        self.mom, self.eps = mom, eps\n",
    "        self.inputSize = inputSize\n",
    "        self.multiplicitiveWeight = nn.Parameter(torch.ones([inputSize]))\n",
    "        self.additiveWeight = nn.Parameter(torch.zeros([inputSize]))\n",
    "        self.register_buffer('running_mean', torch.zeros(inputSize))\n",
    "        self.register_buffer('running_sum_of_squares', torch.zeros(inputSize))\n",
    "        self.register_buffer('running_squared_sum', torch.zeros(inputSize))\n",
    "        self.nBatches = 1\n",
    "    \n",
    "    def resetParams(self):\n",
    "        self.running_mean.zero_()\n",
    "        self.running_sum_of_squares.zero_()\n",
    "        self.running_squared_sum.zero_()\n",
    "        self.nBatches = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.nBatches += 1\n",
    "        self.running_mean = debiasLerp(self.running_mean, x.mean(axis=0), self.mom, self.nBatches)\n",
    "        self.running_mean_of_squares = debiasLerp(self.running_mean, x.pow(2).mean(axis=0), self.mom, self.nBatches)\n",
    "        self.running_squared_mean = debiasLerp(self.running_mean, x.mean(axis=0).pow(2), self.mom, self.nBatches)\n",
    "        mu = self.running_mean\n",
    "        var = torch.max(self.running_mean_of_squares - self.running_squared_mean, torch.tensor(self.eps))\n",
    "        normalizedOutput = (x-mu)/var\n",
    "        return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
    "    \n",
    "class MultiHeadSelfAttentionOld(HelpfulModule):\n",
    "    def __init__(self, n, d, k):\n",
    "        super().__init__()\n",
    "        self.n, self.d, self.k = n,d,k\n",
    "        # Todo: compute initialization scaling factors\n",
    "        # TODO: What about more things than just QKV? Like four or five or something\n",
    "        self.Q = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.K = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.V = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.Wch = nn.Parameter(torch.normal(0, 1, [d, k]))\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "    def forward(self, x):\n",
    "        q = torch.einsum(\"kd,bnd->bnk\", self.Q, x)\n",
    "        k = torch.einsum(\"kd,bnd->bnk\", self.K, x)\n",
    "        v = torch.einsum(\"kd,bnd->bnk\", self.V, x)\n",
    "        dotQueryKey = torch.einsum(\"bij, bkj->bik\", q, k)/math.sqrt(k)\n",
    "        queryPrs = self.softmax(dotQueryKey)\n",
    "        summedRows = torch.einsum(\"bij,bjk->bik\", queryPrs, vh)\n",
    "        return torch.einsum(\"dk,bnk->bnd\", self.Wch, summedRows)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "class VerboseMultiHeadSelfAttentionModuleOld(HelpfulModule):\n",
    "    def __init__(self, n, d, k):\n",
    "        super().__init__()\n",
    "        self.n, self.d, self.k = n,d,k\n",
    "        self.Q = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.K = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.V = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.Wch = nn.Parameter(torch.normal(0, 1, [d, k]))\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "    def forward(self, x):\n",
    "        # x is [b,n,d]\n",
    "        # Q, K, and V are [k,d]\n",
    "        # Q*x[i] is [k,d]*[d] = [k]\n",
    "        # so Q*x should be of size [k,d]*[b,n,d] -> [b,n,k]\n",
    "        q = torch.einsum(\"kd,bnd->bnk\", self.Q, x) # You can test this works by doing Q@x[0,0] and seeing first row is the same row of outputs\n",
    "        k = torch.einsum(\"kd,bnd->bnk\", self.K, x)\n",
    "        v = torch.einsum(\"kd,bnd->bnk\", self.V, x)\n",
    "        \n",
    "        # q[i,j] is a vector of size k\n",
    "        # k[i,j] is a vector of size k\n",
    "        # for every pair of (vector from q, vector from k), we need to get an output by taking their dot product\n",
    "        # normally if you have two matrices A and B of size NxM and MxK,\n",
    "        # when you multiply them, you can think of the output matrix's value in the (i,j)th position as the ith row in A dot jth column in B\n",
    "        # (thus it is every pair of row from first and column from second)\n",
    "        # in einsum, torch.einsum(\"ij,jk->ik\", A, B)\n",
    "        # If instead A and B are of size NxM and NxM and you want to do every pair of rows, you can just do\n",
    "        # torch.einsum(\"ij,kj->ik\") # transpose second matrices indices so it takes rows instead of columns\n",
    "        # we have an additional batch index at the front, so include that\n",
    "        dotQueryKey = torch.einsum(\"bij, bkj->bik\", q, k)/math.sqrt(k)\n",
    "        # now the [b,i,j]th element of dotQueryKey is the dot product of q[b,i] and k[b,j].\n",
    "        # since q and k were of dimension [b,n,k], this becomes [b,n,n]\n",
    "        \n",
    "        # If we look at dotQueryKey[b,i], that is a vector of size n.\n",
    "        # the jth term is the ith query dot the jth key vector.\n",
    "        # thus, each term is \"how much\" the ith query aligns with the jth key.\n",
    "        # so we will take a softmax so we actually get probabilities (TODO: try sigmoid. Seems odd that it's only one class of things)\n",
    "        # This doesn't change the dim, so it's still [b,n,n]\n",
    "        queryPrs = self.softmax(dotQueryKey)\n",
    "        \n",
    "        # the ith output is taking sum over j of (queryPrs[b,i,j])*(vh[b,j])\n",
    "        #                                         scalar            vector\n",
    "        # queryPrs is [b,n,n]\n",
    "        # v        is [b,n,k]\n",
    "        # so         queryPrs[b,i] is of dim n\n",
    "        #            v[b]          is of dim [n,k]\n",
    "        # so         j ranges from 0 to n-1\n",
    "        # fixing b and i and thinking of this as a small matrix, we do\n",
    "        # queryPrs = [0.4, 0.6] (n=2 in this example)\n",
    "        #              \n",
    "        # v        = [1.2,     3.4,     5.2] (each row is of length k, k=3 in this example)\n",
    "        #          = [3.4,     2.3,     1.1] (there are n=2 rows)\n",
    "        # we do\n",
    "        #            [0.4*1.2, 0.4*3.4, 0.4*5.2 ]\n",
    "        #            [0.6*3.4, 0.6*2.3, 0.6*1.1 ]\n",
    "        # and then we sum them:\n",
    "        #            [0.4*1.2+0.6*3.4, 0.4*3.4+0.6*2.3, 0.4*5.2+0.6*1.1]\n",
    "        # In other words, for a given i we dot the ith row of queryPrs[b] (dim n) by each column in v[b] (v[b] is [n,k], so each column is dim n)\n",
    "        # Thus, the output's [b,i,j] value is the ith row of queryPrs[b] dot the jth column of v[b] \n",
    "        # for regular matrix multiplication of A and B, the [i,j]th value is ith row of A dot jth column of B, so this is just regular matrix multiplication.\n",
    "        # In einsum: torch.einsum(\"bij,bjk->bik\")\n",
    "        # which means that our output[b,i] is [b,n,n] x [b,n,k] -> [b,n,k]\n",
    "        summedRows = torch.einsum(\"bij,bjk->bik\", queryPrs, vh)\n",
    "        \n",
    "        # now we need to project back to a [b,n,d]\n",
    "        # W[h,c] is a [d,k] dimensional matrix\n",
    "        # summedRows is [b,n,k]\n",
    "        # we want to do [d,k]x[b,n,k] -> [b,n,d]\n",
    "        res = torch.einsum(\"dk,bnk->bnd\", self.Wch, summedRows)\n",
    "        return res\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
