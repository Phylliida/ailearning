{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "from minGPT.mingpt import model\n",
    "# make deterministic\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(42)\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automataBattle\n",
    "from importlib import reload\n",
    "reload(automataBattle)\n",
    "from torch.utils.data import Dataset\n",
    "class FastLearnAutomataDataset(Dataset):\n",
    "    def __init__(self, nStates, nSymbols, split, sequenceLen, numSequences):\n",
    "        self.nStates = nStates\n",
    "        self.nSymbols = nSymbols\n",
    "        self.split = split # train/test\n",
    "        self.vocab_size = nSymbols*nSymbols\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = sequenceLen\n",
    "        \n",
    "        self.sequenceLen, self.numSequences = sequenceLen, numSequences\n",
    "        \n",
    "        '''\n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "        '''\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.numSequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        a = automataBattle.Automata(nStates=self.nStates, symbols=range(self.nSymbols), randomConnect=True)\n",
    "        a.minimize()\n",
    "        while a.complexity() != self.nStates:\n",
    "            a = automataBattle.Automata(nStates=self.nStates, symbols=range(self.nSymbols), randomConnect=True)\n",
    "            a.minimize()\n",
    "        X, Y = a.generate(self.sequenceLen, lambda: random.choice(range(self.nSymbols)))\n",
    "        x = torch.tensor(X)\n",
    "        y = torch.tensor(Y) # predict the output of the Automata\n",
    "        previous = y[:-1]\n",
    "        shiftedForwadInputsOne = x[1:]\n",
    "        outputs = y[1:] # Todo: look into encoding multiple things (\"tuple encodings\") instead of this gross thing\n",
    "        xOutput = shiftedForwadInputsOne+previous*self.nSymbols\n",
    "        yOutput = outputs\n",
    "        return xOutput, yOutput\n",
    "        \n",
    "        '''\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55c2cd5e7f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastLearnAutomataDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnStates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnSymbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequenceLen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import minGPT\n",
    "from importlib import reload\n",
    "from minGPT.mingpt import trainer\n",
    "from minGPT.mingpt import model\n",
    "reload(minGPT.mingpt.model)\n",
    "reload(minGPT.mingpt.trainer)\n",
    "from minGPT.mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "import gc\n",
    "model = None\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "train_dataset = FastLearnAutomataDataset(nStates=2, nSymbols=2, split='train', sequenceLen=100, numSequences=6000000)\n",
    "test_dataset = FastLearnAutomataDataset(nStates=2, nSymbols=2, split='test', sequenceLen=100, numSequences=2000)\n",
    "print(train_dataset[0], train_dataset[1])\n",
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=8, n_head=8, n_embd=64)\n",
    "model = GPT(mconf)\n",
    "#model.load_state_dict(torch.load(\"juniper_fit_actual_4_states_2\"))\n",
    "from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "set_seed(27)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 11718: train loss 0.20796. lr 1.554741e-05: 100%|██████████| 11719/11719 [31:01<00:00,  6.30it/s]\n",
      "10/20/2020 23:41:15 - INFO - minGPT.mingpt.trainer -   test loss: 0.194861\n",
      "epoch 2 iter 11718: train loss 0.20348. lr 1.392521e-05: 100%|██████████| 11719/11719 [31:02<00:00,  6.29it/s]\n",
      "10/21/2020 00:12:19 - INFO - minGPT.mingpt.trainer -   test loss: 0.193911\n",
      "epoch 3 iter 10580: train loss 0.19852. lr 5.895954e-05:  90%|█████████ | 10581/11719 [28:00<03:00,  6.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-61f9718c874f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       num_workers=16)\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/openai/openai_learning/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openai/openai_learning/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=100, batch_size=512, learning_rate=6e-5,\n",
    "                      lr_decay=True, warmup_tokens=512, final_tokens=50*len(train_dataset)*(2+1),\n",
    "                      num_workers=16)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"juniper_128\"\n",
    "raw_model = model.module if hasattr(model, \"module\") else model\n",
    "torch.save(raw_model.state_dict(), ckpt_path)\n",
    "\n",
    "# seems 8layer, 8head, embed32 got stuck at around 0.5, but it's possible it could have gone further\n",
    "# juniper_fit fit really well, n_layer=8, n_head=8, n_embd=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "not pre-fit to 2, n_layer=8, n_head=8, n_embd=64\n",
    "epoch 1 iter 2343: train loss 0.49225. lr 1.445926e-05: 100%|██████████| 2344/2344 [13:45<00:00,  2.84it/s]\n",
    "10/20/2020 13:35:34 - INFO - minGPT.mingpt.trainer -   test loss: 0.498138\n",
    "epoch 2 iter 2343: train loss 0.49982. lr 1.610073e-05: 100%|██████████| 2344/2344 [13:42<00:00,  2.85it/s]\n",
    "10/20/2020 13:49:17 - INFO - minGPT.mingpt.trainer -   test loss: 0.473743\n",
    "epoch 3 iter 2343: train loss 0.47190. lr 5.994085e-05: 100%|██████████| 2344/2344 [13:41<00:00,  2.85it/s]\n",
    "10/20/2020 14:02:58 - INFO - minGPT.mingpt.trainer -   test loss: 0.592071\n",
    "epoch 4 iter 2343: train loss 0.49541. lr 1.287954e-05: 100%|██████████| 2344/2344 [13:42<00:00,  2.85it/s]\n",
    "10/20/2020 14:16:41 - INFO - minGPT.mingpt.trainer -   test loss: 0.498718\n",
    "epoch 5 iter 2343: train loss 0.47400. lr 1.779652e-05: 100%|██████████| 2344/2344 [13:40<00:00,  2.86it/s]\n",
    "10/20/2020 14:30:22 - INFO - minGPT.mingpt.trainer -   test loss: 0.514086\n",
    "epoch 6 iter 2343: train loss 0.47976. lr 5.976367e-05: 100%|██████████| 2344/2344 [13:41<00:00,  2.85it/s]\n",
    "10/20/2020 14:44:03 - INFO - minGPT.mingpt.trainer -   test loss: 0.396569\n",
    "epoch 7 iter 2343: train loss 0.48898. lr 1.136731e-05: 100%|██████████| 2344/2344 [13:42<00:00,  2.85it/s]\n",
    "10/20/2020 14:57:45 - INFO - minGPT.mingpt.trainer -   test loss: 0.506740\n",
    "epoch 8 iter 2343: train loss 0.47964. lr 1.954042e-05: 100%|██████████| 2344/2344 [13:44<00:00,  2.84it/s]\n",
    "10/20/2020 15:11:30 - INFO - minGPT.mingpt.trainer -   test loss: 0.464614\n",
    "epoch 9 iter 2343: train loss 0.47039. lr 5.946917e-05: 100%|██████████| 2344/2344 [13:40<00:00,  2.86it/s]\n",
    "10/20/2020 15:25:11 - INFO - minGPT.mingpt.trainer -   test loss: 0.503165\n",
    "epoch 10 iter 2343: train loss 0.46504. lr 9.928526e-06: 100%|██████████| 2344/2344 [13:42<00:00,  2.85it/s]\n",
    "10/20/2020 15:38:54 - INFO - minGPT.mingpt.trainer -   test loss: 0.479228\n",
    "epoch 11 iter 2343: train loss 0.46503. lr 2.132556e-05: 100%|██████████| 2344/2344 [13:42<00:00,  2.85it/s]\n",
    "10/20/2020 15:52:37 - INFO - minGPT.mingpt.trainer -   test loss: 0.493589\n",
    "epoch 12 iter 2343: train loss 0.44574. lr 5.905849e-05: 100%|██████████| 2344/2344 [13:43<00:00,  2.85it/s]\n",
    "10/20/2020 16:06:20 - INFO - minGPT.mingpt.trainer -   test loss: 0.436838\n",
    "epoch 13 iter 2343: train loss 0.44981. lr 8.568867e-06: 100%|██████████| 2344/2344 [13:41<00:00,  2.85it/s]\n",
    "10/20/2020 16:20:01 - INFO - minGPT.mingpt.trainer -   test loss: 0.445516\n",
    "epoch 14 iter 2343: train loss 0.44436. lr 2.314489e-05: 100%|██████████| 2344/2344 [13:40<00:00,  2.86it/s]\n",
    "10/20/2020 16:33:42 - INFO - minGPT.mingpt.trainer -   test loss: 0.409083\n",
    "epoch 15 iter 2343: train loss 0.45118. lr 5.853326e-05: 100%|██████████| 2344/2344 [13:41<00:00,  2.85it/s]\n",
    "10/20/2020 16:47:24 - INFO - minGPT.mingpt.trainer -   test loss: 0.320424\n",
    "epoch 16 iter 2343: train loss 0.44524. lr 7.293692e-06: 100%|██████████| 2344/2344 [13:41<00:00,  2.85it/s]\n",
    "10/20/2020 17:01:05 - INFO - minGPT.mingpt.trainer -   test loss: 0.412357\n",
    "epoch 17 iter 2343: train loss 0.44509. lr 2.499124e-05: 100%|██████████| 2344/2344 [13:42<00:00,  2.85it/s]\n",
    "10/20/2020 17:14:48 - INFO - minGPT.mingpt.trainer -   test loss: 0.498460\n",
    "epoch 18 iter 656: train loss 0.43165. lr 6.000000e-06:  28%|██▊       | 657/2344 [03:50<09:52,  2.85it/s]\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
