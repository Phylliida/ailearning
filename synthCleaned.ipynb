{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "from minGPT.mingpt import model\n",
    "# make deterministic\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(42)\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automataBattle\n",
    "from importlib import reload\n",
    "reload(automataBattle)\n",
    "from torch.utils.data import Dataset\n",
    "class FastLearnAutomataDataset(Dataset):\n",
    "    def __init__(self, nStates, nSymbols, split, sequenceLen, numSequences):\n",
    "        self.nStates = nStates\n",
    "        self.nSymbols = nSymbols\n",
    "        self.split = split # train/test\n",
    "        self.vocab_size = nSymbols*nSymbols\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = sequenceLen\n",
    "        \n",
    "        self.sequenceLen, self.numSequences = sequenceLen, numSequences\n",
    "        \n",
    "        '''\n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "        '''\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.numSequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        a = automataBattle.Automata(nStates=self.nStates, symbols=range(self.nSymbols), randomConnect=True)\n",
    "        a.minimize()\n",
    "        while a.complexity() != self.nStates:\n",
    "            a = automataBattle.Automata(nStates=self.nStates, symbols=range(self.nSymbols), randomConnect=True)\n",
    "            a.minimize()\n",
    "        X, Y = a.generate(self.sequenceLen, lambda: random.choice(range(self.nSymbols)))\n",
    "        x = torch.tensor(X)\n",
    "        y = torch.tensor(Y) # predict the output of the Automata\n",
    "        previous = y[:-1]\n",
    "        shiftedForwadInputsOne = x[1:]\n",
    "        outputs = y[1:] # Todo: look into encoding multiple things (\"tuple encodings\") instead of this gross thing\n",
    "        xOutput = shiftedForwadInputsOne+previous*self.nSymbols\n",
    "        yOutput = outputs\n",
    "        return xOutput, yOutput\n",
    "        \n",
    "        '''\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/20/2020 01:51:10 - INFO - minGPT.mingpt.model -   number of parameters: 1.613056e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 2, 2, 2, 3, 2, 2, 3, 3, 3, 1, 3, 1, 3, 1, 2, 0, 3, 3, 3, 0, 2, 2, 2,\n",
      "        2, 3, 3, 2, 1, 2, 2, 3, 3, 2, 0, 3, 3, 3, 0, 2, 2, 2, 2, 3, 3, 3, 0, 2,\n",
      "        2, 3, 3, 2, 1, 3, 3, 0, 3, 3, 3, 1, 2, 0, 2, 2, 3, 3, 2, 0, 2, 2, 2, 2,\n",
      "        3, 3, 3, 1, 2, 1, 2, 2, 2, 3, 3, 2, 1, 3, 3, 1, 2, 1, 2, 3, 3, 3, 0, 3,\n",
      "        3, 2, 1, 3, 3, 1, 3, 0, 2, 2, 3, 3, 2, 1, 3, 3, 1, 3, 1, 2, 1, 3, 3, 0,\n",
      "        2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,\n",
      "        2, 2, 3, 3, 2, 1, 3, 3, 1, 2, 1, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2,\n",
      "        2, 3, 3, 2, 0, 2, 3, 2, 3, 3, 3, 1, 2, 0, 2, 3, 2, 2, 3, 2, 3, 2, 2, 3,\n",
      "        3, 3, 1, 3, 0, 3, 3]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1])) (tensor([3, 3, 1, 3, 3, 1, 3, 2, 0, 3, 2, 1, 2, 0, 3, 3, 1, 2, 2, 2, 3, 3, 0, 2,\n",
      "        3, 3, 2, 0, 3, 3, 1, 2, 3, 2, 0, 2, 2, 2, 2, 3, 3, 1, 3, 3, 0, 2, 2, 3,\n",
      "        2, 3, 3, 0, 3, 2, 2, 2, 3, 3, 1, 3, 3, 0, 3, 3, 3, 1, 3, 2, 1, 2, 0, 3,\n",
      "        3, 1, 3, 2, 1, 2, 0, 3, 2, 0, 2, 3, 3, 1, 3, 3, 0, 3, 2, 2, 2, 3, 2, 1,\n",
      "        3, 1, 2, 3, 2, 1, 2, 1, 3, 0, 2, 3, 2, 2, 3, 3, 0, 2, 3, 3, 2, 1, 2, 1,\n",
      "        2, 0, 3, 2, 0, 2, 2, 2, 2, 3, 2, 1, 3, 1, 2, 2, 3, 2, 1, 2, 1, 3, 0, 2,\n",
      "        2, 3, 2, 2, 2, 3, 2, 0, 3, 3, 1, 3, 2, 1, 3, 0, 2, 3, 3, 3, 1, 3, 3, 1,\n",
      "        3, 2, 1, 2, 1, 2, 0, 3, 2, 1, 3, 0, 2, 2, 2, 3, 2, 2, 2, 3, 2, 0, 2, 3,\n",
      "        2, 1, 2, 0, 3, 3, 0]), tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import minGPT\n",
    "from importlib import reload\n",
    "from minGPT.mingpt import trainer\n",
    "from minGPT.mingpt import model\n",
    "reload(minGPT.mingpt.model)\n",
    "reload(minGPT.mingpt.trainer)\n",
    "from minGPT.mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "import gc\n",
    "model = None\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "train_dataset = FastLearnAutomataDataset(nStates=3, nSymbols=2, split='train', sequenceLen=200, numSequences=600000)\n",
    "test_dataset = FastLearnAutomataDataset(nStates=3, nSymbols=2, split='test', sequenceLen=200, numSequences=10)\n",
    "print(train_dataset[0], train_dataset[1])\n",
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=8, n_head=8, n_embd=128)\n",
    "model = GPT(mconf)\n",
    "from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "set_seed(27)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 2343: train loss 0.48253. lr 1.445926e-05: 100%|██████████| 2344/2344 [15:18<00:00,  2.55it/s]\n",
      "10/20/2020 02:06:29 - INFO - minGPT.mingpt.trainer -   test loss: 0.493038\n",
      "epoch 2 iter 2343: train loss 0.49297. lr 1.610073e-05: 100%|██████████| 2344/2344 [15:18<00:00,  2.55it/s]\n",
      "10/20/2020 02:21:48 - INFO - minGPT.mingpt.trainer -   test loss: 0.465012\n",
      "epoch 3 iter 774: train loss 0.48079. lr 6.000000e-06:  33%|███▎      | 775/2344 [05:03<10:14,  2.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-029b471ac0d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       num_workers=0)\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/openai/openai_learning/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openai/openai_learning/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=100, batch_size=256, learning_rate=6e-5,\n",
    "                      lr_decay=True, warmup_tokens=256, final_tokens=50*len(train_dataset)*(2+1),\n",
    "                      num_workers=0)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"juniper_fit_128_3_states\"\n",
    "raw_model = model.module if hasattr(model, \"module\") else model\n",
    "torch.save(raw_model.state_dict(), ckpt_path)\n",
    "\n",
    "# seems 8layer, 8head, embed32 got stuck at around 0.5, but it's possible it could have gone further\n",
    "# juniper_fit fit really well, n_layer=8, n_head=8, n_embd=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
