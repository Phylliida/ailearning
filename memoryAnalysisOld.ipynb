{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Legg, simple MLP\n",
    "class MLPLazy(nn.Module):\n",
    "    def __init__(self, nx, hidden_layer_dims, ny, device):\n",
    "        super(MLPLazy, self).__init__()\n",
    "        self.hidden_layer_dims = hidden_layer_dims\n",
    "        linear_layers = []\n",
    "        last_dim = nx\n",
    "        for next_dim in hidden_layer_dims:\n",
    "            linear_layer = nn.Linear(last_dim, next_dim).to(device)\n",
    "            linear_layers.append(linear_layer)\n",
    "            last_dim = next_dim\n",
    "        # should push to ModuleList so that params stay on cuda\n",
    "        self.linear_layers = nn.ModuleList(linear_layers)\n",
    "        \n",
    "        self.scorer = nn.Linear(last_dim, ny).to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X has shape (m, nx)\n",
    "        '''\n",
    "        last_X = X\n",
    "        for i, linear_layer in enumerate(self.linear_layers):\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = linear_layer(last_X)\n",
    "            # shape (m, self.hidden_layer_dims[i])\n",
    "            last_X = torch.relu(last_X)\n",
    "        # shape (m, ny)\n",
    "\n",
    "        z = self.scorer(last_X)\n",
    "        # shape (m, ny)\n",
    "        a = torch.softmax(z, dim=1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.profiler import FunctionEventAvg, EventList\n",
    "def key_averages(eventArr, group_by_input_shapes=False, stackKey=None, stackParse=None):\n",
    "    \"\"\"Averages all function events over their keys.\n",
    "    Arguments:\n",
    "        group_by_input_shapes: group entries by\n",
    "        (event name, input shapes) rather than just event name.\n",
    "        This is useful to see which input shapes contribute to the runtime\n",
    "        the most and may help with size-specific optimizations or\n",
    "        choosing the best candidates for quantization (aka fitting a roof line)\n",
    "        group_by_stack_n: group by top n stack trace entries\n",
    "    Returns:\n",
    "        An EventList containing FunctionEventAvg objects.\n",
    "    \"\"\"\n",
    "    eventArr.populate_cpu_children()\n",
    "    stats: Dict[Tuple[int, Tuple[int, int]], FunctionEventAvg] = defaultdict(FunctionEventAvg)\n",
    "\n",
    "    def get_key(event, group_by_input_shapes, stackKey):\n",
    "        key = [str(event.key), str(event.node_id)]\n",
    "        if group_by_input_shapes:\n",
    "            key.append(str(event.input_shapes))\n",
    "        if stackKey is not None:\n",
    "            stackKeys = [stackKey(x) for x in event.stack if stackKey(x) is not None]\n",
    "            if len(stackKeys) > 0:\n",
    "                key += \"\\n\".join(stackKeys)\n",
    "        return tuple(key)\n",
    "    for evt in eventArr:\n",
    "        stats[get_key(evt, group_by_input_shapes, stackKey)].add(evt)\n",
    "    \n",
    "    avg_list = EventList(stats.values(), use_cuda=eventArr._use_cuda, profile_memory=eventArr._profile_memory)\n",
    "    \n",
    "    for evt in avg_list:\n",
    "        if not group_by_input_shapes:\n",
    "            evt.input_shapes = \"\"\n",
    "        if stackParse is not None:\n",
    "            evt.stack = [stackParse(x) for x in evt.stack if stackParse(x) is not None]\n",
    "    return avg_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "device = \"cuda:0\"\n",
    "config = TransformerConfig(numHeads=2, vocabSize=128, embeddingDim=256, posEmbeddingDim=256, keyDim=512, valueDim=512, hiddenSize=512, numLayers=8, seqLen=100)\n",
    "model = Transformer(config)\n",
    "x = torch.randint(0, 127, [1, 100])\n",
    "desired = torch.nn.Softmax(dim=2)(torch.normal(0, 1, [1, 100, 128]))\n",
    "with profiler.profile(profile_memory=True, record_shapes=True, use_cuda=False, with_stack=True) as prof:\n",
    "    for i in range(10):\n",
    "        with profiler.record_function(\"forward\"):\n",
    "            y, loss = model(x, desired)\n",
    "        with profiler.record_function(\"backward\"):\n",
    "            loss.backward()\n",
    "\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::empty: Self CPU time: 204364.46200000675 CPU Time: 204364.46200000675 CPU Memory Usage: 875.01 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::resize_: Self CPU time: 49074.98000000351 CPU Time: 49074.98000000351 CPU Memory Usage: 570.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::bmm: Self CPU time: 467389.50500001595 CPU Time: 904110.946999992 CPU Memory Usage: 570.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "forward: Self CPU time: 91100.56800000387 CPU Time: 3466738.5030000005 CPU Memory Usage: 521.99 Mb\n",
      "BmmBackward0: Self CPU time: 4026.542000013578 CPU Time: 652321.326000008 CPU Memory Usage: 445.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::mul: Self CPU time: 154881.08399999602 CPU Time: 253820.27100000664 CPU Memory Usage: 375.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(140)                             return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div: Self CPU time: 126417.32299999968 CPU Time: 239723.62200000137 CPU Memory Usage: 375.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::resize_: Self CPU time: 23703.947000005937 CPU Time: 23703.947000005937 CPU Memory Usage: 285.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::bmm: Self CPU time: 243623.05300000068 CPU Time: 457522.7360000026 CPU Memory Usage: 285.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 115645.63899997104 CPU Time: 115645.63899997104 CPU Memory Usage: 282.70 Mb\n"
     ]
    }
   ],
   "source": [
    "import modelInspector\n",
    "reload(modelInspector)\n",
    "from modelInspector import inspectModel, displayInspect\n",
    "print(displayInspect(inspectModel(model, prof), sort_key=lambda x: x.cpu_memory_usage, row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "# For every line in the forward function of the given module, this returns something that looks like\n",
    "# ('pathToForwardFile.py(206): forward', 206, 'embeddings = torch.cat([embs, posEmbs], axis=3)')\n",
    "def getForwardPaths(module):\n",
    "    lines, lineNum = inspect.getsourcelines(module.forward)\n",
    "    filePath = inspect.getsourcefile(module.forward)\n",
    "    for i, line in enumerate(lines):\n",
    "        yield f\"{filePath}({lineNum+i}): forward\", (lineNum+i), line\n",
    "\n",
    "# returns a dict that can take a path string that looks like \n",
    "# /home/azureuser/openai_learning/customTransformer.py(206): forward\n",
    "# and returns (module name of that forward function, line number, the code on that line)\n",
    "def makePathMapping(model):\n",
    "    mapping = {}\n",
    "    for mn, m in model.named_modules():\n",
    "        # add model. to front so we don't have empty string for model\n",
    "        if mn == \"\": mn = \"model\"\n",
    "        else: mn = \"model.\" + mn\n",
    "        for forwardPath, lineNum, line in getForwardPaths(m):\n",
    "            mapping[forwardPath] = (mn, lineNum, line)\n",
    "    return mapping\n",
    "\n",
    "# returns a dict that can take a path string that looks like \n",
    "# /home/azureuser/openai_learning/customTransformer.py(206): forward\n",
    "# and returns module name of that forward function\n",
    "def makePathModuleMapping(model):\n",
    "    mapping = {}\n",
    "    for mn, m in model.named_modules():\n",
    "        for forwardPath, lineNum, line in getForwardPaths(m):\n",
    "            mapping[forwardPath] = mn\n",
    "    return mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mappingToCodeLine = makePathMapping(model)\n",
    "mappingToModule = makePathModuleMapping(model)\n",
    "\n",
    "\n",
    "def stackToModule(stackStr):\n",
    "    if stackStr in mappingToModule:\n",
    "        return mappingToModule[stackStr]\n",
    "    else:\n",
    "        return None\n",
    "def stackToLine(stackStr):\n",
    "    if stackStr in mappingToCodeLine:\n",
    "        return mappingToCodeLine[stackStr]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "averages = key_averages(prof.function_events, stackKey=stackToModule, stackParse=stackToLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::empty: Self CPU time: 237243.8650000534 CPU Time: 237243.8650000534 CPU Memory Usage: 875.01 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::resize_: Self CPU time: 53733.82899999546 CPU Time: 53733.82899999546 CPU Memory Usage: 570.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::bmm: Self CPU time: 487643.42800003657 CPU Time: 975239.3980000156 CPU Memory Usage: 570.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "forward: Self CPU time: 93920.36399999895 CPU Time: 3701396.3210000005 CPU Memory Usage: 473.17 Mb\n",
      "BmmBackward0: Self CPU time: 3998.135999975493 CPU Time: 723711.7670000035 CPU Memory Usage: 445.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::mul: Self CPU time: 164757.25399997507 CPU Time: 288976.88899999514 CPU Memory Usage: 375.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(140)                             return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div: Self CPU time: 148631.18599998552 CPU Time: 292131.5399999949 CPU Memory Usage: 375.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 137621.75900000668 CPU Time: 137621.75900000668 CPU Memory Usage: 286.61 Mb\n",
      "aten::resize_: Self CPU time: 26542.770000006356 CPU Time: 26542.770000006356 CPU Memory Usage: 285.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::bmm: Self CPU time: 235091.2849999642 CPU Time: 479263.82999999827 CPU Memory Usage: 285.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::add: Self CPU time: 103831.23799999221 CPU Time: 143097.54799999623 CPU Memory Usage: 281.74 Mb\n",
      "aten::empty: Self CPU time: 96588.16600002296 CPU Time: 96588.16600002296 CPU Memory Usage: 281.25 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::neg: Self CPU time: 61218.731999997806 CPU Time: 136913.70000000368 CPU Memory Usage: 250.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "BmmBackward0: Self CPU time: 1803.6220000067842 CPU Time: 359756.22299999645 CPU Memory Usage: 222.50 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 56873.28099999815 CPU Time: 56873.28099999815 CPU Memory Usage: 218.75 Mb\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::einsum: Self CPU time: 35130.45099998574 CPU Time: 1193514.003999996 CPU Memory Usage: 156.25 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::resize_: Self CPU time: 26907.479000004714 CPU Time: 26907.479000004714 CPU Memory Usage: 137.21 Mb\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::bmm: Self CPU time: 115867.9809999579 CPU Time: 355329.6719999988 CPU Memory Usage: 137.21 Mb\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 39685.204000005026 CPU Time: 39685.204000005026 CPU Memory Usage: 125.00 Mb\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::resize_: Self CPU time: 35345.46300000922 CPU Time: 35345.46300000922 CPU Memory Usage: 125.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(138)                             var = x.var((1,2,3), keepdim=True) # TODO: add correction based on batch size\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div: Self CPU time: 44008.53399999405 CPU Time: 113527.64500000211 CPU Memory Usage: 125.00 Mb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "DivBackward0: Self CPU time: 2224.4199999990524 CPU Time: 133074.23799999367 CPU Memory Usage: 125.00 Mb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "DivBackward0: Self CPU time: 3523.233999983582 CPU Time: 191818.02500000218 CPU Memory Usage: 125.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "MulBackward0: Self CPU time: 2074.620999998995 CPU Time: 95677.66200000257 CPU Memory Usage: 125.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(140)                             return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 44550.4720000006 CPU Time: 44550.4720000006 CPU Memory Usage: 125.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::sub: Self CPU time: 61415.522999998146 CPU Time: 80311.8090000097 CPU Memory Usage: 125.00 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 25152.85999999276 CPU Time: 25152.85999999276 CPU Memory Usage: 125.00 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_like: Self CPU time: 18157.578999996244 CPU Time: 36612.26499999898 CPU Memory Usage: 125.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::clone: Self CPU time: 35606.361999997025 CPU Time: 105952.3519999977 CPU Memory Usage: 125.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::add: Self CPU time: 55586.45800000444 CPU Time: 77486.3790000028 CPU Memory Usage: 125.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::reshape: Self CPU time: 111515.29000003298 CPU Time: 446956.1690000135 CPU Memory Usage: 125.00 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "BmmBackward0: Self CPU time: 1788.5159999835887 CPU Time: 276120.6689999975 CPU Memory Usage: 99.85 Mb\n",
      "  model.encodingLayers.7.attention:(494)                                  u = torch.einsum(\"binj,bjnk->bink\", queryPrs, v)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 50438.4150000052 CPU Time: 50438.4150000052 CPU Memory Usage: 93.75 Mb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "ViewBackward: Self CPU time: 3428.5299999919953 CPU Time: 278478.6770000008 CPU Memory Usage: 93.75 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::clone: Self CPU time: 27653.485999995115 CPU Time: 59186.10299999738 CPU Memory Usage: 93.75 Mb\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::mul: Self CPU time: 33665.237999995574 CPU Time: 74819.95399999598 CPU Memory Usage: 93.75 Mb\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div: Self CPU time: 23805.837999994692 CPU Time: 57922.58699999715 CPU Memory Usage: 62.50 Mb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "DivBackward0: Self CPU time: 1106.2060000019846 CPU Time: 68133.2859999981 CPU Memory Usage: 62.50 Mb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "MeanBackward1: Self CPU time: 1370.409999995085 CPU Time: 126426.26399999484 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "VarBackward1: Self CPU time: 2767.6350000100792 CPU Time: 318681.4030000064 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(138)                             var = x.var((1,2,3), keepdim=True) # TODO: add correction based on batch size\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "SubBackward0: Self CPU time: 1117.0089999969932 CPU Time: 106648.65999999468 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::add: Self CPU time: 26773.445000001942 CPU Time: 36916.95700000206 CPU Memory Usage: 62.50 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::einsum: Self CPU time: 17078.183000005476 CPU Time: 558980.4200000004 CPU Memory Usage: 62.50 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::add: Self CPU time: 25277.458999993687 CPU Time: 35215.052999996085 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7.layerNorm2:(140)                             return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 9664.901999998707 CPU Time: 9664.901999998707 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::add: Self CPU time: 26909.954999997957 CPU Time: 36574.856999996664 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::mul: Self CPU time: 25049.547999999333 CPU Time: 34664.946999999695 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::dropout: Self CPU time: 6755.268999998028 CPU Time: 98377.48599999733 CPU Memory Usage: 62.50 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 14714.25099999929 CPU Time: 14714.25099999929 CPU Memory Usage: 48.82 Mb\n",
      "torch::autograd::AccumulateGrad: Self CPU time: 5883.360999988974 CPU Time: 194600.76499999076 CPU Memory Usage: 48.82 Mb\n",
      "aten::einsum: Self CPU time: 16562.983999998618 CPU Time: 455332.7779999952 CPU Memory Usage: 37.35 Mb\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::to: Self CPU time: 43601.63699999955 CPU Time: 99130.00100000805 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "MulBackward0: Self CPU time: 563.805999992881 CPU Time: 20585.706000000122 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "MaximumBackward: Self CPU time: 1080.0099999936065 CPU Time: 58902.59499999869 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "MinimumBackward: Self CPU time: 1105.105999997235 CPU Time: 60784.61099999782 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(56)                                   lessThan = torch.min(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "MulBackward0: Self CPU time: 559.7070000035455 CPU Time: 32203.523000001034 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "ClampMaxBackward: Self CPU time: 918.0039999939036 CPU Time: 86296.26299999875 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::clamp_max_: Self CPU time: 11391.911999997377 CPU Time: 49958.60399999626 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::sub: Self CPU time: 13574.540000004978 CPU Time: 33517.73900000251 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::add: Self CPU time: 12536.624999998654 CPU Time: 17437.074999996672 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::minimum: Self CPU time: 13173.130000002086 CPU Time: 18189.28300000154 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(56)                                   lessThan = torch.min(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::min: Self CPU time: 4800.449000002278 CPU Time: 22989.732000003816 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(56)                                   lessThan = torch.min(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::maximum: Self CPU time: 13423.429999999258 CPU Time: 18607.082999999933 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::max: Self CPU time: 4810.547999999741 CPU Time: 23417.630999999674 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_like: Self CPU time: 5415.647000006473 CPU Time: 15775.358000001266 CPU Memory Usage: 31.25 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 18599.686999997408 CPU Time: 18599.686999997408 CPU Memory Usage: 24.41 Mb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::resize_: Self CPU time: 12433.82399999816 CPU Time: 12433.82399999816 CPU Memory Usage: 23.44 Mb\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::gt: Self CPU time: 14687.247000007832 CPU Time: 32535.927999995707 CPU Memory Usage: 15.62 Mb\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::lt: Self CPU time: 16574.36699999607 CPU Time: 36355.27200000128 CPU Memory Usage: 15.62 Mb\n",
      "  model.encodingLayers.7.RELU:(56)                                   lessThan = torch.min(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::le: Self CPU time: 16660.163999996556 CPU Time: 79524.6979999948 CPU Memory Usage: 15.62 Mb\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 5597.0719999998055 CPU Time: 5597.0719999998055 CPU Memory Usage: 15.62 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::empty_like: Self CPU time: 9379.492000005117 CPU Time: 18599.488000002446 CPU Memory Usage: 12.21 Mb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::contiguous: Self CPU time: 14374.334999984814 CPU Time: 45566.858999987475 CPU Memory Usage: 12.21 Mb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div: Self CPU time: 22738.732999997017 CPU Time: 60298.80900000461 CPU Memory Usage: 12.21 Mb\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::resize_: Self CPU time: 1722.8179999992717 CPU Time: 1722.8179999992717 CPU Memory Usage: 9.39 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::bmm: Self CPU time: 7273.175999999628 CPU Time: 22237.92700000404 CPU Memory Usage: 9.39 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "BmmBackward0: Self CPU time: 168.50200000056066 CPU Time: 17687.87899999891 CPU Memory Usage: 8.91 Mb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::mul: Self CPU time: 7878.467999999504 CPU Time: 10087.00000000051 CPU Memory Usage: 7.81 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::dropout: Self CPU time: 1436.8119999959144 CPU Time: 21503.812999999456 CPU Memory Usage: 7.81 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 14053.038000007247 CPU Time: 14053.038000007247 CPU Memory Usage: 6.10 Mb\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "DivBackward0: Self CPU time: 545.8059999975958 CPU Time: 32977.13300000026 CPU Memory Usage: 6.10 Mb\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::_softmax_backward_data: Self CPU time: 11061.210000003513 CPU Time: 38249.182999996934 CPU Memory Usage: 6.10 Mb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "SoftmaxBackward: Self CPU time: 539.7070000033127 CPU Time: 38788.89000000025 CPU Memory Usage: 6.10 Mb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::_softmax: Self CPU time: 14901.851000002607 CPU Time: 42660.42799999674 CPU Memory Usage: 6.10 Mb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::softmax: Self CPU time: 5729.056000003031 CPU Time: 48389.48399999977 CPU Memory Usage: 6.10 Mb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 29803.582000012717 CPU Time: 29803.582000012717 CPU Memory Usage: 6.10 Mb\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty: Self CPU time: 10673.904999992112 CPU Time: 10673.904999992112 CPU Memory Usage: 5.38 Mb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::resize_: Self CPU time: 1048.4120000005696 CPU Time: 1048.4120000005696 CPU Memory Usage: 3.91 Mb\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "MulBackward0: Self CPU time: 148.00399999984074 CPU Time: 6685.5650000000605 CPU Memory Usage: 3.91 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(220)                                                             posEmbs = self.embedDropout(posEmbs)\n",
      "\n",
      "aten::_cat: Self CPU time: 2234.5230000006723 CPU Time: 5289.35299999937 CPU Memory Usage: 3.91 Mb\n",
      "  model:(221)                                                         embeddings = torch.cat([embs, posEmbs], axis=3)\n",
      "\n",
      "aten::cat: Self CPU time: 1187.6120000022675 CPU Time: 6476.965000001637 CPU Memory Usage: 3.91 Mb\n",
      "  model:(221)                                                         embeddings = torch.cat([embs, posEmbs], axis=3)\n",
      "\n",
      "aten::empty_like: Self CPU time: 1185.212000001895 CPU Time: 3404.335000001425 CPU Memory Usage: 3.91 Mb\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::mul: Self CPU time: 7546.776000003156 CPU Time: 14140.14000000112 CPU Memory Usage: 2.93 Mb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::neg: Self CPU time: 5539.452000004239 CPU Time: 12384.219000004174 CPU Memory Usage: 2.93 Mb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::sum: Self CPU time: 172718.4439999952 CPU Time: 716030.4210000044 CPU Memory Usage: 2.90 Mb\n",
      "aten::resize_: Self CPU time: 2783.5310000038007 CPU Time: 2783.5310000038007 CPU Memory Usage: 2.44 Mb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::empty: Self CPU time: 1652.0149999990808 CPU Time: 1652.0149999990808 CPU Memory Usage: 2.23 Mb\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "ExpandBackward: Self CPU time: 73.30099999764934 CPU Time: 5554.157999997493 CPU Memory Usage: 1.95 Mb\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "aten::log: Self CPU time: 4057.940999998187 CPU Time: 9023.48999999999 CPU Memory Usage: 1.95 Mb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::zeros: Self CPU time: 551.9030000002822 CPU Time: 3223.8309999996563 CPU Memory Usage: 1.25 Mb\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "IndexBackward: Self CPU time: 91.20200000039767 CPU Time: 6487.064000001061 CPU Memory Usage: 1.25 Mb\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::div: Self CPU time: 4094.6389999983367 CPU Time: 10160.998999998847 CPU Memory Usage: 1007.81 Kb\n",
      "  model.lossFunc.batchedPrLoss:(124)                                  if rollupLosses: return res.mean()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::zeros: Self CPU time: 3822.8370000023497 CPU Time: 9491.797999998587 CPU Memory Usage: 1000.08 Kb\n",
      "IndexBackward: Self CPU time: 127.50000000104774 CPU Time: 7068.869999998715 CPU Memory Usage: 1000.00 Kb\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "LogBackward: Self CPU time: 83.50000000087311 CPU Time: 3770.7369999993825 CPU Memory Usage: 1000.00 Kb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "MulBackward0: Self CPU time: 140.60100000107195 CPU Time: 4657.546000000497 CPU Memory Usage: 1000.00 Kb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::rsub: Self CPU time: 2987.831000001577 CPU Time: 7809.077999999165 CPU Memory Usage: 1000.00 Kb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::empty: Self CPU time: 1096.4150000000373 CPU Time: 1096.4150000000373 CPU Memory Usage: 1000.00 Kb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::empty: Self CPU time: 2786.1310000037192 CPU Time: 2786.1310000037192 CPU Memory Usage: 1000.00 Kb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::index: Self CPU time: 1474.7159999984524 CPU Time: 3968.439000000195 CPU Memory Usage: 1000.00 Kb\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "aten::index: Self CPU time: 1689.6179999979186 CPU Time: 4556.14600000027 CPU Memory Usage: 1000.00 Kb\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::div: Self CPU time: 1158.0109999991837 CPU Time: 3311.6320000008564 CPU Memory Usage: 500.00 Kb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "DivBackward0: Self CPU time: 83.9999999976717 CPU Time: 3940.7389999989537 CPU Memory Usage: 500.00 Kb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::_softmax_backward_data: Self CPU time: 1249.0100000011735 CPU Time: 2740.428999999247 CPU Memory Usage: 500.00 Kb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "SoftmaxBackward: Self CPU time: 71.0000000015716 CPU Time: 2811.4290000008186 CPU Memory Usage: 500.00 Kb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "RsubBackward1: Self CPU time: 44.20199999748729 CPU Time: 6056.660999998916 CPU Memory Usage: 500.00 Kb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "SubBackward0: Self CPU time: 165.20099999877857 CPU Time: 6567.066000000341 CPU Memory Usage: 500.00 Kb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::sub: Self CPU time: 1306.1099999970756 CPU Time: 1876.2179999983055 CPU Memory Usage: 500.00 Kb\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::_softmax: Self CPU time: 1497.1150000008638 CPU Time: 2581.9269999995595 CPU Memory Usage: 500.00 Kb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::softmax: Self CPU time: 625.7059999992489 CPU Time: 3207.6329999988084 CPU Memory Usage: 500.00 Kb\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::add: Self CPU time: 1448.216000000306 CPU Time: 2028.6210000003339 CPU Memory Usage: 500.00 Kb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::einsum: Self CPU time: 986.609999996319 CPU Time: 28167.684000000765 CPU Memory Usage: 500.00 Kb\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::arange: Self CPU time: 1272.7120000021496 CPU Time: 3393.4330000012596 CPU Memory Usage: 15.62 Kb\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "aten::sum: Self CPU time: 1961.0230000019656 CPU Time: 5234.453999999736 CPU Memory Usage: 3.95 Kb\n",
      "  model.lossFunc.batchedPrLoss:(123)                                  res = vals.sum(axis=2)\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "MeanBackward0: Self CPU time: 96.4010000010021 CPU Time: 6359.361999998975 CPU Memory Usage: 3.91 Kb\n",
      "  model.lossFunc.batchedPrLoss:(124)                                  if rollupLosses: return res.mean()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 51264.422999978604 CPU Time: 51264.422999978604 CPU Memory Usage: 3.75 Kb\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::to: Self CPU time: 73628.31500001933 CPU Time: 160584.97700000866 CPU Memory Usage: 3.12 Kb\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::to: Self CPU time: 39736.89100002847 CPU Time: 116296.06900002825 CPU Memory Usage: 2.50 Kb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 17956.89500000033 CPU Time: 17956.89500000033 CPU Memory Usage: 1.25 Kb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::to: Self CPU time: 19411.185999993526 CPU Time: 56338.75899999965 CPU Memory Usage: 1.25 Kb\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::sum: Self CPU time: 31377.61799996603 CPU Time: 286978.77300000296 CPU Memory Usage: 1.25 Kb\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::mean: Self CPU time: 40325.724000003895 CPU Time: 422627.84800000687 CPU Memory Usage: 1.25 Kb\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::clone: Self CPU time: 17812.581999999413 CPU Time: 35347.25300000224 CPU Memory Usage: 640 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "MaximumBackward: Self CPU time: 2081.6340000155615 CPU Time: 97594.69100000907 CPU Memory Usage: 640 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::maximum: Self CPU time: 21517.210000005063 CPU Time: 31267.311999999154 CPU Memory Usage: 640 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::max: Self CPU time: 9607.112000000052 CPU Time: 40874.423999999206 CPU Memory Usage: 640 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::var: Self CPU time: 63482.139999985935 CPU Time: 83156.73899999686 CPU Memory Usage: 640 b\n",
      "  model.encodingLayers.7.layerNorm2:(138)                             var = x.var((1,2,3), keepdim=True) # TODO: add correction based on batch size\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::to: Self CPU time: 9552.19900000247 CPU Time: 28232.29000000359 CPU Memory Usage: 640 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::le: Self CPU time: 26218.55699999997 CPU Time: 60151.69900000782 CPU Memory Usage: 320 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 5154.750999999182 CPU Time: 5154.750999999182 CPU Memory Usage: 320 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::to: Self CPU time: 5434.760999997685 CPU Time: 15913.560999998524 CPU Memory Usage: 320 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 3258.132000004989 CPU Time: 3258.132000004989 CPU Memory Usage: 240 b\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::to: Self CPU time: 4423.540999991354 CPU Time: 11086.307999999379 CPU Memory Usage: 240 b\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::empty_strided: Self CPU time: 1044.8159999998752 CPU Time: 1044.8159999998752 CPU Memory Usage: 80 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::to: Self CPU time: 1113.9060000017635 CPU Time: 3263.529000001028 CPU Memory Usage: 80 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::empty_strided: Self CPU time: 1143.7090000002172 CPU Time: 1143.7090000002172 CPU Memory Usage: 80 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::to: Self CPU time: 1193.4129999977977 CPU Time: 3520.337999999356 CPU Memory Usage: 80 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::empty_like: Self CPU time: 537.0020000021905 CPU Time: 1060.5100000015227 CPU Memory Usage: 40 b\n",
      "aten::ones_like: Self CPU time: 625.6089999976684 CPU Time: 2196.2210000000778 CPU Memory Usage: 40 b\n",
      "aten::mean: Self CPU time: 1176.9120000003604 CPU Time: 6700.167999999714 CPU Memory Usage: 40 b\n",
      "  model.lossFunc.batchedPrLoss:(124)                                  if rollupLosses: return res.mean()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::add_: Self CPU time: 155647.97200000216 CPU Time: 155647.97200000216 CPU Memory Usage: 0 b\n",
      "aten::_index_put_impl_: Self CPU time: 1266.6120000015944 CPU Time: 3172.0310000010068 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::fill_: Self CPU time: 646.8069999981672 CPU Time: 646.8069999981672 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::zero_: Self CPU time: 972.7100000013597 CPU Time: 1619.516999999527 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::_index_put_impl_: Self CPU time: 1379.5109999966808 CPU Time: 3731.434999999008 CPU Memory Usage: 0 b\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "aten::conj: Self CPU time: 1067.9060000006575 CPU Time: 1067.9060000006575 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(220)                                                             posEmbs = self.embedDropout(posEmbs)\n",
      "\n",
      "CatBackward: Self CPU time: 54.89999999594875 CPU Time: 4210.842999997432 CPU Memory Usage: 0 b\n",
      "  model:(221)                                                         embeddings = torch.cat([embs, posEmbs], axis=3)\n",
      "\n",
      "aten::conj: Self CPU time: 4485.845000002708 CPU Time: 4485.845000002708 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::squeeze: Self CPU time: 33077.736000011384 CPU Time: 49330.29400000116 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(494)                                  u = torch.einsum(\"binj,bjnk->bink\", queryPrs, v)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "UnsqueezeBackward0: Self CPU time: 605.4089999986463 CPU Time: 49935.702999999805 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(494)                                  u = torch.einsum(\"binj,bjnk->bink\", queryPrs, v)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::transpose: Self CPU time: 33664.13600001228 CPU Time: 49901.5990000157 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(494)                                  u = torch.einsum(\"binj,bjnk->bink\", queryPrs, v)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "PermuteBackward: Self CPU time: 1964.0090000121272 CPU Time: 124327.25600001408 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(494)                                  u = torch.einsum(\"binj,bjnk->bink\", queryPrs, v)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "ViewBackward: Self CPU time: 1664.7180000068038 CPU Time: 100850.32200000982 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(494)                                  u = torch.einsum(\"binj,bjnk->bink\", queryPrs, v)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::squeeze: Self CPU time: 137623.9729999749 CPU Time: 202316.2289999912 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "UnsqueezeBackward0: Self CPU time: 2496.528000018734 CPU Time: 204812.75700000994 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "CloneBackward: Self CPU time: 65.20000000525033 CPU Time: 65.20000000525033 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "UnsafeViewBackward: Self CPU time: 189.1000000028289 CPU Time: 12544.827000001038 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::transpose: Self CPU time: 68893.60000002262 CPU Time: 101120.00600001484 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "PermuteBackward: Self CPU time: 7578.6789999985485 CPU Time: 253661.571000004 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "AddBackward0: Self CPU time: 338.6020000024582 CPU Time: 338.6020000024582 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::conj: Self CPU time: 17322.1729999925 CPU Time: 17322.1729999925 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::conj: Self CPU time: 4606.447000006156 CPU Time: 4606.447000006156 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::masked_fill_: Self CPU time: 33085.530000008934 CPU Time: 33085.530000008934 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(56)                                   lessThan = torch.min(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::conj: Self CPU time: 4527.238999998779 CPU Time: 4527.238999998779 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "AddBackward0: Self CPU time: 66.10099999961676 CPU Time: 66.10099999961676 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "SubBackward0: Self CPU time: 89.7000000027474 CPU Time: 89.7000000027474 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::_local_scalar_dense: Self CPU time: 4337.044999993523 CPU Time: 4337.044999993523 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::item: Self CPU time: 4155.54100000544 CPU Time: 8492.585999998963 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::squeeze: Self CPU time: 69257.99099999503 CPU Time: 102209.82199998904 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "UnsqueezeBackward0: Self CPU time: 1221.0090000071214 CPU Time: 103430.83099999616 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::transpose: Self CPU time: 34341.241999995545 CPU Time: 50315.99699999206 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "PermuteBackward: Self CPU time: 1983.9250000073807 CPU Time: 124708.9640000108 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "ViewBackward: Self CPU time: 1781.118999994942 CPU Time: 102195.79899999319 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "AddBackward0: Self CPU time: 164.70200000563636 CPU Time: 164.70200000563636 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::conj: Self CPU time: 9104.492999998969 CPU Time: 9104.492999998969 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7:(568)                                        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "AddBackward0: Self CPU time: 204.70300000178395 CPU Time: 204.70300000178395 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::expand: Self CPU time: 18057.48000000714 CPU Time: 26223.761000006343 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::masked_fill_: Self CPU time: 17532.578999985417 CPU Time: 17532.578999985417 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::conj: Self CPU time: 34929.144000010914 CPU Time: 34929.144000010914 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(140)                             return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::unsqueeze: Self CPU time: 34222.946000024036 CPU Time: 66029.36100000347 CPU Memory Usage: 0 b\n",
      "aten::select: Self CPU time: 104095.9409999834 CPU Time: 201350.623999983 CPU Memory Usage: 0 b\n",
      "AddBackward0: Self CPU time: 188.3040000058245 CPU Time: 188.3040000058245 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(140)                             return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
      "\n",
      "  model.encodingLayers.7:(569)                                        return self.layerNorm2(ui+projectedBack)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "ViewBackward: Self CPU time: 81.10100000508828 CPU Time: 4843.14800000214 CPU Memory Usage: 0 b\n",
      "  model:(228)                                                         flattenedOutputs = forwardPass.reshape((b,L,n*d))\n",
      "\n",
      "aten::squeeze: Self CPU time: 3088.5310000000172 CPU Time: 4527.146999999997 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "UnsqueezeBackward0: Self CPU time: 77.20000000292202 CPU Time: 4604.347000002919 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::transpose: Self CPU time: 2150.621999994153 CPU Time: 3166.7309999956633 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "PermuteBackward: Self CPU time: 126.4999999954016 CPU Time: 7763.07799999899 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "ViewBackward: Self CPU time: 108.6019999954151 CPU Time: 6643.3679999965825 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::copy_: Self CPU time: 117586.28700001753 CPU Time: 120128.11300001747 CPU Memory Usage: 0 b\n",
      "AddBackward0: Self CPU time: 13.100999999500345 CPU Time: 13.100999999500345 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::conj: Self CPU time: 545.1070000004256 CPU Time: 545.1070000004256 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::conj: Self CPU time: 1101.8110000003362 CPU Time: 1101.8110000003362 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::unsqueeze: Self CPU time: 1072.910000001837 CPU Time: 1560.8190000008326 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(123)                                  res = vals.sum(axis=2)\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "SumBackward1: Self CPU time: 48.99999999930151 CPU Time: 3187.232000000484 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(123)                                  res = vals.sum(axis=2)\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::expand: Self CPU time: 2592.9240000000573 CPU Time: 3585.434000000474 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(124)                                  if rollupLosses: return res.mean()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "torch::autograd::GraphRoot: Self CPU time: 11.799999999522697 CPU Time: 11.799999999522697 CPU Memory Usage: 0 b\n",
      "aten::div_: Self CPU time: 1139.8119999998016 CPU Time: 5962.357999999309 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(124)                                  if rollupLosses: return res.mean()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::fill_: Self CPU time: 1090.9099999989849 CPU Time: 1090.9099999989849 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(123)                                  res = vals.sum(axis=2)\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::as_strided: Self CPU time: 2573.228999999177 CPU Time: 2573.228999999177 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(123)                                  res = vals.sum(axis=2)\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::copy_: Self CPU time: 3404.635000003036 CPU Time: 3404.635000003036 CPU Memory Usage: 0 b\n",
      "  model.lossFunc.batchedPrLoss:(121)                                  vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
      "\n",
      "  model.lossFunc:(79)                                                    loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "  model:(240)                                                             loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
      "\n",
      "aten::contiguous: Self CPU time: 1479.815999996732 CPU Time: 1479.815999996732 CPU Memory Usage: 0 b\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::copy_: Self CPU time: 1104.8069999993895 CPU Time: 1104.8069999993895 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::div_: Self CPU time: 1299.5139999997918 CPU Time: 2947.0300000012503 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::select: Self CPU time: 1634.3149999972666 CPU Time: 3194.2339999988326 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::stride: Self CPU time: 8379.58100000472 CPU Time: 8379.58100000472 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::view: Self CPU time: 8969.887999994622 CPU Time: 8969.887999994622 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::reshape: Self CPU time: 3289.634000003687 CPU Time: 9729.89700000215 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::unsqueeze: Self CPU time: 3632.436000001908 CPU Time: 5237.554000002507 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::as_strided: Self CPU time: 10750.312000007078 CPU Time: 10750.312000007078 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::permute: Self CPU time: 10839.511000003607 CPU Time: 15970.06100000703 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model:(237)                                                         wordPrs = self.softmax(self.finalProjection2(flattenedOutputs, \"bli,iv->blv\"))\n",
      "\n",
      "aten::clamp_max: Self CPU time: 5594.95800000133 CPU Time: 5594.95800000133 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(59)                                       res.clamp_max_(self.maxMag)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::copy_: Self CPU time: 47375.77700000574 CPU Time: 47375.77700000574 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(57)                                   res = biggerThan + lessThan*self.weightLess - self.offset\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "detach_: Self CPU time: 9368.394000002561 CPU Time: 9368.394000002561 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::detach_: Self CPU time: 9403.197999990589 CPU Time: 18771.59199999315 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.RELU:(55)                                   biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::copy_: Self CPU time: 18970.67800000579 CPU Time: 18970.67800000579 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div_: Self CPU time: 26321.175000003248 CPU Time: 57178.68300000151 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::select: Self CPU time: 26545.567000015493 CPU Time: 51522.41600000974 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::stride: Self CPU time: 140335.89700001857 CPU Time: 140335.89700001857 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::view: Self CPU time: 145392.3730000283 CPU Time: 145392.3730000283 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::reshape: Self CPU time: 54497.80699995923 CPU Time: 158403.95699998233 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::unsqueeze: Self CPU time: 82103.53399999469 CPU Time: 122375.53400000694 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::as_strided: Self CPU time: 201240.10600000696 CPU Time: 201240.10600000696 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::permute: Self CPU time: 184084.3449999914 CPU Time: 271149.01600000134 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7:(566)                                        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "detach_: Self CPU time: 9689.594000003359 CPU Time: 9689.594000003359 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::detach_: Self CPU time: 9777.908999995932 CPU Time: 19467.50299999929 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(139)                             normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div_: Self CPU time: 39503.902000003894 CPU Time: 171057.19999998686 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::unsqueeze: Self CPU time: 18560.489999996244 CPU Time: 36294.06299999969 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::copy_: Self CPU time: 108688.96500000879 CPU Time: 108688.96500000879 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::as_strided: Self CPU time: 78079.46600000578 CPU Time: 78079.46600000578 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::select: Self CPU time: 56522.97000001163 CPU Time: 108702.58200001477 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::fill_: Self CPU time: 18233.492000009057 CPU Time: 18233.492000009057 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.layerNorm2:(137)                             mu = x.mean((1,2,3), keepdim=True)\n",
      "\n",
      "  model.encodingLayers.7:(563)                                        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::copy_: Self CPU time: 5324.049000001658 CPU Time: 5324.049000001658 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div_: Self CPU time: 20421.311000003792 CPU Time: 61744.62600000262 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::bernoulli_: Self CPU time: 15084.742999999507 CPU Time: 20262.492999997114 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model.encodingLayers.7.attention:(538)                                  res = self.attentionDropout(res)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::_unsafe_view: Self CPU time: 36971.97299999831 CPU Time: 53743.1569999962 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(536)                              res = self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::copy_: Self CPU time: 12593.036000000215 CPU Time: 12593.036000000215 CPU Memory Usage: 0 b\n",
      "  model.softmax:(1198)                                                 return F.softmax(input, self.dim, _stacklevel=5)\n",
      "\n",
      "  model.encodingLayers.7.attention:(492)                                  queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::copy_: Self CPU time: 9677.803999997835 CPU Time: 9677.803999997835 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::select: Self CPU time: 25685.243000010996 CPU Time: 50442.30399999191 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::stride: Self CPU time: 136585.3610000316 CPU Time: 136585.3610000316 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::view: Self CPU time: 146730.1520000131 CPU Time: 146730.1520000131 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::reshape: Self CPU time: 53724.840999988315 CPU Time: 157592.7720000031 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::unsqueeze: Self CPU time: 39942.60600000978 CPU Time: 58714.49300000096 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::as_strided: Self CPU time: 162158.24799995255 CPU Time: 162158.24799995255 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::permute: Self CPU time: 184110.76200001134 CPU Time: 270250.0409999986 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention:(489)                                  dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::copy_: Self CPU time: 73106.73199999344 CPU Time: 73106.73199999344 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::div_: Self CPU time: 57634.08099999036 CPU Time: 122077.62500000258 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(172)                                        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::select: Self CPU time: 52750.623999978794 CPU Time: 103320.45299998553 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::stride: Self CPU time: 278722.4709999711 CPU Time: 278722.4709999711 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::view: Self CPU time: 278679.0309999775 CPU Time: 278679.0309999775 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::unsqueeze: Self CPU time: 166648.1119999725 CPU Time: 245694.4789999854 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::as_strided: Self CPU time: 405771.9930000318 CPU Time: 405771.9930000318 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::permute: Self CPU time: 377176.28600002476 CPU Time: 556413.4210000283 CPU Memory Usage: 0 b\n",
      "  model.finalProjection2:(171)                                            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
      "\n",
      "  model.encodingLayers.7.attention:(462)                              q = self.Q(x, \"blnd,dk->blnk\")\n",
      "\n",
      "  model.encodingLayers.7:(562)                                        attentionOut = self.attention(x)\n",
      "\n",
      "  model.encodingLayers:(117)                                              input = module(input)\n",
      "\n",
      "  model:(224)                                                         forwardPass = self.encodingLayers(embeddings)\n",
      "\n",
      "aten::slice: Self CPU time: 2564.526000002259 CPU Time: 4052.637999999033 CPU Memory Usage: 0 b\n",
      "  model:(221)                                                         embeddings = torch.cat([embs, posEmbs], axis=3)\n",
      "\n",
      "aten::narrow: Self CPU time: 1615.2200000016473 CPU Time: 5667.8580000006805 CPU Memory Usage: 0 b\n",
      "  model:(221)                                                         embeddings = torch.cat([embs, posEmbs], axis=3)\n",
      "\n",
      "aten::reshape: Self CPU time: 3546.232000001344 CPU Time: 8817.886999998951 CPU Memory Usage: 0 b\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "aten::stride: Self CPU time: 3995.738000000396 CPU Time: 3995.738000000396 CPU Memory Usage: 0 b\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "aten::to: Self CPU time: 557.6060000009602 CPU Time: 557.6060000009602 CPU Memory Usage: 0 b\n",
      "  model:(218)                                                         posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,self.config.posEmbeddingDim)).expand((b,L,n,self.config.posEmbeddingDim))\n",
      "\n",
      "aten::copy_: Self CPU time: 1183.216000001341 CPU Time: 1183.216000001341 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::div_: Self CPU time: 4075.2410000027794 CPU Time: 12804.732000002807 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::bernoulli_: Self CPU time: 3280.3249999997624 CPU Time: 4449.7419999990325 CPU Memory Usage: 0 b\n",
      "  model.encodingLayers.7.attention.attentionDropout:(58)             return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "  model:(216)                                                             embs = self.embedDropout(embs)\n",
      "\n",
      "aten::as_strided: Self CPU time: 148559.48599997687 CPU Time: 148559.48599997687 CPU Memory Usage: 0 b\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::expand: Self CPU time: 2141.5219999996243 CPU Time: 3099.7319999989104 CPU Memory Usage: 0 b\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::view: Self CPU time: 59080.30400000204 CPU Time: 59080.30400000204 CPU Memory Usage: 0 b\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::view: Self CPU time: 1037.6130000001976 CPU Time: 1037.6130000001976 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::reshape: Self CPU time: 1062.809000001218 CPU Time: 2100.4220000014157 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::as_strided: Self CPU time: 1055.6120000006867 CPU Time: 1055.6120000006867 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::stride: Self CPU time: 1016.3090000004277 CPU Time: 1016.3090000004277 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::to: Self CPU time: 656.0070000003552 CPU Time: 656.0070000003552 CPU Memory Usage: 0 b\n",
      "  model.embedding:(186)                                               return self.embeddings[x]\n",
      "\n",
      "  model:(214)                                                         embs = self.embedding(x).view((b,L,1,self.config.embeddingDim)).expand((b, L, n, self.config.embeddingDim))\n",
      "\n",
      "aten::fill_: Self CPU time: 66971.73399999733 CPU Time: 66971.73399999733 CPU Memory Usage: 0 b\n",
      "aten::zero_: Self CPU time: 1983.11999999812 CPU Time: 3575.8409999962296 CPU Memory Usage: 0 b\n",
      "backward: Self CPU time: 110227.10899995884 CPU Time: 5505890.164 CPU Memory Usage: -473.17 Mb\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd.profiler import format_memory, format_time_share, format_time\n",
    "events = list(averages)\n",
    "events.sort(key=lambda x: x.cpu_memory_usage)\n",
    "def stackItemToStr(stackItem):\n",
    "    moduleName, lineNum, code = stackItem\n",
    "    return f\"{moduleName}:({lineNum})\"\n",
    "\n",
    "largestModuleNameSize = max([max([len(stackItemToStr(s)) for s in event.stack]) for event in events if len(event.stack) > 0])\n",
    "for event in events[::-1]:\n",
    "    print(f\"{event.key}: Self CPU time: {event.self_cpu_time_total} CPU Time: {event.cpu_time_total} CPU Memory Usage: {format_memory(event.cpu_memory_usage)}\")\n",
    "    for moduleName, lineNum, code in event.stack:\n",
    "        moduleStr = \"  \" + stackItemToStr((moduleName, lineNum, s))\n",
    "        padding = \" \"*(largestModuleNameSize-len(moduleName)) # ensures code lines all line up\n",
    "        print(moduleStr + padding + code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, (x, desired))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to sort by:\n",
    "# cpu_time, cpu_time_total\n",
    "# cuda_time, cuda_time_total\n",
    "# self_cpu_memory_usage, cpu_memory_usage\n",
    "# self_cuda_memory_usage, cuda_memory_usage\n",
    "# count\n",
    "#print(dir(list(prof.key_averages())[0]))\n",
    "events = [x for x in prof.function_events]\n",
    "events.sort(key=lambda x: x.name)\n",
    "from collections import defaultdict\n",
    "\n",
    "class CustomEvent(object):\n",
    "    def __init__(self, event):\n",
    "        self.event = event\n",
    "        self.stackItems = [s for s in event.stack if \"forward\" in s]\n",
    "\n",
    "def groupBy(arr, keyFunc):\n",
    "    groupedBy = defaultdict(lambda: [])\n",
    "    for x in arr:\n",
    "        groupedBy[keyFunc(x)].append(x)\n",
    "    return groupedBy\n",
    "\n",
    "\n",
    "goodEvents = [CustomEvent(event) for event in events]\n",
    "\n",
    "groupedByStack = groupBy(goodEvents, lambda x: \"\\n\".join(x.stackItems))\n",
    "\n",
    "#print([x for x in prof.function_events][100].stack)\n",
    "#print([x for x in prof.function_events][0].cpu_children)\n",
    "#print(list(prof.key_averages())[0].cpu_children)\n",
    "#print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"self_cuda_memory_usage\", row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(groupedByStack[firstKey][0].event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
