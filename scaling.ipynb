{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, ndigit, split):\n",
    "        self.split = split # train/test\n",
    "        self.ndigit = ndigit\n",
    "        self.vocab_size = 10 # 10 possible digits 0..9\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset for e.g. 2-digit addition\n",
    "ndigit = 1\n",
    "train_dataset = FactoringDataset(ndigit=ndigit, base=64, split='train')\n",
    "test_dataset = FactoringDataset(ndigit=ndigit, base=64, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bases greater than 36 not handled in base_repr.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-a0be1c17c5b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-73e0ca6f0313>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masStr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mdix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# convert each character to its token index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# x will be input to GPT and y will be the associated expected outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-73e0ca6f0313>\u001b[0m in \u001b[0;36masStr\u001b[0;34m(self, idx, base)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msortedThings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msortedThings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{formatBase(c, base, self.ndigit*2)}{formatBase(a, base, self.ndigit)}{formatBase(b, base, self.ndigit)}'\u001b[0m \u001b[0;31m# e.g. 15=3*5= becomes \"150305\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-73e0ca6f0313>\u001b[0m in \u001b[0;36mformatBase\u001b[0;34m(n, base, numDigits)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mformatBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumDigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumDigits\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mbase_repr\u001b[0;34m(number, base, padding)\u001b[0m\n\u001b[1;32m   2051\u001b[0m     \u001b[0mdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bases greater than 36 not handled in base_repr.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bases less than 2 not handled in base_repr.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Bases greater than 36 not handled in base_repr."
     ]
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 22:22:56 - INFO - minGPT.mingpt.model -   number of parameters: 4.024960e+05\n"
     ]
    }
   ],
   "source": [
    "from minGPT.mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "\n",
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=8, n_head=8, n_embd=64)\n",
    "model = GPT(mconf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 22:23:19 - INFO - minGPT.mingpt.trainer -   loading from checkpoint layer8head8emb64FactoringDatasetbase16digit2\n",
      "11/12/2020 22:23:19 - INFO - minGPT.mingpt.trainer -   loading losses from checkpoint layer8head8emb64FactoringDatasetbase16digit2losses.json\n",
      "epoch 1 iter 2: train loss 2.64284. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.25it/s]\n",
      "11/12/2020 22:23:20 - INFO - minGPT.mingpt.trainer -   test loss: 2.608861\n",
      "11/12/2020 22:23:20 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 2/2333 = 0.09% correct\n",
      "final score: 0/583 = 0.00% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 iter 2: train loss 2.58420. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:23:21 - INFO - minGPT.mingpt.trainer -   test loss: 2.547491\n",
      "11/12/2020 22:23:21 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 3 iter 2: train loss 2.52601. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:23:21 - INFO - minGPT.mingpt.trainer -   test loss: 2.502897\n",
      "11/12/2020 22:23:21 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 4 iter 2: train loss 2.50215. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s]\n",
      "11/12/2020 22:23:22 - INFO - minGPT.mingpt.trainer -   test loss: 2.463198\n",
      "11/12/2020 22:23:22 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 5 iter 2: train loss 2.45072. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:23:22 - INFO - minGPT.mingpt.trainer -   test loss: 2.427675\n",
      "11/12/2020 22:23:22 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 6 iter 2: train loss 2.41203. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:23:23 - INFO - minGPT.mingpt.trainer -   test loss: 2.393545\n",
      "11/12/2020 22:23:23 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 7 iter 2: train loss 2.39267. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:23:24 - INFO - minGPT.mingpt.trainer -   test loss: 2.365149\n",
      "11/12/2020 22:23:24 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 8 iter 2: train loss 2.36379. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  5.71it/s]\n",
      "11/12/2020 22:23:24 - INFO - minGPT.mingpt.trainer -   test loss: 2.341083\n",
      "11/12/2020 22:23:24 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 9 iter 2: train loss 2.34280. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.22it/s]\n",
      "11/12/2020 22:23:25 - INFO - minGPT.mingpt.trainer -   test loss: 2.318405\n",
      "11/12/2020 22:23:25 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 10 iter 2: train loss 2.31900. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "11/12/2020 22:23:25 - INFO - minGPT.mingpt.trainer -   test loss: 2.292832\n",
      "11/12/2020 22:23:25 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 11 iter 2: train loss 2.28648. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:23:26 - INFO - minGPT.mingpt.trainer -   test loss: 2.269589\n",
      "11/12/2020 22:23:26 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 12 iter 2: train loss 2.26976. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:23:26 - INFO - minGPT.mingpt.trainer -   test loss: 2.246603\n",
      "11/12/2020 22:23:26 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 13 iter 2: train loss 2.22852. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:23:27 - INFO - minGPT.mingpt.trainer -   test loss: 2.228808\n",
      "11/12/2020 22:23:27 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 14 iter 2: train loss 2.22758. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:23:27 - INFO - minGPT.mingpt.trainer -   test loss: 2.204715\n",
      "11/12/2020 22:23:27 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 15 iter 2: train loss 2.21802. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:23:28 - INFO - minGPT.mingpt.trainer -   test loss: 2.191232\n",
      "11/12/2020 22:23:28 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 16 iter 2: train loss 2.19447. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:23:28 - INFO - minGPT.mingpt.trainer -   test loss: 2.168904\n",
      "11/12/2020 22:23:28 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 17 iter 2: train loss 2.17633. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:23:29 - INFO - minGPT.mingpt.trainer -   test loss: 2.156213\n",
      "11/12/2020 22:23:29 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 18 iter 2: train loss 2.16703. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:23:29 - INFO - minGPT.mingpt.trainer -   test loss: 2.143734\n",
      "11/12/2020 22:23:29 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 19 iter 2: train loss 2.14607. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.16it/s]\n",
      "11/12/2020 22:23:30 - INFO - minGPT.mingpt.trainer -   test loss: 2.126366\n",
      "11/12/2020 22:23:30 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 20 iter 2: train loss 2.12553. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:23:31 - INFO - minGPT.mingpt.trainer -   test loss: 2.120265\n",
      "11/12/2020 22:23:31 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 21 iter 2: train loss 2.12271. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.31it/s]\n",
      "11/12/2020 22:23:31 - INFO - minGPT.mingpt.trainer -   test loss: 2.106125\n",
      "11/12/2020 22:23:31 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 18/2333 = 0.77% correct\n",
      "final score: 3/583 = 0.51% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 22 iter 2: train loss 2.10692. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:23:32 - INFO - minGPT.mingpt.trainer -   test loss: 2.099664\n",
      "11/12/2020 22:23:32 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 23 iter 2: train loss 2.10657. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:23:33 - INFO - minGPT.mingpt.trainer -   test loss: 2.086666\n",
      "11/12/2020 22:23:33 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 24 iter 2: train loss 2.10408. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.17it/s]\n",
      "11/12/2020 22:23:33 - INFO - minGPT.mingpt.trainer -   test loss: 2.082760\n",
      "11/12/2020 22:23:33 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 25 iter 2: train loss 2.10511. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.14it/s]\n",
      "11/12/2020 22:23:34 - INFO - minGPT.mingpt.trainer -   test loss: 2.074566\n",
      "11/12/2020 22:23:34 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 26 iter 2: train loss 2.08575. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.32it/s]\n",
      "11/12/2020 22:23:34 - INFO - minGPT.mingpt.trainer -   test loss: 2.067551\n",
      "11/12/2020 22:23:34 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 27 iter 2: train loss 2.06922. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.63it/s]\n",
      "11/12/2020 22:23:35 - INFO - minGPT.mingpt.trainer -   test loss: 2.065868\n",
      "11/12/2020 22:23:35 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 28 iter 2: train loss 2.06767. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:23:35 - INFO - minGPT.mingpt.trainer -   test loss: 2.058151\n",
      "11/12/2020 22:23:35 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 29 iter 2: train loss 2.07576. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.61it/s]\n",
      "11/12/2020 22:23:36 - INFO - minGPT.mingpt.trainer -   test loss: 2.054259\n",
      "11/12/2020 22:23:36 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 30 iter 2: train loss 2.06761. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:23:36 - INFO - minGPT.mingpt.trainer -   test loss: 2.054526\n",
      "epoch 31 iter 2: train loss 2.06964. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.62it/s]\n",
      "11/12/2020 22:23:37 - INFO - minGPT.mingpt.trainer -   test loss: 2.052151\n",
      "11/12/2020 22:23:37 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 32 iter 2: train loss 2.05970. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:23:37 - INFO - minGPT.mingpt.trainer -   test loss: 2.048304\n",
      "11/12/2020 22:23:37 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 33 iter 2: train loss 2.04486. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:23:38 - INFO - minGPT.mingpt.trainer -   test loss: 2.046422\n",
      "11/12/2020 22:23:38 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 34 iter 2: train loss 2.06454. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:23:38 - INFO - minGPT.mingpt.trainer -   test loss: 2.044613\n",
      "11/12/2020 22:23:38 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 35 iter 2: train loss 2.05347. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:23:39 - INFO - minGPT.mingpt.trainer -   test loss: 2.041679\n",
      "11/12/2020 22:23:39 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 36 iter 2: train loss 2.05889. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:23:39 - INFO - minGPT.mingpt.trainer -   test loss: 2.038908\n",
      "11/12/2020 22:23:39 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 37 iter 2: train loss 2.04548. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:23:40 - INFO - minGPT.mingpt.trainer -   test loss: 2.038826\n",
      "11/12/2020 22:23:40 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 38 iter 2: train loss 2.03224. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:23:40 - INFO - minGPT.mingpt.trainer -   test loss: 2.035640\n",
      "11/12/2020 22:23:40 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 39 iter 2: train loss 2.04755. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:23:41 - INFO - minGPT.mingpt.trainer -   test loss: 2.031395\n",
      "11/12/2020 22:23:41 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 40 iter 2: train loss 2.05538. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:23:41 - INFO - minGPT.mingpt.trainer -   test loss: 2.030136\n",
      "11/12/2020 22:23:41 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 41 iter 2: train loss 2.03014. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:23:42 - INFO - minGPT.mingpt.trainer -   test loss: 2.030080\n",
      "11/12/2020 22:23:42 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 10/2333 = 0.43% correct\n",
      "final score: 1/583 = 0.17% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 42 iter 2: train loss 2.04001. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:23:43 - INFO - minGPT.mingpt.trainer -   test loss: 2.027187\n",
      "11/12/2020 22:23:43 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 43 iter 2: train loss 2.02765. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:23:43 - INFO - minGPT.mingpt.trainer -   test loss: 2.022385\n",
      "11/12/2020 22:23:43 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 44 iter 2: train loss 2.02776. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:23:44 - INFO - minGPT.mingpt.trainer -   test loss: 2.021202\n",
      "11/12/2020 22:23:44 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 45 iter 2: train loss 2.04191. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.23it/s]\n",
      "11/12/2020 22:23:45 - INFO - minGPT.mingpt.trainer -   test loss: 2.019410\n",
      "11/12/2020 22:23:45 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 46 iter 2: train loss 2.02635. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:23:45 - INFO - minGPT.mingpt.trainer -   test loss: 2.016769\n",
      "11/12/2020 22:23:45 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 47 iter 2: train loss 2.02004. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:23:46 - INFO - minGPT.mingpt.trainer -   test loss: 2.014127\n",
      "11/12/2020 22:23:46 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 48 iter 2: train loss 2.04288. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:23:46 - INFO - minGPT.mingpt.trainer -   test loss: 2.010440\n",
      "11/12/2020 22:23:46 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 49 iter 2: train loss 2.02540. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "11/12/2020 22:23:47 - INFO - minGPT.mingpt.trainer -   test loss: 2.004325\n",
      "11/12/2020 22:23:47 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 50 iter 2: train loss 2.01922. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.29it/s]\n",
      "11/12/2020 22:23:47 - INFO - minGPT.mingpt.trainer -   test loss: 2.002157\n",
      "11/12/2020 22:23:47 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 51 iter 2: train loss 2.01239. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.24it/s]\n",
      "11/12/2020 22:23:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.996643\n",
      "11/12/2020 22:23:48 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 52 iter 2: train loss 2.00375. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:23:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.986219\n",
      "11/12/2020 22:23:48 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 53 iter 2: train loss 2.01333. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:23:49 - INFO - minGPT.mingpt.trainer -   test loss: 1.980863\n",
      "11/12/2020 22:23:49 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 54 iter 2: train loss 1.98596. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.31it/s]\n",
      "11/12/2020 22:23:49 - INFO - minGPT.mingpt.trainer -   test loss: 1.975432\n",
      "11/12/2020 22:23:49 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 55 iter 2: train loss 1.99353. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.24it/s]\n",
      "11/12/2020 22:23:50 - INFO - minGPT.mingpt.trainer -   test loss: 1.967029\n",
      "11/12/2020 22:23:50 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 56 iter 2: train loss 1.97973. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:23:50 - INFO - minGPT.mingpt.trainer -   test loss: 1.963012\n",
      "11/12/2020 22:23:50 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 57 iter 2: train loss 1.96996. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  5.91it/s]\n",
      "11/12/2020 22:23:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.943311\n",
      "11/12/2020 22:23:51 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 58 iter 2: train loss 1.94987. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:23:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.936935\n",
      "11/12/2020 22:23:51 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 59 iter 2: train loss 1.96176. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:23:52 - INFO - minGPT.mingpt.trainer -   test loss: 1.928187\n",
      "11/12/2020 22:23:52 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 60 iter 2: train loss 1.93409. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:23:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.912735\n",
      "11/12/2020 22:23:53 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 61 iter 2: train loss 1.92474. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.27it/s]\n",
      "11/12/2020 22:23:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.898877\n",
      "11/12/2020 22:23:54 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 16/2333 = 0.69% correct\n",
      "final score: 1/583 = 0.17% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 62 iter 2: train loss 1.92505. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.66it/s]\n",
      "11/12/2020 22:23:54 - INFO - minGPT.mingpt.trainer -   test loss: 1.888919\n",
      "11/12/2020 22:23:54 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 63 iter 2: train loss 1.90549. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:23:55 - INFO - minGPT.mingpt.trainer -   test loss: 1.879326\n",
      "11/12/2020 22:23:55 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 64 iter 2: train loss 1.87523. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:23:55 - INFO - minGPT.mingpt.trainer -   test loss: 1.877223\n",
      "11/12/2020 22:23:55 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 65 iter 2: train loss 1.87192. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:23:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.852389\n",
      "11/12/2020 22:23:56 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 66 iter 2: train loss 1.85275. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:23:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.838595\n",
      "11/12/2020 22:23:56 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 67 iter 2: train loss 1.86385. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:23:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.834528\n",
      "11/12/2020 22:23:57 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 68 iter 2: train loss 1.86643. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:23:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.834201\n",
      "11/12/2020 22:23:57 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 69 iter 2: train loss 1.83125. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:23:58 - INFO - minGPT.mingpt.trainer -   test loss: 1.808018\n",
      "11/12/2020 22:23:58 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 70 iter 2: train loss 1.83072. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:23:58 - INFO - minGPT.mingpt.trainer -   test loss: 1.824450\n",
      "epoch 71 iter 2: train loss 1.83861. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:23:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.791176\n",
      "11/12/2020 22:23:59 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 72 iter 2: train loss 1.80595. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  5.81it/s]\n",
      "11/12/2020 22:23:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.775537\n",
      "11/12/2020 22:23:59 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 73 iter 2: train loss 1.78784. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:24:00 - INFO - minGPT.mingpt.trainer -   test loss: 1.755812\n",
      "11/12/2020 22:24:00 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 74 iter 2: train loss 1.77857. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:00 - INFO - minGPT.mingpt.trainer -   test loss: 1.741960\n",
      "11/12/2020 22:24:00 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 75 iter 2: train loss 1.76729. lr 3.000000e-04: 100%|██████████| 3/3 [00:00<00:00,  5.93it/s]\n",
      "11/12/2020 22:24:01 - INFO - minGPT.mingpt.trainer -   test loss: 1.750097\n",
      "epoch 76 iter 2: train loss 1.75011. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:24:01 - INFO - minGPT.mingpt.trainer -   test loss: 1.726106\n",
      "11/12/2020 22:24:01 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 77 iter 2: train loss 1.73963. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:24:02 - INFO - minGPT.mingpt.trainer -   test loss: 1.715815\n",
      "11/12/2020 22:24:02 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 78 iter 2: train loss 1.71941. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.31it/s]\n",
      "11/12/2020 22:24:03 - INFO - minGPT.mingpt.trainer -   test loss: 1.699506\n",
      "11/12/2020 22:24:03 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 79 iter 2: train loss 1.70346. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:24:03 - INFO - minGPT.mingpt.trainer -   test loss: 1.690610\n",
      "11/12/2020 22:24:03 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 80 iter 2: train loss 1.72044. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.59it/s]\n",
      "11/12/2020 22:24:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.678964\n",
      "11/12/2020 22:24:04 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 81 iter 2: train loss 1.70579. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.679047\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 29/2333 = 1.24% correct\n",
      "final score: 10/583 = 1.72% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 82 iter 2: train loss 1.68702. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:24:05 - INFO - minGPT.mingpt.trainer -   test loss: 1.671738\n",
      "11/12/2020 22:24:05 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 83 iter 2: train loss 1.66155. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.656869\n",
      "11/12/2020 22:24:06 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 84 iter 2: train loss 1.68418. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.21it/s]\n",
      "11/12/2020 22:24:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.640218\n",
      "11/12/2020 22:24:06 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 85 iter 2: train loss 1.67717. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:24:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.651881\n",
      "epoch 86 iter 2: train loss 1.65012. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:24:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.636378\n",
      "11/12/2020 22:24:07 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 87 iter 2: train loss 1.64383. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:24:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.643451\n",
      "epoch 88 iter 2: train loss 1.64770. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:24:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.619312\n",
      "11/12/2020 22:24:08 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 89 iter 2: train loss 1.63989. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "11/12/2020 22:24:09 - INFO - minGPT.mingpt.trainer -   test loss: 1.610466\n",
      "11/12/2020 22:24:09 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 90 iter 2: train loss 1.64255. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  5.86it/s]\n",
      "11/12/2020 22:24:09 - INFO - minGPT.mingpt.trainer -   test loss: 1.600057\n",
      "11/12/2020 22:24:09 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 91 iter 2: train loss 1.61760. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:24:10 - INFO - minGPT.mingpt.trainer -   test loss: 1.584460\n",
      "11/12/2020 22:24:10 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 92 iter 2: train loss 1.62597. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:24:10 - INFO - minGPT.mingpt.trainer -   test loss: 1.586497\n",
      "epoch 93 iter 2: train loss 1.60160. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:11 - INFO - minGPT.mingpt.trainer -   test loss: 1.582398\n",
      "11/12/2020 22:24:11 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 94 iter 2: train loss 1.60107. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.21it/s]\n",
      "11/12/2020 22:24:11 - INFO - minGPT.mingpt.trainer -   test loss: 1.572613\n",
      "11/12/2020 22:24:11 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 95 iter 2: train loss 1.57855. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:12 - INFO - minGPT.mingpt.trainer -   test loss: 1.564274\n",
      "11/12/2020 22:24:12 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 96 iter 2: train loss 1.58027. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.562678\n",
      "11/12/2020 22:24:13 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 97 iter 2: train loss 1.57620. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:24:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.553794\n",
      "11/12/2020 22:24:13 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 98 iter 2: train loss 1.57658. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:24:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.554329\n",
      "epoch 99 iter 2: train loss 1.57600. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:24:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.562306\n",
      "epoch 100 iter 2: train loss 1.59257. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:24:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.543381\n",
      "11/12/2020 22:24:15 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 101 iter 2: train loss 1.59043. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:24:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.543600\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 51/2333 = 2.19% correct\n",
      "final score: 10/583 = 1.72% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 102 iter 2: train loss 1.55738. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:24:16 - INFO - minGPT.mingpt.trainer -   test loss: 1.542955\n",
      "11/12/2020 22:24:16 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 103 iter 2: train loss 1.56941. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.57it/s]\n",
      "11/12/2020 22:24:17 - INFO - minGPT.mingpt.trainer -   test loss: 1.541761\n",
      "11/12/2020 22:24:17 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 104 iter 2: train loss 1.55510. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:24:17 - INFO - minGPT.mingpt.trainer -   test loss: 1.534936\n",
      "11/12/2020 22:24:17 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 105 iter 2: train loss 1.56729. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.27it/s]\n",
      "11/12/2020 22:24:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.532205\n",
      "11/12/2020 22:24:18 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 106 iter 2: train loss 1.55455. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:24:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.538493\n",
      "epoch 107 iter 2: train loss 1.53470. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.79it/s]\n",
      "11/12/2020 22:24:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.537293\n",
      "epoch 108 iter 2: train loss 1.57335. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.63it/s]\n",
      "11/12/2020 22:24:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.529006\n",
      "11/12/2020 22:24:19 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 109 iter 2: train loss 1.53789. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:20 - INFO - minGPT.mingpt.trainer -   test loss: 1.529851\n",
      "epoch 110 iter 2: train loss 1.55605. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:20 - INFO - minGPT.mingpt.trainer -   test loss: 1.529296\n",
      "epoch 111 iter 2: train loss 1.56876. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.526728\n",
      "11/12/2020 22:24:21 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 112 iter 2: train loss 1.54486. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:24:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.526452\n",
      "11/12/2020 22:24:21 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 113 iter 2: train loss 1.54048. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:24:22 - INFO - minGPT.mingpt.trainer -   test loss: 1.522185\n",
      "11/12/2020 22:24:22 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 114 iter 2: train loss 1.55625. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.51it/s]\n",
      "11/12/2020 22:24:22 - INFO - minGPT.mingpt.trainer -   test loss: 1.522037\n",
      "11/12/2020 22:24:22 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 115 iter 2: train loss 1.53120. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:23 - INFO - minGPT.mingpt.trainer -   test loss: 1.521728\n",
      "11/12/2020 22:24:23 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 116 iter 2: train loss 1.55596. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:24:23 - INFO - minGPT.mingpt.trainer -   test loss: 1.521436\n",
      "11/12/2020 22:24:23 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 117 iter 2: train loss 1.53870. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:24:24 - INFO - minGPT.mingpt.trainer -   test loss: 1.520089\n",
      "11/12/2020 22:24:24 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 118 iter 2: train loss 1.55531. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:24:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.519865\n",
      "11/12/2020 22:24:25 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 119 iter 2: train loss 1.53344. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:24:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.518011\n",
      "11/12/2020 22:24:25 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 120 iter 2: train loss 1.53860. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:24:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.516076\n",
      "11/12/2020 22:24:26 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 121 iter 2: train loss 1.53001. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.31it/s]\n",
      "11/12/2020 22:24:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.519071\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 47/2333 = 2.01% correct\n",
      "final score: 8/583 = 1.37% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 122 iter 2: train loss 1.54388. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:24:27 - INFO - minGPT.mingpt.trainer -   test loss: 1.511563\n",
      "11/12/2020 22:24:27 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 123 iter 2: train loss 1.55829. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.02it/s]\n",
      "11/12/2020 22:24:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.513318\n",
      "epoch 124 iter 2: train loss 1.55328. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:24:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.512260\n",
      "epoch 125 iter 2: train loss 1.52786. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:29 - INFO - minGPT.mingpt.trainer -   test loss: 1.510003\n",
      "11/12/2020 22:24:29 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 126 iter 2: train loss 1.52791. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:24:29 - INFO - minGPT.mingpt.trainer -   test loss: 1.505548\n",
      "11/12/2020 22:24:29 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 127 iter 2: train loss 1.53306. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:24:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.503721\n",
      "11/12/2020 22:24:30 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 128 iter 2: train loss 1.51357. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:24:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.495896\n",
      "11/12/2020 22:24:30 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 129 iter 2: train loss 1.51303. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:24:31 - INFO - minGPT.mingpt.trainer -   test loss: 1.496442\n",
      "epoch 130 iter 2: train loss 1.51143. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.28it/s]\n",
      "11/12/2020 22:24:31 - INFO - minGPT.mingpt.trainer -   test loss: 1.493118\n",
      "11/12/2020 22:24:31 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 131 iter 2: train loss 1.52408. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:24:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.491505\n",
      "11/12/2020 22:24:32 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 132 iter 2: train loss 1.50951. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.493760\n",
      "epoch 133 iter 2: train loss 1.51159. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:24:33 - INFO - minGPT.mingpt.trainer -   test loss: 1.479713\n",
      "11/12/2020 22:24:33 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 134 iter 2: train loss 1.53685. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:24:33 - INFO - minGPT.mingpt.trainer -   test loss: 1.476852\n",
      "11/12/2020 22:24:33 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 135 iter 2: train loss 1.51524. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:24:34 - INFO - minGPT.mingpt.trainer -   test loss: 1.473230\n",
      "11/12/2020 22:24:34 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 136 iter 2: train loss 1.47309. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.61it/s]\n",
      "11/12/2020 22:24:34 - INFO - minGPT.mingpt.trainer -   test loss: 1.466688\n",
      "11/12/2020 22:24:34 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 137 iter 2: train loss 1.50567. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:24:35 - INFO - minGPT.mingpt.trainer -   test loss: 1.468685\n",
      "epoch 138 iter 2: train loss 1.51416. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:24:36 - INFO - minGPT.mingpt.trainer -   test loss: 1.474202\n",
      "epoch 139 iter 2: train loss 1.47115. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:24:36 - INFO - minGPT.mingpt.trainer -   test loss: 1.463844\n",
      "11/12/2020 22:24:36 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 140 iter 2: train loss 1.48026. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  5.98it/s]\n",
      "11/12/2020 22:24:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.472120\n",
      "epoch 141 iter 2: train loss 1.49483. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.443547\n",
      "11/12/2020 22:24:38 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 64/2333 = 2.74% correct\n",
      "final score: 6/583 = 1.03% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 142 iter 2: train loss 1.46604. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.51it/s]\n",
      "11/12/2020 22:24:38 - INFO - minGPT.mingpt.trainer -   test loss: 1.445410\n",
      "epoch 143 iter 2: train loss 1.49864. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.27it/s]\n",
      "11/12/2020 22:24:39 - INFO - minGPT.mingpt.trainer -   test loss: 1.469734\n",
      "epoch 144 iter 2: train loss 1.47778. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  5.45it/s]\n",
      "11/12/2020 22:24:39 - INFO - minGPT.mingpt.trainer -   test loss: 1.433300\n",
      "11/12/2020 22:24:39 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 145 iter 2: train loss 1.47840. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  5.60it/s]\n",
      "11/12/2020 22:24:40 - INFO - minGPT.mingpt.trainer -   test loss: 1.479576\n",
      "epoch 146 iter 2: train loss 1.48679. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:24:40 - INFO - minGPT.mingpt.trainer -   test loss: 1.434687\n",
      "epoch 147 iter 2: train loss 1.46899. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.51it/s]\n",
      "11/12/2020 22:24:41 - INFO - minGPT.mingpt.trainer -   test loss: 1.440749\n",
      "epoch 148 iter 2: train loss 1.47534. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:41 - INFO - minGPT.mingpt.trainer -   test loss: 1.444380\n",
      "epoch 149 iter 2: train loss 1.44563. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:24:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.410181\n",
      "11/12/2020 22:24:42 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 150 iter 2: train loss 1.48701. lr 3.000000e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:24:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.441294\n",
      "epoch 151 iter 2: train loss 1.45565. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.59it/s]\n",
      "11/12/2020 22:24:43 - INFO - minGPT.mingpt.trainer -   test loss: 1.404812\n",
      "11/12/2020 22:24:43 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 152 iter 2: train loss 1.42200. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.63it/s]\n",
      "11/12/2020 22:24:43 - INFO - minGPT.mingpt.trainer -   test loss: 1.413977\n",
      "epoch 153 iter 2: train loss 1.45231. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.401031\n",
      "11/12/2020 22:24:44 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 154 iter 2: train loss 1.43918. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:24:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.403021\n",
      "epoch 155 iter 2: train loss 1.41122. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:24:45 - INFO - minGPT.mingpt.trainer -   test loss: 1.398377\n",
      "11/12/2020 22:24:45 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 156 iter 2: train loss 1.41735. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.63it/s]\n",
      "11/12/2020 22:24:46 - INFO - minGPT.mingpt.trainer -   test loss: 1.437399\n",
      "epoch 157 iter 2: train loss 1.40361. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:46 - INFO - minGPT.mingpt.trainer -   test loss: 1.385309\n",
      "11/12/2020 22:24:46 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 158 iter 2: train loss 1.39090. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  5.89it/s]\n",
      "11/12/2020 22:24:47 - INFO - minGPT.mingpt.trainer -   test loss: 1.393536\n",
      "epoch 159 iter 2: train loss 1.41875. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:24:47 - INFO - minGPT.mingpt.trainer -   test loss: 1.397141\n",
      "epoch 160 iter 2: train loss 1.40929. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.60it/s]\n",
      "11/12/2020 22:24:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.391374\n",
      "epoch 161 iter 2: train loss 1.41895. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.23it/s]\n",
      "11/12/2020 22:24:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.383486\n",
      "11/12/2020 22:24:49 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 63/2333 = 2.70% correct\n",
      "final score: 3/583 = 0.51% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 162 iter 2: train loss 1.39447. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:24:49 - INFO - minGPT.mingpt.trainer -   test loss: 1.371096\n",
      "11/12/2020 22:24:49 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 163 iter 2: train loss 1.38296. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:24:50 - INFO - minGPT.mingpt.trainer -   test loss: 1.362366\n",
      "11/12/2020 22:24:50 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 164 iter 2: train loss 1.36628. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:50 - INFO - minGPT.mingpt.trainer -   test loss: 1.383081\n",
      "epoch 165 iter 2: train loss 1.37189. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "11/12/2020 22:24:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.353999\n",
      "11/12/2020 22:24:51 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 166 iter 2: train loss 1.37682. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.19it/s]\n",
      "11/12/2020 22:24:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.351456\n",
      "11/12/2020 22:24:51 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 167 iter 2: train loss 1.39960. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:24:52 - INFO - minGPT.mingpt.trainer -   test loss: 1.349805\n",
      "11/12/2020 22:24:52 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 168 iter 2: train loss 1.37815. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:52 - INFO - minGPT.mingpt.trainer -   test loss: 1.354340\n",
      "epoch 169 iter 2: train loss 1.38712. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.58it/s]\n",
      "11/12/2020 22:24:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.334023\n",
      "11/12/2020 22:24:53 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 170 iter 2: train loss 1.37204. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:24:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.336793\n",
      "epoch 171 iter 2: train loss 1.35633. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:24:54 - INFO - minGPT.mingpt.trainer -   test loss: 1.329976\n",
      "11/12/2020 22:24:54 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 172 iter 2: train loss 1.36635. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:24:54 - INFO - minGPT.mingpt.trainer -   test loss: 1.329918\n",
      "11/12/2020 22:24:54 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 173 iter 2: train loss 1.35777. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  5.90it/s]\n",
      "11/12/2020 22:24:55 - INFO - minGPT.mingpt.trainer -   test loss: 1.328625\n",
      "11/12/2020 22:24:55 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 174 iter 2: train loss 1.31876. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.29it/s]\n",
      "11/12/2020 22:24:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.319678\n",
      "11/12/2020 22:24:56 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 175 iter 2: train loss 1.36540. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:24:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.319577\n",
      "11/12/2020 22:24:56 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 176 iter 2: train loss 1.36308. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:24:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.322002\n",
      "epoch 177 iter 2: train loss 1.32682. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:24:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.315531\n",
      "11/12/2020 22:24:57 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 178 iter 2: train loss 1.34956. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:24:58 - INFO - minGPT.mingpt.trainer -   test loss: 1.314651\n",
      "11/12/2020 22:24:58 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 179 iter 2: train loss 1.34069. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "11/12/2020 22:24:58 - INFO - minGPT.mingpt.trainer -   test loss: 1.319052\n",
      "epoch 180 iter 2: train loss 1.38130. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:24:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.315530\n",
      "epoch 181 iter 2: train loss 1.35633. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:24:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.312203\n",
      "11/12/2020 22:25:00 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 99/2333 = 4.24% correct\n",
      "final score: 9/583 = 1.54% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 182 iter 2: train loss 1.35504. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:25:00 - INFO - minGPT.mingpt.trainer -   test loss: 1.312045\n",
      "11/12/2020 22:25:00 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 183 iter 2: train loss 1.32153. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:01 - INFO - minGPT.mingpt.trainer -   test loss: 1.312091\n",
      "epoch 184 iter 2: train loss 1.35484. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.52it/s]\n",
      "11/12/2020 22:25:01 - INFO - minGPT.mingpt.trainer -   test loss: 1.309867\n",
      "11/12/2020 22:25:01 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 185 iter 2: train loss 1.34575. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.93it/s]\n",
      "11/12/2020 22:25:02 - INFO - minGPT.mingpt.trainer -   test loss: 1.307514\n",
      "11/12/2020 22:25:02 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 186 iter 2: train loss 1.34369. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:02 - INFO - minGPT.mingpt.trainer -   test loss: 1.308166\n",
      "epoch 187 iter 2: train loss 1.31337. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:25:03 - INFO - minGPT.mingpt.trainer -   test loss: 1.307782\n",
      "epoch 188 iter 2: train loss 1.35215. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.85it/s]\n",
      "11/12/2020 22:25:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.311819\n",
      "epoch 189 iter 2: train loss 1.33662. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:25:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.308097\n",
      "epoch 190 iter 2: train loss 1.36129. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.54it/s]\n",
      "11/12/2020 22:25:05 - INFO - minGPT.mingpt.trainer -   test loss: 1.305384\n",
      "11/12/2020 22:25:05 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 191 iter 2: train loss 1.32816. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:25:05 - INFO - minGPT.mingpt.trainer -   test loss: 1.307133\n",
      "epoch 192 iter 2: train loss 1.34512. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.306324\n",
      "epoch 193 iter 2: train loss 1.33521. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.302822\n",
      "11/12/2020 22:25:06 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 194 iter 2: train loss 1.32236. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.303945\n",
      "epoch 195 iter 2: train loss 1.34063. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:25:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.301398\n",
      "11/12/2020 22:25:07 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 196 iter 2: train loss 1.34136. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.302556\n",
      "epoch 197 iter 2: train loss 1.35066. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.60it/s]\n",
      "11/12/2020 22:25:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.302641\n",
      "epoch 198 iter 2: train loss 1.34005. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:25:09 - INFO - minGPT.mingpt.trainer -   test loss: 1.299559\n",
      "11/12/2020 22:25:09 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 199 iter 2: train loss 1.29517. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:09 - INFO - minGPT.mingpt.trainer -   test loss: 1.297936\n",
      "11/12/2020 22:25:09 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 200 iter 2: train loss 1.32248. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:25:10 - INFO - minGPT.mingpt.trainer -   test loss: 1.316080\n",
      "epoch 201 iter 2: train loss 1.33733. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:25:10 - INFO - minGPT.mingpt.trainer -   test loss: 1.300691\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 109/2333 = 4.67% correct\n",
      "final score: 7/583 = 1.20% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 202 iter 2: train loss 1.35878. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:25:11 - INFO - minGPT.mingpt.trainer -   test loss: 1.305374\n",
      "epoch 203 iter 2: train loss 1.33971. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:25:12 - INFO - minGPT.mingpt.trainer -   test loss: 1.296928\n",
      "11/12/2020 22:25:12 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 204 iter 2: train loss 1.33267. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:12 - INFO - minGPT.mingpt.trainer -   test loss: 1.298494\n",
      "epoch 205 iter 2: train loss 1.32430. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:25:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.298942\n",
      "epoch 206 iter 2: train loss 1.32644. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:25:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.301240\n",
      "epoch 207 iter 2: train loss 1.32090. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  5.88it/s]\n",
      "11/12/2020 22:25:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.299058\n",
      "epoch 208 iter 2: train loss 1.34440. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:25:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.293183\n",
      "11/12/2020 22:25:14 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 209 iter 2: train loss 1.30219. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:25:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.293024\n",
      "11/12/2020 22:25:15 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 210 iter 2: train loss 1.34590. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.298293\n",
      "epoch 211 iter 2: train loss 1.33714. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:25:16 - INFO - minGPT.mingpt.trainer -   test loss: 1.304447\n",
      "epoch 212 iter 2: train loss 1.33095. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:16 - INFO - minGPT.mingpt.trainer -   test loss: 1.292125\n",
      "11/12/2020 22:25:16 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 213 iter 2: train loss 1.32223. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:25:17 - INFO - minGPT.mingpt.trainer -   test loss: 1.279217\n",
      "11/12/2020 22:25:17 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 214 iter 2: train loss 1.29977. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:25:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.305839\n",
      "epoch 215 iter 2: train loss 1.33275. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.305218\n",
      "epoch 216 iter 2: train loss 1.33991. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:25:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.292969\n",
      "epoch 217 iter 2: train loss 1.32400. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:25:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.280551\n",
      "epoch 218 iter 2: train loss 1.31784. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:25:20 - INFO - minGPT.mingpt.trainer -   test loss: 1.287890\n",
      "epoch 219 iter 2: train loss 1.30620. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:20 - INFO - minGPT.mingpt.trainer -   test loss: 1.289279\n",
      "epoch 220 iter 2: train loss 1.31825. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.283169\n",
      "epoch 221 iter 2: train loss 1.32192. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:25:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.282600\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 86/2333 = 3.69% correct\n",
      "final score: 6/583 = 1.03% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 222 iter 2: train loss 1.30737. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.51it/s]\n",
      "11/12/2020 22:25:22 - INFO - minGPT.mingpt.trainer -   test loss: 1.271666\n",
      "11/12/2020 22:25:22 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 223 iter 2: train loss 1.30732. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  5.86it/s]\n",
      "11/12/2020 22:25:23 - INFO - minGPT.mingpt.trainer -   test loss: 1.287003\n",
      "epoch 224 iter 2: train loss 1.32323. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.27it/s]\n",
      "11/12/2020 22:25:23 - INFO - minGPT.mingpt.trainer -   test loss: 1.295130\n",
      "epoch 225 iter 2: train loss 1.28263. lr 3.000000e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:25:24 - INFO - minGPT.mingpt.trainer -   test loss: 1.284902\n",
      "epoch 226 iter 2: train loss 1.31343. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:25:24 - INFO - minGPT.mingpt.trainer -   test loss: 1.282009\n",
      "epoch 227 iter 2: train loss 1.29717. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.278242\n",
      "epoch 228 iter 2: train loss 1.31106. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:25:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.291743\n",
      "epoch 229 iter 2: train loss 1.29867. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.273293\n",
      "epoch 230 iter 2: train loss 1.32040. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:25:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.265737\n",
      "11/12/2020 22:25:26 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 231 iter 2: train loss 1.27229. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:25:27 - INFO - minGPT.mingpt.trainer -   test loss: 1.263103\n",
      "11/12/2020 22:25:27 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 232 iter 2: train loss 1.28141. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:25:27 - INFO - minGPT.mingpt.trainer -   test loss: 1.264578\n",
      "epoch 233 iter 2: train loss 1.27138. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:25:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.257066\n",
      "11/12/2020 22:25:28 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 234 iter 2: train loss 1.27226. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.27it/s]\n",
      "11/12/2020 22:25:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.269738\n",
      "epoch 235 iter 2: train loss 1.33054. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s]\n",
      "11/12/2020 22:25:29 - INFO - minGPT.mingpt.trainer -   test loss: 1.293595\n",
      "epoch 236 iter 2: train loss 1.27743. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.19it/s]\n",
      "11/12/2020 22:25:29 - INFO - minGPT.mingpt.trainer -   test loss: 1.258007\n",
      "epoch 237 iter 2: train loss 1.26638. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:25:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.245882\n",
      "11/12/2020 22:25:30 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 238 iter 2: train loss 1.28579. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:25:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.261095\n",
      "epoch 239 iter 2: train loss 1.25500. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:31 - INFO - minGPT.mingpt.trainer -   test loss: 1.255889\n",
      "epoch 240 iter 2: train loss 1.25226. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:25:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.247908\n",
      "epoch 241 iter 2: train loss 1.26164. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  5.77it/s]\n",
      "11/12/2020 22:25:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.246068\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 112/2333 = 4.80% correct\n",
      "final score: 6/583 = 1.03% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 242 iter 2: train loss 1.27587. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:25:33 - INFO - minGPT.mingpt.trainer -   test loss: 1.248540\n",
      "epoch 243 iter 2: train loss 1.25645. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:34 - INFO - minGPT.mingpt.trainer -   test loss: 1.237594\n",
      "11/12/2020 22:25:34 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 244 iter 2: train loss 1.25846. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.32it/s]\n",
      "11/12/2020 22:25:34 - INFO - minGPT.mingpt.trainer -   test loss: 1.222584\n",
      "11/12/2020 22:25:34 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 245 iter 2: train loss 1.26047. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:35 - INFO - minGPT.mingpt.trainer -   test loss: 1.235701\n",
      "epoch 246 iter 2: train loss 1.24787. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:35 - INFO - minGPT.mingpt.trainer -   test loss: 1.234381\n",
      "epoch 247 iter 2: train loss 1.27686. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:25:36 - INFO - minGPT.mingpt.trainer -   test loss: 1.234370\n",
      "epoch 248 iter 2: train loss 1.23560. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.61it/s]\n",
      "11/12/2020 22:25:36 - INFO - minGPT.mingpt.trainer -   test loss: 1.226806\n",
      "epoch 249 iter 2: train loss 1.25376. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s]\n",
      "11/12/2020 22:25:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.224644\n",
      "epoch 250 iter 2: train loss 1.25602. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.51it/s]\n",
      "11/12/2020 22:25:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.217310\n",
      "11/12/2020 22:25:37 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 251 iter 2: train loss 1.23617. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:25:38 - INFO - minGPT.mingpt.trainer -   test loss: 1.213200\n",
      "11/12/2020 22:25:38 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 252 iter 2: train loss 1.24009. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:25:38 - INFO - minGPT.mingpt.trainer -   test loss: 1.219662\n",
      "epoch 253 iter 2: train loss 1.24765. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:25:39 - INFO - minGPT.mingpt.trainer -   test loss: 1.215810\n",
      "epoch 254 iter 2: train loss 1.25050. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:25:39 - INFO - minGPT.mingpt.trainer -   test loss: 1.218227\n",
      "epoch 255 iter 2: train loss 1.25613. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:25:40 - INFO - minGPT.mingpt.trainer -   test loss: 1.216014\n",
      "epoch 256 iter 2: train loss 1.22110. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.05it/s]\n",
      "11/12/2020 22:25:40 - INFO - minGPT.mingpt.trainer -   test loss: 1.215863\n",
      "epoch 257 iter 2: train loss 1.21419. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:25:41 - INFO - minGPT.mingpt.trainer -   test loss: 1.220513\n",
      "epoch 258 iter 2: train loss 1.22856. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:25:41 - INFO - minGPT.mingpt.trainer -   test loss: 1.218611\n",
      "epoch 259 iter 2: train loss 1.23567. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:25:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.215467\n",
      "epoch 260 iter 2: train loss 1.25216. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:25:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.218081\n",
      "epoch 261 iter 2: train loss 1.22108. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:43 - INFO - minGPT.mingpt.trainer -   test loss: 1.214675\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 134/2333 = 5.74% correct\n",
      "final score: 8/583 = 1.37% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 262 iter 2: train loss 1.23671. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:25:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.212020\n",
      "11/12/2020 22:25:44 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 263 iter 2: train loss 1.23952. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:25:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.211789\n",
      "11/12/2020 22:25:44 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 264 iter 2: train loss 1.22654. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:45 - INFO - minGPT.mingpt.trainer -   test loss: 1.209665\n",
      "11/12/2020 22:25:45 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 265 iter 2: train loss 1.22901. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:25:45 - INFO - minGPT.mingpt.trainer -   test loss: 1.209409\n",
      "11/12/2020 22:25:45 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 266 iter 2: train loss 1.23261. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:46 - INFO - minGPT.mingpt.trainer -   test loss: 1.211278\n",
      "epoch 267 iter 2: train loss 1.25487. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:25:46 - INFO - minGPT.mingpt.trainer -   test loss: 1.208855\n",
      "11/12/2020 22:25:46 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 268 iter 2: train loss 1.21227. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:47 - INFO - minGPT.mingpt.trainer -   test loss: 1.208969\n",
      "epoch 269 iter 2: train loss 1.21752. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.212411\n",
      "epoch 270 iter 2: train loss 1.21658. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.86it/s]\n",
      "11/12/2020 22:25:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.209904\n",
      "epoch 271 iter 2: train loss 1.22246. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  5.47it/s]\n",
      "11/12/2020 22:25:49 - INFO - minGPT.mingpt.trainer -   test loss: 1.212487\n",
      "epoch 272 iter 2: train loss 1.23136. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  5.29it/s]\n",
      "11/12/2020 22:25:49 - INFO - minGPT.mingpt.trainer -   test loss: 1.213735\n",
      "epoch 273 iter 2: train loss 1.23912. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:25:50 - INFO - minGPT.mingpt.trainer -   test loss: 1.210006\n",
      "epoch 274 iter 2: train loss 1.18904. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:25:50 - INFO - minGPT.mingpt.trainer -   test loss: 1.208784\n",
      "11/12/2020 22:25:50 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 275 iter 2: train loss 1.22572. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:25:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.211866\n",
      "epoch 276 iter 2: train loss 1.23196. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.28it/s]\n",
      "11/12/2020 22:25:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.214691\n",
      "epoch 277 iter 2: train loss 1.20873. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:52 - INFO - minGPT.mingpt.trainer -   test loss: 1.215006\n",
      "epoch 278 iter 2: train loss 1.22998. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.59it/s]\n",
      "11/12/2020 22:25:52 - INFO - minGPT.mingpt.trainer -   test loss: 1.221364\n",
      "epoch 279 iter 2: train loss 1.20439. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:25:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.210513\n",
      "epoch 280 iter 2: train loss 1.27178. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:25:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.234139\n",
      "epoch 281 iter 2: train loss 1.21829. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s]\n",
      "11/12/2020 22:25:54 - INFO - minGPT.mingpt.trainer -   test loss: 1.218159\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 101/2333 = 4.33% correct\n",
      "final score: 9/583 = 1.54% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 282 iter 2: train loss 1.21987. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:55 - INFO - minGPT.mingpt.trainer -   test loss: 1.234465\n",
      "epoch 283 iter 2: train loss 1.20413. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:25:55 - INFO - minGPT.mingpt.trainer -   test loss: 1.209336\n",
      "epoch 284 iter 2: train loss 1.22751. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.65it/s]\n",
      "11/12/2020 22:25:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.229955\n",
      "epoch 285 iter 2: train loss 1.23041. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:25:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.212913\n",
      "epoch 286 iter 2: train loss 1.23847. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:25:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.214017\n",
      "epoch 287 iter 2: train loss 1.22970. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:25:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.219243\n",
      "epoch 288 iter 2: train loss 1.23417. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.21it/s]\n",
      "11/12/2020 22:25:58 - INFO - minGPT.mingpt.trainer -   test loss: 1.230620\n",
      "epoch 289 iter 2: train loss 1.24793. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:25:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.219512\n",
      "epoch 290 iter 2: train loss 1.20948. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  5.96it/s]\n",
      "11/12/2020 22:25:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.207818\n",
      "11/12/2020 22:25:59 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 291 iter 2: train loss 1.24058. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:26:00 - INFO - minGPT.mingpt.trainer -   test loss: 1.214964\n",
      "epoch 292 iter 2: train loss 1.22537. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:26:00 - INFO - minGPT.mingpt.trainer -   test loss: 1.214059\n",
      "epoch 293 iter 2: train loss 1.23387. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:26:01 - INFO - minGPT.mingpt.trainer -   test loss: 1.227079\n",
      "epoch 294 iter 2: train loss 1.23552. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:01 - INFO - minGPT.mingpt.trainer -   test loss: 1.223785\n",
      "epoch 295 iter 2: train loss 1.25510. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:26:02 - INFO - minGPT.mingpt.trainer -   test loss: 1.233329\n",
      "epoch 296 iter 2: train loss 1.28719. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:26:02 - INFO - minGPT.mingpt.trainer -   test loss: 1.206582\n",
      "11/12/2020 22:26:02 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 297 iter 2: train loss 1.23088. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:26:03 - INFO - minGPT.mingpt.trainer -   test loss: 1.217039\n",
      "epoch 298 iter 2: train loss 1.20209. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:26:03 - INFO - minGPT.mingpt.trainer -   test loss: 1.213586\n",
      "epoch 299 iter 2: train loss 1.23093. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.29it/s]\n",
      "11/12/2020 22:26:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.206385\n",
      "11/12/2020 22:26:04 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 300 iter 2: train loss 1.20024. lr 3.000000e-04: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:26:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.208000\n",
      "epoch 301 iter 2: train loss 1.21729. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.21it/s]\n",
      "11/12/2020 22:26:05 - INFO - minGPT.mingpt.trainer -   test loss: 1.197358\n",
      "11/12/2020 22:26:05 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 117/2333 = 5.02% correct\n",
      "final score: 13/583 = 2.23% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 302 iter 2: train loss 1.21445. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:26:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.217766\n",
      "epoch 303 iter 2: train loss 1.20480. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.208963\n",
      "epoch 304 iter 2: train loss 1.21461. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:26:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.249311\n",
      "epoch 305 iter 2: train loss 1.23285. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.06it/s]\n",
      "11/12/2020 22:26:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.203547\n",
      "epoch 306 iter 2: train loss 1.22115. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.208102\n",
      "epoch 307 iter 2: train loss 1.22065. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:26:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.201190\n",
      "epoch 308 iter 2: train loss 1.21367. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:09 - INFO - minGPT.mingpt.trainer -   test loss: 1.224880\n",
      "epoch 309 iter 2: train loss 1.22249. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:26:09 - INFO - minGPT.mingpt.trainer -   test loss: 1.199797\n",
      "epoch 310 iter 2: train loss 1.17387. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:26:10 - INFO - minGPT.mingpt.trainer -   test loss: 1.189179\n",
      "11/12/2020 22:26:10 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 311 iter 2: train loss 1.20691. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:26:11 - INFO - minGPT.mingpt.trainer -   test loss: 1.191726\n",
      "epoch 312 iter 2: train loss 1.20397. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:26:11 - INFO - minGPT.mingpt.trainer -   test loss: 1.200963\n",
      "epoch 313 iter 2: train loss 1.23718. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:26:12 - INFO - minGPT.mingpt.trainer -   test loss: 1.196544\n",
      "epoch 314 iter 2: train loss 1.20262. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:26:12 - INFO - minGPT.mingpt.trainer -   test loss: 1.198594\n",
      "epoch 315 iter 2: train loss 1.19974. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:26:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.194691\n",
      "epoch 316 iter 2: train loss 1.17863. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.180815\n",
      "11/12/2020 22:26:13 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 317 iter 2: train loss 1.18844. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.184642\n",
      "epoch 318 iter 2: train loss 1.16009. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.182859\n",
      "epoch 319 iter 2: train loss 1.20675. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.196574\n",
      "epoch 320 iter 2: train loss 1.19100. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:26:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.189104\n",
      "epoch 321 iter 2: train loss 1.20318. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:16 - INFO - minGPT.mingpt.trainer -   test loss: 1.174292\n",
      "11/12/2020 22:26:16 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 130/2333 = 5.57% correct\n",
      "final score: 12/583 = 2.06% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 322 iter 2: train loss 1.18005. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:26:17 - INFO - minGPT.mingpt.trainer -   test loss: 1.185714\n",
      "epoch 323 iter 2: train loss 1.18685. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:26:17 - INFO - minGPT.mingpt.trainer -   test loss: 1.175491\n",
      "epoch 324 iter 2: train loss 1.16534. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:26:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.173310\n",
      "11/12/2020 22:26:18 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 325 iter 2: train loss 1.14855. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:26:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.167332\n",
      "11/12/2020 22:26:18 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 326 iter 2: train loss 1.17090. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.66it/s]\n",
      "11/12/2020 22:26:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.176183\n",
      "epoch 327 iter 2: train loss 1.15174. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:26:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.177291\n",
      "epoch 328 iter 2: train loss 1.17493. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:26:20 - INFO - minGPT.mingpt.trainer -   test loss: 1.163831\n",
      "11/12/2020 22:26:20 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 329 iter 2: train loss 1.13926. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:26:20 - INFO - minGPT.mingpt.trainer -   test loss: 1.159283\n",
      "11/12/2020 22:26:20 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 330 iter 2: train loss 1.16254. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:26:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.164281\n",
      "epoch 331 iter 2: train loss 1.16127. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:26:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.165252\n",
      "epoch 332 iter 2: train loss 1.14177. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:22 - INFO - minGPT.mingpt.trainer -   test loss: 1.164372\n",
      "epoch 333 iter 2: train loss 1.16328. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.58it/s]\n",
      "11/12/2020 22:26:22 - INFO - minGPT.mingpt.trainer -   test loss: 1.162969\n",
      "epoch 334 iter 2: train loss 1.16901. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:23 - INFO - minGPT.mingpt.trainer -   test loss: 1.164750\n",
      "epoch 335 iter 2: train loss 1.19798. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:23 - INFO - minGPT.mingpt.trainer -   test loss: 1.163459\n",
      "epoch 336 iter 2: train loss 1.15569. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.00it/s]\n",
      "11/12/2020 22:26:24 - INFO - minGPT.mingpt.trainer -   test loss: 1.162608\n",
      "epoch 337 iter 2: train loss 1.17426. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.51it/s]\n",
      "11/12/2020 22:26:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.159162\n",
      "11/12/2020 22:26:25 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 338 iter 2: train loss 1.14936. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.75it/s]\n",
      "11/12/2020 22:26:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.160345\n",
      "epoch 339 iter 2: train loss 1.18156. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  5.49it/s]\n",
      "11/12/2020 22:26:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.164371\n",
      "epoch 340 iter 2: train loss 1.13271. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.12it/s]\n",
      "11/12/2020 22:26:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.163777\n",
      "epoch 341 iter 2: train loss 1.13415. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:26:27 - INFO - minGPT.mingpt.trainer -   test loss: 1.160270\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 173/2333 = 7.42% correct\n",
      "final score: 13/583 = 2.23% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 342 iter 2: train loss 1.14934. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.20it/s]\n",
      "11/12/2020 22:26:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.161005\n",
      "epoch 343 iter 2: train loss 1.16569. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:26:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.161404\n",
      "epoch 344 iter 2: train loss 1.16056. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.62it/s]\n",
      "11/12/2020 22:26:29 - INFO - minGPT.mingpt.trainer -   test loss: 1.163353\n",
      "epoch 345 iter 2: train loss 1.18048. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:26:29 - INFO - minGPT.mingpt.trainer -   test loss: 1.164088\n",
      "epoch 346 iter 2: train loss 1.12876. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "11/12/2020 22:26:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.164714\n",
      "epoch 347 iter 2: train loss 1.18554. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.34it/s]\n",
      "11/12/2020 22:26:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.164745\n",
      "epoch 348 iter 2: train loss 1.16006. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:31 - INFO - minGPT.mingpt.trainer -   test loss: 1.170785\n",
      "epoch 349 iter 2: train loss 1.12840. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:26:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.162940\n",
      "epoch 350 iter 2: train loss 1.13969. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.158340\n",
      "11/12/2020 22:26:32 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 351 iter 2: train loss 1.16163. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.31it/s]\n",
      "11/12/2020 22:26:33 - INFO - minGPT.mingpt.trainer -   test loss: 1.159481\n",
      "epoch 352 iter 2: train loss 1.19084. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:26:33 - INFO - minGPT.mingpt.trainer -   test loss: 1.172071\n",
      "epoch 353 iter 2: train loss 1.18501. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  5.99it/s]\n",
      "11/12/2020 22:26:34 - INFO - minGPT.mingpt.trainer -   test loss: 1.163964\n",
      "epoch 354 iter 2: train loss 1.18191. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  5.55it/s]\n",
      "11/12/2020 22:26:34 - INFO - minGPT.mingpt.trainer -   test loss: 1.175558\n",
      "epoch 355 iter 2: train loss 1.18152. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  5.21it/s]\n",
      "11/12/2020 22:26:35 - INFO - minGPT.mingpt.trainer -   test loss: 1.163942\n",
      "epoch 356 iter 2: train loss 1.15462. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.03it/s]\n",
      "11/12/2020 22:26:35 - INFO - minGPT.mingpt.trainer -   test loss: 1.160668\n",
      "epoch 357 iter 2: train loss 1.16458. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  5.89it/s]\n",
      "11/12/2020 22:26:36 - INFO - minGPT.mingpt.trainer -   test loss: 1.178718\n",
      "epoch 358 iter 2: train loss 1.13867. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.160058\n",
      "epoch 359 iter 2: train loss 1.15054. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.161947\n",
      "epoch 360 iter 2: train loss 1.17478. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  5.80it/s]\n",
      "11/12/2020 22:26:38 - INFO - minGPT.mingpt.trainer -   test loss: 1.170373\n",
      "epoch 361 iter 2: train loss 1.15155. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  5.67it/s]\n",
      "11/12/2020 22:26:38 - INFO - minGPT.mingpt.trainer -   test loss: 1.173499\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 165/2333 = 7.07% correct\n",
      "final score: 11/583 = 1.89% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 362 iter 2: train loss 1.14487. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  5.58it/s]\n",
      "11/12/2020 22:26:39 - INFO - minGPT.mingpt.trainer -   test loss: 1.175518\n",
      "epoch 363 iter 2: train loss 1.17576. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  5.92it/s]\n",
      "11/12/2020 22:26:40 - INFO - minGPT.mingpt.trainer -   test loss: 1.175099\n",
      "epoch 364 iter 2: train loss 1.15596. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:26:40 - INFO - minGPT.mingpt.trainer -   test loss: 1.188699\n",
      "epoch 365 iter 2: train loss 1.17887. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.05it/s]\n",
      "11/12/2020 22:26:41 - INFO - minGPT.mingpt.trainer -   test loss: 1.189256\n",
      "epoch 366 iter 2: train loss 1.20229. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  5.51it/s]\n",
      "11/12/2020 22:26:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.193600\n",
      "epoch 367 iter 2: train loss 1.12915. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.00it/s]\n",
      "11/12/2020 22:26:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.181436\n",
      "epoch 368 iter 2: train loss 1.14054. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:26:43 - INFO - minGPT.mingpt.trainer -   test loss: 1.178450\n",
      "epoch 369 iter 2: train loss 1.16205. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.61it/s]\n",
      "11/12/2020 22:26:43 - INFO - minGPT.mingpt.trainer -   test loss: 1.177580\n",
      "epoch 370 iter 2: train loss 1.14035. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  5.87it/s]\n",
      "11/12/2020 22:26:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.180076\n",
      "epoch 371 iter 2: train loss 1.16046. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.30it/s]\n",
      "11/12/2020 22:26:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.163508\n",
      "epoch 372 iter 2: train loss 1.20232. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.57it/s]\n",
      "11/12/2020 22:26:45 - INFO - minGPT.mingpt.trainer -   test loss: 1.192947\n",
      "epoch 373 iter 2: train loss 1.17014. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.24it/s]\n",
      "11/12/2020 22:26:45 - INFO - minGPT.mingpt.trainer -   test loss: 1.168550\n",
      "epoch 374 iter 2: train loss 1.16320. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.62it/s]\n",
      "11/12/2020 22:26:46 - INFO - minGPT.mingpt.trainer -   test loss: 1.153909\n",
      "11/12/2020 22:26:46 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 375 iter 2: train loss 1.16540. lr 3.000000e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:26:46 - INFO - minGPT.mingpt.trainer -   test loss: 1.162344\n",
      "epoch 376 iter 2: train loss 1.19092. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:26:47 - INFO - minGPT.mingpt.trainer -   test loss: 1.158852\n",
      "epoch 377 iter 2: train loss 1.14384. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:26:47 - INFO - minGPT.mingpt.trainer -   test loss: 1.181932\n",
      "epoch 378 iter 2: train loss 1.14720. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:26:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.161280\n",
      "epoch 379 iter 2: train loss 1.16658. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:26:48 - INFO - minGPT.mingpt.trainer -   test loss: 1.160866\n",
      "epoch 380 iter 2: train loss 1.15064. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:26:49 - INFO - minGPT.mingpt.trainer -   test loss: 1.171562\n",
      "epoch 381 iter 2: train loss 1.15177. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.63it/s]\n",
      "11/12/2020 22:26:49 - INFO - minGPT.mingpt.trainer -   test loss: 1.171576\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 162/2333 = 6.94% correct\n",
      "final score: 7/583 = 1.20% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 382 iter 2: train loss 1.16670. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:26:50 - INFO - minGPT.mingpt.trainer -   test loss: 1.175828\n",
      "epoch 383 iter 2: train loss 1.16933. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:26:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.184990\n",
      "epoch 384 iter 2: train loss 1.09083. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:26:51 - INFO - minGPT.mingpt.trainer -   test loss: 1.161512\n",
      "epoch 385 iter 2: train loss 1.15724. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:26:52 - INFO - minGPT.mingpt.trainer -   test loss: 1.139747\n",
      "11/12/2020 22:26:52 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 386 iter 2: train loss 1.15868. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:26:52 - INFO - minGPT.mingpt.trainer -   test loss: 1.164320\n",
      "epoch 387 iter 2: train loss 1.10970. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  5.76it/s]\n",
      "11/12/2020 22:26:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.162164\n",
      "epoch 388 iter 2: train loss 1.13433. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.59it/s]\n",
      "11/12/2020 22:26:53 - INFO - minGPT.mingpt.trainer -   test loss: 1.162946\n",
      "epoch 389 iter 2: train loss 1.14048. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:26:54 - INFO - minGPT.mingpt.trainer -   test loss: 1.164405\n",
      "epoch 390 iter 2: train loss 1.14510. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:26:55 - INFO - minGPT.mingpt.trainer -   test loss: 1.152142\n",
      "epoch 391 iter 2: train loss 1.11165. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:26:55 - INFO - minGPT.mingpt.trainer -   test loss: 1.153352\n",
      "epoch 392 iter 2: train loss 1.15226. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:26:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.164795\n",
      "epoch 393 iter 2: train loss 1.12553. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:26:56 - INFO - minGPT.mingpt.trainer -   test loss: 1.154252\n",
      "epoch 394 iter 2: train loss 1.12874. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:26:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.138095\n",
      "11/12/2020 22:26:57 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 395 iter 2: train loss 1.11552. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "11/12/2020 22:26:57 - INFO - minGPT.mingpt.trainer -   test loss: 1.150459\n",
      "epoch 396 iter 2: train loss 1.12627. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:26:58 - INFO - minGPT.mingpt.trainer -   test loss: 1.143491\n",
      "epoch 397 iter 2: train loss 1.11526. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s]\n",
      "11/12/2020 22:26:58 - INFO - minGPT.mingpt.trainer -   test loss: 1.143248\n",
      "epoch 398 iter 2: train loss 1.11727. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:26:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.132328\n",
      "11/12/2020 22:26:59 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 399 iter 2: train loss 1.09192. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.16it/s]\n",
      "11/12/2020 22:26:59 - INFO - minGPT.mingpt.trainer -   test loss: 1.124518\n",
      "11/12/2020 22:26:59 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 400 iter 2: train loss 1.08692. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.24it/s]\n",
      "11/12/2020 22:27:00 - INFO - minGPT.mingpt.trainer -   test loss: 1.123310\n",
      "11/12/2020 22:27:00 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 401 iter 2: train loss 1.10683. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "11/12/2020 22:27:00 - INFO - minGPT.mingpt.trainer -   test loss: 1.135501\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 219/2333 = 9.39% correct\n",
      "final score: 13/583 = 2.23% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 402 iter 2: train loss 1.13490. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.10it/s]\n",
      "11/12/2020 22:27:01 - INFO - minGPT.mingpt.trainer -   test loss: 1.130997\n",
      "epoch 403 iter 2: train loss 1.10006. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:27:02 - INFO - minGPT.mingpt.trainer -   test loss: 1.127344\n",
      "epoch 404 iter 2: train loss 1.09157. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.28it/s]\n",
      "11/12/2020 22:27:02 - INFO - minGPT.mingpt.trainer -   test loss: 1.126338\n",
      "epoch 405 iter 2: train loss 1.08073. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s]\n",
      "11/12/2020 22:27:03 - INFO - minGPT.mingpt.trainer -   test loss: 1.125237\n",
      "epoch 406 iter 2: train loss 1.08809. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.45it/s]\n",
      "11/12/2020 22:27:03 - INFO - minGPT.mingpt.trainer -   test loss: 1.125956\n",
      "epoch 407 iter 2: train loss 1.06729. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:27:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.123719\n",
      "epoch 408 iter 2: train loss 1.10335. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:27:04 - INFO - minGPT.mingpt.trainer -   test loss: 1.126645\n",
      "epoch 409 iter 2: train loss 1.09700. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.60it/s]\n",
      "11/12/2020 22:27:05 - INFO - minGPT.mingpt.trainer -   test loss: 1.126773\n",
      "epoch 410 iter 2: train loss 1.11739. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:27:05 - INFO - minGPT.mingpt.trainer -   test loss: 1.124290\n",
      "epoch 411 iter 2: train loss 1.08059. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:27:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.122887\n",
      "11/12/2020 22:27:06 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 412 iter 2: train loss 1.10526. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.62it/s]\n",
      "11/12/2020 22:27:06 - INFO - minGPT.mingpt.trainer -   test loss: 1.124284\n",
      "epoch 413 iter 2: train loss 1.08704. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:27:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.121893\n",
      "11/12/2020 22:27:07 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 414 iter 2: train loss 1.08147. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.42it/s]\n",
      "11/12/2020 22:27:07 - INFO - minGPT.mingpt.trainer -   test loss: 1.120769\n",
      "11/12/2020 22:27:07 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 415 iter 2: train loss 1.09411. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:27:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.120808\n",
      "epoch 416 iter 2: train loss 1.08710. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.56it/s]\n",
      "11/12/2020 22:27:08 - INFO - minGPT.mingpt.trainer -   test loss: 1.117078\n",
      "11/12/2020 22:27:08 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 417 iter 2: train loss 1.08050. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:27:09 - INFO - minGPT.mingpt.trainer -   test loss: 1.118711\n",
      "epoch 418 iter 2: train loss 1.08372. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:27:10 - INFO - minGPT.mingpt.trainer -   test loss: 1.120242\n",
      "epoch 419 iter 2: train loss 1.08051. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.04it/s]\n",
      "11/12/2020 22:27:10 - INFO - minGPT.mingpt.trainer -   test loss: 1.121360\n",
      "epoch 420 iter 2: train loss 1.08795. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "11/12/2020 22:27:11 - INFO - minGPT.mingpt.trainer -   test loss: 1.123124\n",
      "epoch 421 iter 2: train loss 1.10538. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.60it/s]\n",
      "11/12/2020 22:27:11 - INFO - minGPT.mingpt.trainer -   test loss: 1.120381\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 250/2333 = 10.72% correct\n",
      "final score: 17/583 = 2.92% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 422 iter 2: train loss 1.11921. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.21it/s]\n",
      "11/12/2020 22:27:12 - INFO - minGPT.mingpt.trainer -   test loss: 1.120996\n",
      "epoch 423 iter 2: train loss 1.06694. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.23it/s]\n",
      "11/12/2020 22:27:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.125630\n",
      "epoch 424 iter 2: train loss 1.07766. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:13 - INFO - minGPT.mingpt.trainer -   test loss: 1.124812\n",
      "epoch 425 iter 2: train loss 1.10743. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:27:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.118151\n",
      "epoch 426 iter 2: train loss 1.10557. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:27:14 - INFO - minGPT.mingpt.trainer -   test loss: 1.122246\n",
      "epoch 427 iter 2: train loss 1.07008. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:27:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.129633\n",
      "epoch 428 iter 2: train loss 1.07996. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:27:15 - INFO - minGPT.mingpt.trainer -   test loss: 1.129511\n",
      "epoch 429 iter 2: train loss 1.08956. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.32it/s]\n",
      "11/12/2020 22:27:16 - INFO - minGPT.mingpt.trainer -   test loss: 1.128394\n",
      "epoch 430 iter 2: train loss 1.08066. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:27:16 - INFO - minGPT.mingpt.trainer -   test loss: 1.123767\n",
      "epoch 431 iter 2: train loss 1.07206. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:27:17 - INFO - minGPT.mingpt.trainer -   test loss: 1.139333\n",
      "epoch 432 iter 2: train loss 1.11058. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.48it/s]\n",
      "11/12/2020 22:27:17 - INFO - minGPT.mingpt.trainer -   test loss: 1.133158\n",
      "epoch 433 iter 2: train loss 1.06668. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.123760\n",
      "epoch 434 iter 2: train loss 1.09318. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:18 - INFO - minGPT.mingpt.trainer -   test loss: 1.135299\n",
      "epoch 435 iter 2: train loss 1.08468. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  5.54it/s]\n",
      "11/12/2020 22:27:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.136208\n",
      "epoch 436 iter 2: train loss 1.10785. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  5.41it/s]\n",
      "11/12/2020 22:27:19 - INFO - minGPT.mingpt.trainer -   test loss: 1.140640\n",
      "epoch 437 iter 2: train loss 1.09459. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  5.08it/s]\n",
      "11/12/2020 22:27:20 - INFO - minGPT.mingpt.trainer -   test loss: 1.150895\n",
      "epoch 438 iter 2: train loss 1.11104. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  5.44it/s]\n",
      "11/12/2020 22:27:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.142321\n",
      "epoch 439 iter 2: train loss 1.09365. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  5.45it/s]\n",
      "11/12/2020 22:27:21 - INFO - minGPT.mingpt.trainer -   test loss: 1.138060\n",
      "epoch 440 iter 2: train loss 1.08438. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  5.41it/s]\n",
      "11/12/2020 22:27:22 - INFO - minGPT.mingpt.trainer -   test loss: 1.139410\n",
      "epoch 441 iter 2: train loss 1.07724. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  5.48it/s]\n",
      "11/12/2020 22:27:23 - INFO - minGPT.mingpt.trainer -   test loss: 1.129877\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 234/2333 = 10.03% correct\n",
      "final score: 12/583 = 2.06% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 442 iter 2: train loss 1.10624. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s]\n",
      "11/12/2020 22:27:24 - INFO - minGPT.mingpt.trainer -   test loss: 1.126918\n",
      "epoch 443 iter 2: train loss 1.11807. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  5.49it/s]\n",
      "11/12/2020 22:27:24 - INFO - minGPT.mingpt.trainer -   test loss: 1.129481\n",
      "epoch 444 iter 2: train loss 1.09457. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  5.64it/s]\n",
      "11/12/2020 22:27:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.134041\n",
      "epoch 445 iter 2: train loss 1.08715. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.57it/s]\n",
      "11/12/2020 22:27:25 - INFO - minGPT.mingpt.trainer -   test loss: 1.138533\n",
      "epoch 446 iter 2: train loss 1.13210. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]\n",
      "11/12/2020 22:27:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.147942\n",
      "epoch 447 iter 2: train loss 1.10229. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:27:26 - INFO - minGPT.mingpt.trainer -   test loss: 1.141530\n",
      "epoch 448 iter 2: train loss 1.12272. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  6.59it/s]\n",
      "11/12/2020 22:27:27 - INFO - minGPT.mingpt.trainer -   test loss: 1.157137\n",
      "epoch 449 iter 2: train loss 1.10720. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:27:27 - INFO - minGPT.mingpt.trainer -   test loss: 1.158672\n",
      "epoch 450 iter 2: train loss 1.11056. lr 3.000000e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:27:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.153955\n",
      "epoch 451 iter 2: train loss 1.11842. lr 2.994739e-04: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:27:28 - INFO - minGPT.mingpt.trainer -   test loss: 1.138055\n",
      "epoch 452 iter 2: train loss 1.09499. lr 2.978994e-04: 100%|██████████| 3/3 [00:00<00:00,  5.97it/s]\n",
      "11/12/2020 22:27:29 - INFO - minGPT.mingpt.trainer -   test loss: 1.168740\n",
      "epoch 453 iter 2: train loss 1.10296. lr 2.952875e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.132787\n",
      "epoch 454 iter 2: train loss 1.08623. lr 2.916565e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:30 - INFO - minGPT.mingpt.trainer -   test loss: 1.145445\n",
      "epoch 455 iter 2: train loss 1.09341. lr 2.870318e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:27:31 - INFO - minGPT.mingpt.trainer -   test loss: 1.133439\n",
      "epoch 456 iter 2: train loss 1.10533. lr 2.814460e-04: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s]\n",
      "11/12/2020 22:27:31 - INFO - minGPT.mingpt.trainer -   test loss: 1.139421\n",
      "epoch 457 iter 2: train loss 1.11818. lr 2.749382e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.135526\n",
      "epoch 458 iter 2: train loss 1.09706. lr 2.675540e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:27:32 - INFO - minGPT.mingpt.trainer -   test loss: 1.137405\n",
      "epoch 459 iter 2: train loss 1.07828. lr 2.593453e-04: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:33 - INFO - minGPT.mingpt.trainer -   test loss: 1.123019\n",
      "epoch 460 iter 2: train loss 1.07729. lr 2.503696e-04: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s]\n",
      "11/12/2020 22:27:33 - INFO - minGPT.mingpt.trainer -   test loss: 1.119905\n",
      "epoch 461 iter 2: train loss 1.08386. lr 2.406899e-04: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:27:34 - INFO - minGPT.mingpt.trainer -   test loss: 1.120929\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 311/2333 = 13.33% correct\n",
      "final score: 22/583 = 3.77% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 462 iter 2: train loss 1.05749. lr 2.303740e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:27:35 - INFO - minGPT.mingpt.trainer -   test loss: 1.114174\n",
      "11/12/2020 22:27:35 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 463 iter 2: train loss 1.07924. lr 2.194944e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:27:35 - INFO - minGPT.mingpt.trainer -   test loss: 1.124339\n",
      "epoch 464 iter 2: train loss 1.06731. lr 2.081273e-04: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n",
      "11/12/2020 22:27:36 - INFO - minGPT.mingpt.trainer -   test loss: 1.105644\n",
      "11/12/2020 22:27:36 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 465 iter 2: train loss 1.05559. lr 1.963525e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:27:36 - INFO - minGPT.mingpt.trainer -   test loss: 1.102253\n",
      "11/12/2020 22:27:36 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 466 iter 2: train loss 1.03085. lr 1.842526e-04: 100%|██████████| 3/3 [00:00<00:00,  6.19it/s]\n",
      "11/12/2020 22:27:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.112749\n",
      "epoch 467 iter 2: train loss 1.07601. lr 1.719125e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:27:37 - INFO - minGPT.mingpt.trainer -   test loss: 1.122997\n",
      "epoch 468 iter 2: train loss 1.06267. lr 1.594186e-04: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:27:38 - INFO - minGPT.mingpt.trainer -   test loss: 1.105466\n",
      "epoch 469 iter 2: train loss 1.04292. lr 1.468586e-04: 100%|██████████| 3/3 [00:00<00:00,  6.43it/s]\n",
      "11/12/2020 22:27:38 - INFO - minGPT.mingpt.trainer -   test loss: 1.098245\n",
      "11/12/2020 22:27:38 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 470 iter 2: train loss 1.06347. lr 1.343207e-04: 100%|██████████| 3/3 [00:00<00:00,  5.83it/s]\n",
      "11/12/2020 22:27:39 - INFO - minGPT.mingpt.trainer -   test loss: 1.105203\n",
      "epoch 471 iter 2: train loss 1.01894. lr 1.218928e-04: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "11/12/2020 22:27:39 - INFO - minGPT.mingpt.trainer -   test loss: 1.094699\n",
      "11/12/2020 22:27:39 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 472 iter 2: train loss 1.05353. lr 1.096620e-04: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "11/12/2020 22:27:40 - INFO - minGPT.mingpt.trainer -   test loss: 1.106907\n",
      "epoch 473 iter 2: train loss 1.03458. lr 9.771419e-05: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:27:41 - INFO - minGPT.mingpt.trainer -   test loss: 1.092342\n",
      "11/12/2020 22:27:41 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 474 iter 2: train loss 1.01683. lr 8.613311e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:41 - INFO - minGPT.mingpt.trainer -   test loss: 1.087355\n",
      "11/12/2020 22:27:41 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 475 iter 2: train loss 1.03655. lr 7.500000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.40it/s]\n",
      "11/12/2020 22:27:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.089081\n",
      "epoch 476 iter 2: train loss 1.02721. lr 6.439296e-05: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s]\n",
      "11/12/2020 22:27:42 - INFO - minGPT.mingpt.trainer -   test loss: 1.089990\n",
      "epoch 477 iter 2: train loss 1.03903. lr 5.438640e-05: 100%|██████████| 3/3 [00:00<00:00,  6.46it/s]\n",
      "11/12/2020 22:27:43 - INFO - minGPT.mingpt.trainer -   test loss: 1.086987\n",
      "11/12/2020 22:27:43 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 478 iter 2: train loss 1.06487. lr 4.505050e-05: 100%|██████████| 3/3 [00:00<00:00,  6.35it/s]\n",
      "11/12/2020 22:27:43 - INFO - minGPT.mingpt.trainer -   test loss: 1.089437\n",
      "epoch 479 iter 2: train loss 1.03542. lr 3.645074e-05: 100%|██████████| 3/3 [00:00<00:00,  6.50it/s]\n",
      "11/12/2020 22:27:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.089192\n",
      "epoch 480 iter 2: train loss 1.00345. lr 3.000000e-05: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]\n",
      "11/12/2020 22:27:44 - INFO - minGPT.mingpt.trainer -   test loss: 1.086414\n",
      "11/12/2020 22:27:44 - INFO - minGPT.mingpt.trainer -   saving layer8head8emb64FactoringDatasetbase16digit2\n",
      "epoch 481 iter 1: train loss 1.02000. lr 3.000000e-05:  67%|██████▋   | 2/3 [00:00<00:00,  4.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-a6e813233a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mgive_exam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mgive_exam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openai_learning/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openai_learning/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import os\n",
    "import minGPT.mingpt.trainer\n",
    "reload(minGPT.mingpt.trainer)\n",
    "from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "# with 1024*4 batch size 1 layer 1 head n_embed=128 stalled out at 0.05 test and 0.25 train\n",
    "# initialize a trainer instance and kick off training\n",
    "# 5:11 for epoch at 16\n",
    "# 01:04 for epoch at 512\n",
    "# 01:02 for epoch at 2048\n",
    "# 1:20 for epoch at 2048*4\n",
    "\n",
    "\n",
    "\n",
    "saveString = f'layer{mconf.n_layer}head{mconf.n_head}emb{mconf.n_embd}{str(type(train_dataset).__name__)}base{train_dataset.base}digit{train_dataset.ndigit}'\n",
    "lossesPath = saveString + \"losses.json\"\n",
    "tconf = TrainerConfig(max_epochs=10000, batch_size=1024, learning_rate=3e-4,\n",
    "                      lr_decay=True, warmup_tokens=0, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
    "                      num_workers=0, ckpt_path=saveString, losses_path=lossesPath)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf, useCuda=True)\n",
    "while True:\n",
    "    from IPython.display import clear_output\n",
    "    def doCallback():\n",
    "        give_exam(train_dataset, batch_size=1024, max_batches=10)\n",
    "        give_exam(test_dataset, batch_size=1024, max_batches=10)\n",
    "    trainer.train(doCallback)\n",
    "    clear_output(wait=True)\n",
    "    tconf.warmup_tokens = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's give the trained model an addition exam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from minGPT.mingpt.utils import sample\n",
    "\n",
    "def give_exam(dataset, batch_size=32, max_batches=-1, showFailures=False, failureCallback=None):\n",
    "    \n",
    "    results = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(trainer.device)\n",
    "        y = y.to(trainer.device)\n",
    "        numMask = (y[0]==-100).int().sum() # we mask away the inputs, so sum\n",
    "        numOutput = y[0].shape[0]-numMask\n",
    "        d1d2 = x[:, :numMask+1] # +1 because y is shifted one so we actually mask two things\n",
    "        d1d2d3 = sample(model, d1d2, numOutput).to(trainer.device)\n",
    "        desiredOutput = y[:,numMask:]\n",
    "        modelOutput = d1d2d3[:,numMask+1:]\n",
    "        correct = (desiredOutput == modelOutput).cpu().all(axis=1) \n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "            judge = 'YEP!!!' if correct[i] else 'NOPE'\n",
    "            if not correct[i] and showFailures:\n",
    "                print(f\"GPT claims that {str(x[i].cpu().numpy())} gives {str(d1d2d3[i][1:].cpu().numpy())} actual is {str(y[i].cpu().numpy())}\")\n",
    "            if not correct[i] and failureCallback is not None:\n",
    "                failureCallback(x[i].cpu().numpy(), y[i].cpu().numpy(), d1d2d3[i][1:].cpu().numpy())\n",
    "        \n",
    "        if max_batches >= 0 and b+1 >= max_batches:\n",
    "            break\n",
    "\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "313337 316409\n"
     ]
    }
   ],
   "source": [
    "import customDatasets\n",
    "from importlib import reload\n",
    "reload(customDatasets)\n",
    "\n",
    "def toNum(base, arr):\n",
    "    a = arr\n",
    "    if type(arr) == torch.tensor:\n",
    "        a = arr.cpu().numpy()\n",
    "    val = 0\n",
    "    for i, x in enumerate(a[::-1]):\n",
    "        val += x*(base**i)\n",
    "    return val\n",
    "        \n",
    "def multArrays(base, a, b):\n",
    "    aVal = toNum(base, a)\n",
    "    bVal = toNum(base, b)\n",
    "    return aVal*bVal\n",
    "\n",
    "base = 32\n",
    "print(datasets.prime(toNum(base, [24, 29])))\n",
    "print(convertArrayToNum(base, [9,17,31,25]), multArrays(base, [12, 13], [24, 29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "global numPrimes\n",
    "global numNotPrimes\n",
    "numPrimes = 0\n",
    "numNotPrimes = 0\n",
    "def failureFactor(x, y, networkOutput):\n",
    "    global numPrimes\n",
    "    global numNotPrimes\n",
    "    nDigits = (len(x)+1)//4\n",
    "    productResult = toNum(base, x[:nDigits*2])\n",
    "    actualA, actualB = toNum(base, y[-nDigits*2:-nDigits]), toNum(base, y[-nDigits:])\n",
    "    outputA, outputB = toNum(base, networkOutput[-nDigits*2:-nDigits]), toNum(base, networkOutput[-nDigits:])\n",
    "    print(\"acutal:\", productResult, \"=\", actualA, \"*\", actualB, \"output\", (outputA*outputB), \"=\", outputA, \"*\", outputB)\n",
    "    diff = productResult-outputA*outputB\n",
    "    if customDatasets.prime(outputA): numPrimes += 1\n",
    "    else: numNotPrimes += 1\n",
    "    if customDatasets.prime(outputB): numPrimes += 1\n",
    "    else: numNotPrimes += 1\n",
    "    print(\"difference:\", diff, \"relative magnitude:\", float(diff)/productResult, \"isPrime:\", outputA, customDatasets.prime(outputA), outputB, customDatasets.prime(outputB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that [0 0 0 6 0 2 0] gives [ 0  0  6  0  2  0 19] actual is [-100 -100 -100    0    2    0    3]\n",
      "acutal: 6 = 2 * 3 output 38 = 2 * 19\n",
      "difference: -32 relative magnitude: -5.333333333333333 isPrime: 2 True 19 True\n",
      "GPT claims that [ 9 17 31 25 13 15 22] gives [17 31 25 12 13 24 29] actual is [-100 -100 -100   13   15   22   23]\n",
      "acutal: 313337 = 431 * 727 output 316409 = 397 * 797\n",
      "difference: -3072 relative magnitude: -0.00980414058984416 isPrime: 397 True 797 True\n",
      "GPT claims that [ 3  2  4  9  9 29  9] gives [ 2  4  9  6  1 16  9] actual is [-100 -100 -100    9   29    9   29]\n",
      "acutal: 100489 = 317 * 317 output 100553 = 193 * 521\n",
      "difference: -64 relative magnitude: -0.0006368856292728557 isPrime: 193 True 521 True\n",
      "GPT claims that [ 2 31 30 21  3  1 31] gives [31 30 21  8 13 11  9] actual is [-100 -100 -100    3    1   31   21]\n",
      "acutal: 98261 = 97 * 1013 output 97109 = 269 * 361\n",
      "difference: 1152 relative magnitude: 0.0117238782426395 isPrime: 269 True 361 False\n",
      "GPT claims that [ 2 29 31 29  3  5 29] gives [29 31 29  6  7 14 27] actual is [-100 -100 -100    3    5   29   25]\n",
      "acutal: 96253 = 101 * 953 output 94525 = 199 * 475\n",
      "difference: 1728 relative magnitude: 0.017952687188970733 isPrime: 199 True 475 False\n",
      "GPT claims that [ 2  5 29 25  3  5 22] gives [ 5 29 25  8 21  8 21] actual is [-100 -100 -100    3    5   22    5]\n",
      "acutal: 71609 = 101 * 709 output 76729 = 277 * 277\n",
      "difference: -5120 relative magnitude: -0.07149939253445796 isPrime: 277 True 277 True\n",
      "GPT claims that [ 0  3  9 23  0 17  6] gives [ 3  9 23  0  7 14 17] actual is [-100 -100 -100    0   17    6    7]\n",
      "acutal: 3383 = 17 * 199 output 3255 = 7 * 465\n",
      "difference: 128 relative magnitude: 0.03783624002364765 isPrime: 7 True 465 False\n",
      "GPT claims that [13 19 21  7 16 11 26] gives [19 21  7 15 23 27 17] actual is [-100 -100 -100   16   11   26   21]\n",
      "acutal: 446119 = 523 * 853 output 443143 = 503 * 881\n",
      "difference: 2976 relative magnitude: 0.006670865845211703 isPrime: 503 True 881 True\n",
      "GPT claims that [ 0  6 23  9  2 19  2] gives [ 6 23  9  2  7  3 15] actual is [-100 -100 -100    2   19    2   19]\n",
      "acutal: 6889 = 83 * 83 output 7881 = 71 * 111\n",
      "difference: -992 relative magnitude: -0.14399767745681522 isPrime: 71 True 111 False\n",
      "GPT claims that [26 17 28 21 28 23 29] gives [17 28 21 27 19 30 23] actual is [-100 -100 -100   28   23   29   19]\n",
      "acutal: 870293 = 919 * 947 output 867989 = 883 * 983\n",
      "difference: 2304 relative magnitude: 0.0026473842717337722 isPrime: 883 True 983 True\n",
      "GPT claims that [ 4 11 28  5  7  3 19] gives [11 28  5 10 27 11 31] actual is [-100 -100 -100    7    3   19   23]\n",
      "acutal: 143237 = 227 * 631 output 132901 = 347 * 383\n",
      "difference: 10336 relative magnitude: 0.0721601262243694 isPrime: 347 True 383 True\n",
      "GPT claims that [22 14 28 11 26 21 26] gives [14 28 11 25  9 27 19] actual is [-100 -100 -100   26   21   26   31]\n",
      "acutal: 736139 = 853 * 863 output 714347 = 809 * 883\n",
      "difference: 21792 relative magnitude: 0.02960310484840499 isPrime: 809 True 883 True\n",
      "GPT claims that [ 9 29 18 27 12 17 25] gives [29 18 27 10 11 30 17] actual is [-100 -100 -100   12   17   25   11]\n",
      "acutal: 325211 = 401 * 811 output 323387 = 331 * 977\n",
      "difference: 1824 relative magnitude: 0.00560866637352365 isPrime: 331 True 977 True\n",
      "final score: 10227/10240 = 99.87% correct\n",
      "44 8\n"
     ]
    }
   ],
   "source": [
    "give_exam(train_dataset, batch_size=1024, max_batches=10, showFailures=True, failureCallback=failureFactor)\n",
    "print(numPrimes, numNotPrimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that [16 16 13 21 19  9 27] gives [16 13 21 20 13 25  9] actual is [-100 -100 -100   19    9   27   13]\n",
      "acutal: 541109 = 617 * 877 output 528277 = 653 * 809\n",
      "difference: 12832 relative magnitude: 0.023714260897527116 isPrime: 653 True 809 True\n",
      "GPT claims that [20  6 29 29 24  5 26] gives [ 6 29 29 22 15 29 19] actual is [-100 -100 -100   24    5   26   25]\n",
      "acutal: 662461 = 773 * 857 output 680893 = 719 * 947\n",
      "difference: -18432 relative magnitude: -0.027823524705605313 isPrime: 719 True 947 True\n",
      "GPT claims that [16  4  4 17 22 23 22] gives [ 4  4 17 19  5 26 25] actual is [-100 -100 -100   22   23   22   23]\n",
      "acutal: 528529 = 727 * 727 output 525341 = 613 * 857\n",
      "difference: 3188 relative magnitude: 0.006031835528419444 isPrime: 613 True 857 True\n",
      "GPT claims that [ 0 10  8 21  0 13 25] gives [10  8 21  2  3  5  7] actual is [-100 -100 -100    0   13   25    9]\n",
      "acutal: 10517 = 13 * 809 output 11189 = 67 * 167\n",
      "difference: -672 relative magnitude: -0.06389654844537415 isPrime: 67 True 167 True\n",
      "GPT claims that [ 9 29 18 27 12 17 25] gives [29 18 27 10 11 30 17] actual is [-100 -100 -100   12   17   25   11]\n",
      "acutal: 325211 = 401 * 811 output 323387 = 331 * 977\n",
      "difference: 1824 relative magnitude: 0.00560866637352365 isPrime: 331 True 977 True\n",
      "GPT claims that [ 2  5 29 25  3  5 22] gives [ 5 29 25  8 21  8 21] actual is [-100 -100 -100    3    5   22    5]\n",
      "acutal: 71609 = 101 * 709 output 76729 = 277 * 277\n",
      "difference: -5120 relative magnitude: -0.07149939253445796 isPrime: 277 True 277 True\n",
      "GPT claims that [ 0  3 11  3  0 23  4] gives [ 3 11  3  0  7 15  5] actual is [-100 -100 -100    0   23    4   21]\n",
      "acutal: 3427 = 23 * 149 output 3395 = 7 * 485\n",
      "difference: 32 relative magnitude: 0.009337613072658301 isPrime: 7 True 485 False\n",
      "GPT claims that [ 4 11  0 13  8 27 15] gives [11  0 13  4 29 27 17] actual is [-100 -100 -100    8   27   15   23]\n",
      "acutal: 142349 = 283 * 503 output 138317 = 157 * 881\n",
      "difference: 4032 relative magnitude: 0.028324751139804285 isPrime: 157 True 881 True\n",
      "GPT claims that [ 0  0 22 30  0  2 11] gives [ 0 22 30  0  2 11 31] actual is [-100 -100 -100    0    2   11   15]\n",
      "acutal: 734 = 2 * 367 output 766 = 2 * 383\n",
      "difference: -32 relative magnitude: -0.043596730245231606 isPrime: 2 True 383 True\n",
      "GPT claims that [10 17  6  3 15  7 22] gives [17  6  3 14  1 23  3] actual is [-100 -100 -100   15    7   22    5]\n",
      "acutal: 345283 = 487 * 709 output 331811 = 449 * 739\n",
      "difference: 13472 relative magnitude: 0.03901726989165409 isPrime: 449 True 739 True\n",
      "GPT claims that [13 19 21  7 16 11 26] gives [19 21  7 15 23 27 17] actual is [-100 -100 -100   16   11   26   21]\n",
      "acutal: 446119 = 523 * 853 output 443143 = 503 * 881\n",
      "difference: 2976 relative magnitude: 0.006670865845211703 isPrime: 503 True 881 True\n",
      "GPT claims that [ 2 22  2 29  6  7 13] gives [22  2 29  3  1 29 25] actual is [-100 -100 -100    6    7   13   27]\n",
      "acutal: 88157 = 199 * 443 output 92441 = 97 * 953\n",
      "difference: -4284 relative magnitude: -0.04859512006987533 isPrime: 97 True 953 True\n",
      "GPT claims that [ 1 14 11 21  6  5  7] gives [14 11 21  3  7 15  3] actual is [-100 -100 -100    6    5    7   17]\n",
      "acutal: 47477 = 197 * 241 output 49749 = 103 * 483\n",
      "difference: -2272 relative magnitude: -0.04785475072140194 isPrime: 103 True 483 False\n",
      "GPT claims that [15 16 16 11 21  5 23] gives [16 16 11 20  1 25 11] actual is [-100 -100 -100   21    5   23   15]\n",
      "acutal: 508427 = 677 * 751 output 519851 = 641 * 811\n",
      "difference: -11424 relative magnitude: -0.02246930237772581 isPrime: 641 True 811 True\n",
      "GPT claims that [26 17 28 21 28 23 29] gives [17 28 21 27 19 30 23] actual is [-100 -100 -100   28   23   29   19]\n",
      "acutal: 870293 = 919 * 947 output 867989 = 883 * 983\n",
      "difference: 2304 relative magnitude: 0.0026473842717337722 isPrime: 883 True 983 True\n",
      "GPT claims that [ 6 25 30 21  9 19 22] gives [25 30 21  8 25 24 29] actual is [-100 -100 -100    9   19   22   23]\n",
      "acutal: 223189 = 307 * 727 output 223957 = 281 * 797\n",
      "difference: -768 relative magnitude: -0.0034410297998557276 isPrime: 281 True 797 True\n",
      "GPT claims that [ 0  2  8 23  0 13  5] gives [ 2  8 23  0 17  4  7] actual is [-100 -100 -100    0   13    5   19]\n",
      "acutal: 2327 = 13 * 179 output 2295 = 17 * 135\n",
      "difference: 32 relative magnitude: 0.013751611516974646 isPrime: 17 True 135 False\n",
      "GPT claims that [22 14 28 11 26 21 26] gives [14 28 11 25  9 27 19] actual is [-100 -100 -100   26   21   26   31]\n",
      "acutal: 736139 = 853 * 863 output 714347 = 809 * 883\n",
      "difference: 21792 relative magnitude: 0.02960310484840499 isPrime: 809 True 883 True\n",
      "GPT claims that [ 7 29 28 11 15 19 16] gives [29 28 11  9  5 27 15] actual is [-100 -100 -100   15   19   16    9]\n",
      "acutal: 259979 = 499 * 521 output 257547 = 293 * 879\n",
      "difference: 2432 relative magnitude: 0.009354601717831056 isPrime: 293 True 879 False\n",
      "GPT claims that [ 0  3  6  1  0  5 20] gives [ 3  6  1  0 29  3 21] actual is [-100 -100 -100    0    5   20   13]\n",
      "acutal: 3265 = 5 * 653 output 3393 = 29 * 117\n",
      "difference: -128 relative magnitude: -0.03920367534456355 isPrime: 29 True 117 False\n",
      "GPT claims that [ 0 10  8 21  0 13 25] gives [10  8 21  2  3  5  7] actual is [-100 -100 -100    0   13   25    9]\n",
      "acutal: 10517 = 13 * 809 output 11189 = 67 * 167\n",
      "difference: -672 relative magnitude: -0.06389654844537415 isPrime: 67 True 167 True\n",
      "GPT claims that [ 1  6 21 17  6  7  6] gives [ 6 21 17  1 29 21  5] actual is [-100 -100 -100    6    7    6    7]\n",
      "acutal: 39601 = 199 * 199 output 41297 = 61 * 677\n",
      "difference: -1696 relative magnitude: -0.04282720133329966 isPrime: 61 True 677 True\n",
      "GPT claims that [10 30 12 17 18 23 18] gives [30 12 17 13  3 26 27] actual is [-100 -100 -100   18   23   18   23]\n",
      "acutal: 358801 = 599 * 599 output 359921 = 419 * 859\n",
      "difference: -1120 relative magnitude: -0.003121507465140844 isPrime: 419 True 859 True\n",
      "GPT claims that [ 7 26  0 27 15 23 15] gives [26  0 27 14  1 17 27] actual is [-100 -100 -100   15   23   15   29]\n",
      "acutal: 256027 = 503 * 509 output 256379 = 449 * 571\n",
      "difference: -352 relative magnitude: -0.0013748549957621657 isPrime: 449 True 571 True\n",
      "GPT claims that [ 7 17  5 29 15 11 15] gives [17  5 29 11 31 20  3] actual is [-100 -100 -100   15   11   15   23]\n",
      "acutal: 246973 = 491 * 503 output 246269 = 383 * 643\n",
      "difference: 704 relative magnitude: 0.0028505140238001725 isPrime: 383 True 643 True\n",
      "GPT claims that [ 2 17 13 21  6  7 13] gives [17 13 21  3 31 19 11] actual is [-100 -100 -100    6    7   13    3]\n",
      "acutal: 83381 = 199 * 419 output 78613 = 127 * 619\n",
      "difference: 4768 relative magnitude: 0.05718329115745793 isPrime: 127 True 619 True\n",
      "GPT claims that [ 2 22  2 29  6  7 13] gives [22  2 29  3  1 29 25] actual is [-100 -100 -100    6    7   13   27]\n",
      "acutal: 88157 = 199 * 443 output 92441 = 97 * 953\n",
      "difference: -4284 relative magnitude: -0.04859512006987533 isPrime: 97 True 953 True\n",
      "GPT claims that [ 1  9  5  5  3 17 11] gives [ 9  5  5  2  7 17 19] actual is [-100 -100 -100    3   17   11   21]\n",
      "acutal: 42149 = 113 * 373 output 39973 = 71 * 563\n",
      "difference: 2176 relative magnitude: 0.05162637310493725 isPrime: 71 True 563 True\n",
      "GPT claims that [ 2 31 30 21  3  1 31] gives [31 30 21  8 13 11  9] actual is [-100 -100 -100    3    1   31   21]\n",
      "acutal: 98261 = 97 * 1013 output 97109 = 269 * 361\n",
      "difference: 1152 relative magnitude: 0.0117238782426395 isPrime: 269 True 361 False\n",
      "GPT claims that [22 16 18 25 26 27 26] gives [16 18 25 25 29 27 13] actual is [-100 -100 -100   26   27   26   27]\n",
      "acutal: 737881 = 859 * 859 output 727033 = 829 * 877\n",
      "difference: 10848 relative magnitude: 0.01470155756822577 isPrime: 829 True 877 True\n",
      "GPT claims that [0 0 0 6 0 2 0] gives [ 0  0  6  0  2  0 19] actual is [-100 -100 -100    0    2    0    3]\n",
      "acutal: 6 = 2 * 3 output 38 = 2 * 19\n",
      "difference: -32 relative magnitude: -5.333333333333333 isPrime: 2 True 19 True\n",
      "GPT claims that [ 9 13 11 17 13  5 22] gives [13 11 17 15 23 18 23] actual is [-100 -100 -100   13    5   22   29]\n",
      "acutal: 308593 = 421 * 733 output 301297 = 503 * 599\n",
      "difference: 7296 relative magnitude: 0.023642791638177146 isPrime: 503 True 599 True\n",
      "GPT claims that [ 5 16 19  9  6  1 29] gives [16 19  9  9  5 18 21] actual is [-100 -100 -100    6    1   29    9]\n",
      "acutal: 180841 = 193 * 937 output 174921 = 293 * 597\n",
      "difference: 5920 relative magnitude: 0.03273593930579902 isPrime: 293 True 597 False\n",
      "GPT claims that [ 0 11 11 23  3  7  3] gives [11 11 23  0 13 27 19] actual is [-100 -100 -100    3    7    3   17]\n",
      "acutal: 11639 = 103 * 113 output 11479 = 13 * 883\n",
      "difference: 160 relative magnitude: 0.013746885471260418 isPrime: 13 True 883 True\n",
      "GPT claims that [ 6 15 17  9 14 13 14] gives [15 17  9 10 27 19 11] actual is [-100 -100 -100   14   13   14   13]\n",
      "acutal: 212521 = 461 * 461 output 214793 = 347 * 619\n",
      "difference: -2272 relative magnitude: -0.010690708212364895 isPrime: 347 True 619 True\n",
      "GPT claims that [ 1  9  5  5  3 17 11] gives [ 9  5  5  2  7 17 19] actual is [-100 -100 -100    3   17   11   21]\n",
      "acutal: 42149 = 113 * 373 output 39973 = 71 * 563\n",
      "difference: 2176 relative magnitude: 0.05162637310493725 isPrime: 71 True 563 True\n",
      "GPT claims that [ 5 16 19  9  6  1 29] gives [16 19  9  9  5 18 21] actual is [-100 -100 -100    6    1   29    9]\n",
      "acutal: 180841 = 193 * 937 output 174921 = 293 * 597\n",
      "difference: 5920 relative magnitude: 0.03273593930579902 isPrime: 293 True 597 False\n",
      "GPT claims that [23 21 31  1 27 17 27] gives [21 31  1 26  7 28 23] actual is [-100 -100 -100   27   17   27   17]\n",
      "acutal: 776161 = 881 * 881 output 771041 = 839 * 919\n",
      "difference: 5120 relative magnitude: 0.006596569526167896 isPrime: 839 True 919 True\n",
      "GPT claims that [ 6 19 24 31  7  5 29] gives [19 24 31  9 19 22  5] actual is [-100 -100 -100    7    5   29   19]\n",
      "acutal: 216863 = 229 * 947 output 217663 = 307 * 709\n",
      "difference: -800 relative magnitude: -0.003688964922554793 isPrime: 307 True 709 True\n",
      "GPT claims that [15 26 16  7 18 25 26] gives [26 16  7 18 23 27 17] actual is [-100 -100 -100   18   25   26   31]\n",
      "acutal: 518663 = 601 * 863 output 527719 = 599 * 881\n",
      "difference: -9056 relative magnitude: -0.017460277675484852 isPrime: 599 True 881 True\n",
      "GPT claims that [ 4 27  8 11  5 13 28] gives [27  8 11  5 31 26 21] actual is [-100 -100 -100    5   13   28   23]\n",
      "acutal: 158987 = 173 * 919 output 162923 = 191 * 853\n",
      "difference: -3936 relative magnitude: -0.02475674111719826 isPrime: 191 True 853 True\n",
      "GPT claims that [ 3 14 26 11  8 27 12] gives [14 26 11  4  9 25 19] actual is [-100 -100 -100    8   27   12   17]\n",
      "acutal: 113483 = 283 * 401 output 112203 = 137 * 819\n",
      "difference: 1280 relative magnitude: 0.011279222438603138 isPrime: 137 True 819 False\n",
      "GPT claims that [ 3 14 26 11  8 27 12] gives [14 26 11  4  9 25 19] actual is [-100 -100 -100    8   27   12   17]\n",
      "acutal: 113483 = 283 * 401 output 112203 = 137 * 819\n",
      "difference: 1280 relative magnitude: 0.011279222438603138 isPrime: 137 True 819 False\n",
      "GPT claims that [19  2 19  1 22 29 26] gives [ 2 19  1 21 19 26 27] actual is [-100 -100 -100   22   29   26   21]\n",
      "acutal: 625249 = 733 * 853 output 593569 = 691 * 859\n",
      "difference: 31680 relative magnitude: 0.05066781394292514 isPrime: 691 True 859 True\n",
      "GPT claims that [ 1 18 28  1  1 27 27] gives [18 28  1  1 21 31 29] actual is [-100 -100 -100    1   27   27   19]\n",
      "acutal: 52097 = 59 * 883 output 54113 = 53 * 1021\n",
      "difference: -2016 relative magnitude: -0.03869704589515711 isPrime: 53 True 1021 True\n",
      "GPT claims that [ 3 10  4  1  3 13 31] gives [10  4  1  4 21 22 29] actual is [-100 -100 -100    3   13   31    5]\n",
      "acutal: 108673 = 109 * 997 output 109217 = 149 * 733\n",
      "difference: -544 relative magnitude: -0.005005843217726574 isPrime: 149 True 733 True\n",
      "GPT claims that [ 1 14 13 19  4  9 10] gives [14 13 19  1 21 26  7] actual is [-100 -100 -100    4    9   10   27]\n",
      "acutal: 47539 = 137 * 347 output 44467 = 53 * 839\n",
      "difference: 3072 relative magnitude: 0.06462062727444835 isPrime: 53 True 839 True\n",
      "GPT claims that [ 0  3  9 23  0 17  6] gives [ 3  9 23  0  7 14 17] actual is [-100 -100 -100    0   17    6    7]\n",
      "acutal: 3383 = 17 * 199 output 3255 = 7 * 465\n",
      "difference: 128 relative magnitude: 0.03783624002364765 isPrime: 7 True 465 False\n",
      "GPT claims that [ 4 18  1 21 10 27 13] gives [18  1 21  4 29 29 25] actual is [-100 -100 -100   10   27   13   15]\n",
      "acutal: 149557 = 347 * 431 output 149621 = 157 * 953\n",
      "difference: -64 relative magnitude: -0.0004279304880413488 isPrime: 157 True 953 True\n",
      "GPT claims that [ 2 17 13 21  6  7 13] gives [17 13 21  3 31 19 11] actual is [-100 -100 -100    6    7   13    3]\n",
      "acutal: 83381 = 199 * 419 output 78613 = 127 * 619\n",
      "difference: 4768 relative magnitude: 0.05718329115745793 isPrime: 127 True 619 True\n",
      "GPT claims that [ 0  1 29  9  1  5  1] gives [ 1 29  9  0  5 11 21] actual is [-100 -100 -100    1    5    1   21]\n",
      "acutal: 1961 = 37 * 53 output 1865 = 5 * 373\n",
      "difference: 96 relative magnitude: 0.04895461499235084 isPrime: 5 True 373 True\n",
      "GPT claims that [ 4 27  8 11  5 13 28] gives [27  8 11  5 31 26 21] actual is [-100 -100 -100    5   13   28   23]\n",
      "acutal: 158987 = 173 * 919 output 162923 = 191 * 853\n",
      "difference: -3936 relative magnitude: -0.02475674111719826 isPrime: 191 True 853 True\n",
      "GPT claims that [ 6 19 24 31  7  5 29] gives [19 24 31  9 19 22  5] actual is [-100 -100 -100    7    5   29   19]\n",
      "acutal: 216863 = 229 * 947 output 217663 = 307 * 709\n",
      "difference: -800 relative magnitude: -0.003688964922554793 isPrime: 307 True 709 True\n",
      "GPT claims that [ 2 29 31 29  3  5 29] gives [29 31 29  6  7 14 27] actual is [-100 -100 -100    3    5   29   25]\n",
      "acutal: 96253 = 101 * 953 output 94525 = 199 * 475\n",
      "difference: 1728 relative magnitude: 0.017952687188970733 isPrime: 199 True 475 False\n",
      "GPT claims that [ 8 22  6 27 13 15 20] gives [22  6 27 13 27 20  1] actual is [-100 -100 -100   13   15   20   21]\n",
      "acutal: 284891 = 431 * 661 output 283963 = 443 * 641\n",
      "difference: 928 relative magnitude: 0.003257386158214896 isPrime: 443 True 641 True\n",
      "GPT claims that [19  2 19  1 22 29 26] gives [ 2 19  1 21 19 26 27] actual is [-100 -100 -100   22   29   26   21]\n",
      "acutal: 625249 = 733 * 853 output 593569 = 691 * 859\n",
      "difference: 31680 relative magnitude: 0.05066781394292514 isPrime: 691 True 859 True\n",
      "GPT claims that [ 3 27 26 27  6 19 18] gives [27 26 27  9 23 12 25] actual is [-100 -100 -100    6   19   18   25]\n",
      "acutal: 126811 = 211 * 601 output 127199 = 311 * 409\n",
      "difference: -388 relative magnitude: -0.0030596714796035044 isPrime: 311 True 409 True\n",
      "GPT claims that [ 4 11 28  5  7  3 19] gives [11 28  5 10 27 11 31] actual is [-100 -100 -100    7    3   19   23]\n",
      "acutal: 143237 = 227 * 631 output 132901 = 347 * 383\n",
      "difference: 10336 relative magnitude: 0.0721601262243694 isPrime: 347 True 383 True\n",
      "GPT claims that [ 0  6 20 13  0  7 30] gives [ 6 20 13  0 11 20  7] actual is [-100 -100 -100    0    7   30   11]\n",
      "acutal: 6797 = 7 * 971 output 7117 = 11 * 647\n",
      "difference: -320 relative magnitude: -0.04707959393850228 isPrime: 11 True 647 True\n",
      "GPT claims that [ 5 14  8 31  5 19 31] gives [14  8 31  7  5 24 19] actual is [-100 -100 -100    5   19   31    5]\n",
      "acutal: 178463 = 179 * 997 output 180223 = 229 * 787\n",
      "difference: -1760 relative magnitude: -0.009861988199234575 isPrime: 229 True 787 True\n",
      "GPT claims that [ 3 27 26 27  6 19 18] gives [27 26 27  9 23 12 25] actual is [-100 -100 -100    6   19   18   25]\n",
      "acutal: 126811 = 211 * 601 output 127199 = 311 * 409\n",
      "difference: -388 relative magnitude: -0.0030596714796035044 isPrime: 311 True 409 True\n",
      "GPT claims that [14 19  7 31 17 13 26] gives [19  7 31 18 11 25 29] actual is [-100 -100 -100   17   13   26   27]\n",
      "acutal: 478463 = 557 * 859 output 486623 = 587 * 829\n",
      "difference: -8160 relative magnitude: -0.017054610283344793 isPrime: 587 True 829 True\n",
      "GPT claims that [ 5 14  8 31  5 19 31] gives [14  8 31  7  5 24 19] actual is [-100 -100 -100    5   19   31    5]\n",
      "acutal: 178463 = 179 * 997 output 180223 = 229 * 787\n",
      "difference: -1760 relative magnitude: -0.009861988199234575 isPrime: 229 True 787 True\n",
      "GPT claims that [ 1 14 13 19  4  9 10] gives [14 13 19  1 21 26  7] actual is [-100 -100 -100    4    9   10   27]\n",
      "acutal: 47539 = 137 * 347 output 44467 = 53 * 839\n",
      "difference: 3072 relative magnitude: 0.06462062727444835 isPrime: 53 True 839 True\n",
      "GPT claims that [ 4 11  0 13  8 27 15] gives [11  0 13  4 29 27 17] actual is [-100 -100 -100    8   27   15   23]\n",
      "acutal: 142349 = 283 * 503 output 138317 = 157 * 881\n",
      "difference: 4032 relative magnitude: 0.028324751139804285 isPrime: 157 True 881 True\n",
      "final score: 935/1000 = 93.50% correct\n",
      "118 12\n"
     ]
    }
   ],
   "source": [
    "global numPrimes, numNotPrimes\n",
    "numPrimes, numNotPrimes = 0,0\n",
    "give_exam(test_dataset, batch_size=1024, max_batches=10, showFailures=True, failureCallback=failureFactor)\n",
    "print(numPrimes, numNotPrimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_layer=2, n_head=2, n_embd=128 - gets 100%\n",
    "\n",
    "\n",
    "\n",
    "# factoring:\n",
    "# base 16, 2 digits:\n",
    "# 8 layers, 2 heads stalls out at 0.14311, and also seems to be memorizing because test loss goes down but then back up\n",
    "\n",
    "# 8 layers, 8 heads, embed 64\n",
    "# epoch 453 iter 3: train loss 0.98656. lr 5.825030e-04:  80%|████████  | 4/5 [00:00<00:00,  7.09it/s]\n",
    "# epoch 102 iter 0: train loss 1.47119. lr 1.204984e-04:  20%|██        | 1/5 [00:00<00:00,  9.02it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
