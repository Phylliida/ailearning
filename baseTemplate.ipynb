{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FplHOfK2094n"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qqq wandb pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M9AO80L80smw"
   },
   "outputs": [],
   "source": [
    "# numpy for non-GPU array math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HFCYqe-LE0y6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pytorch lightning\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "\n",
    "# weights and biases\n",
    "import wandb\n",
    "\n",
    "# Use the wandb logger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# login to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WXtU8qhm1oAr"
   },
   "outputs": [],
   "source": [
    "class LitMLP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_dims, n_classes=10,\n",
    "                 n_layer_1=128, n_layer_2=256, lr=1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        # we flatten the input Tensors and pass them through an MLP\n",
    "        self.layer_1 = nn.Linear(np.prod(in_dims), n_layer_1)\n",
    "        self.layer_2 = nn.Linear(n_layer_1, n_layer_2)\n",
    "        self.layer_3 = nn.Linear(n_layer_2, n_classes)\n",
    "\n",
    "        # log hyperparameters (saves to self.hparams, which is logged to wandb as the config)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # compute the accuracy -- no need  to roll your own!\n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "        self.valid_acc = pl.metrics.Accuracy()\n",
    "        self.test_acc = pl.metrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines a forward pass using the Stem-Learner-Task\n",
    "        design pattern from Deep Learning Design Patterns:\n",
    "        https://www.manning.com/books/deep-learning-design-patterns\n",
    "        \"\"\"\n",
    "        batch_size, *dims = x.size()\n",
    "\n",
    "        # stem: flatten\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # learner: two fully-connected layers\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        \n",
    "        # task: compute class logits\n",
    "        x = self.layer_3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # convenient method to get the loss on a batch\n",
    "    def loss(self, xs, ys):\n",
    "        logits = self(xs)  # this calls self.forward\n",
    "        loss = F.nll_loss(logits, ys)\n",
    "        return logits, loss\n",
    "    \n",
    "    # takes a batch and computes the loss; backprop goes through it\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        xs, ys = batch\n",
    "        logits, loss = self.loss(xs, ys)\n",
    "\n",
    "        # logging metrics we calculated by hand\n",
    "        # Here's the docs for reference https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#log\n",
    "        # this takes a name and value, and under the hood it uses wandb.log\n",
    "        self.log('train/loss', loss, on_epoch=True) # if you do on_step=False (by default this is true) then it'll only do epoch wise averaging outputs, see test_step below\n",
    "        # logging a pl.Metric\n",
    "        self.train_acc(logits, ys)\n",
    "        self.log('train/acc', self.train_acc, on_epoch=True) \n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    # returns the torch.optim.Optimizer to apply after the training_step\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams[\"lr\"])\n",
    "    \n",
    "    # validation_step and test_step will trigger on each batch\n",
    "    # {training, validation, test}_epoch_end will trigger at end of epoch, or a full pass over a given dataset\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        xs, ys = batch\n",
    "        logits, loss = self.loss(xs, ys)\n",
    "        self.test_acc(logits, ys)\n",
    "        self.log(\"test/loss_epoch\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/acc_epoch\", self.test_acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "    # save the model after we are done with testing, we will use ONNX format (https://onnx.ai/) cause it lets us use nice things like the neutron model viewer in W&B (https://github.com/lutzroeder/netron)\n",
    "    def test_epoch_end(self, test_step_outputs):  # args are defined as part of pl API\n",
    "        dummy_input = torch.zeros(self.hparams[\"in_dims\"], device=self.device)\n",
    "        model_filename = \"model_final.onnx\"\n",
    "        torch.onnx.export(self, dummy_input, model_filename)\n",
    "        wandb.save(model_filename)\n",
    "        \n",
    "    # return the logits so they can be used by validation_epoch_end\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        xs, ys = batch\n",
    "        logits, loss = self.loss(xs, ys)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "        self.valid_acc(logits, ys)\n",
    "\n",
    "        self.log(\"valid/loss_epoch\", loss)  # default on val/test is on_epoch only\n",
    "        self.log('valid/acc_epoch', self.valid_acc)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    # example of how to log the logits as a histogram\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        dummy_input = torch.zeros(self.hparams[\"in_dims\"], device=self.device)\n",
    "        model_filename = f\"model_{str(self.global_step).zfill(5)}.onnx\" # save the model on every epoch end, so we can roll it back if needed\n",
    "        torch.onnx.export(self, dummy_input, model_filename)\n",
    "        wandb.save(model_filename)\n",
    "\n",
    "        flattened_logits = torch.flatten(torch.cat(validation_step_outputs))\n",
    "        self.logger.experiment.log(\n",
    "            {\"valid/logits\": wandb.Histogram(flattened_logits.to(\"cpu\")),\n",
    "             \"global_step\": self.global_step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mwxGAUqwF_bg"
   },
   "outputs": [],
   "source": [
    "# custom callback that logs input images and output predictions\n",
    "class ImagePredictionLogger(pl.Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "        self.val_imgs = self.val_imgs[:num_samples]\n",
    "        self.val_labels = self.val_labels[:num_samples]\n",
    "          \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "        logits = pl_module(val_imgs)\n",
    "        preds = torch.argmax(logits, -1)\n",
    "\n",
    "        trainer.logger.experiment.log({\n",
    "            \"examples\": [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
    "                            for x, pred, y in zip(val_imgs, preds, self.val_labels)],\n",
    "            \"global_step\": trainer.global_step\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n6EvnEfK65ML"
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir='./', batch_size=128):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    # optional, only called once and one 1 GPU, typically for something like data download\n",
    "    def prepare_data(self):\n",
    "        # download data, train then test\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    # called on each GPU seperately and accepts stage to define if we are at fit or test step\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # we set up only relevant datasets when stage is specified\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist = MNIST(self.data_dir, train=True, download=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    # we define a separate DataLoader for each of train/val/test\n",
    "    def train_dataloader(self):\n",
    "        mnist_train = DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "        return mnist_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        mnist_val = DataLoader(self.mnist_val, batch_size=10 * self.batch_size)\n",
    "        return mnist_val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        mnist_test = DataLoader(self.mnist_test, batch_size=10 * self.batch_size)\n",
    "        return mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975040c8000f41bc8801c06eb8165097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76da69e2152647ebb45ccfd1adccae55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0935ecbb0db4d8abba1d129a2e86535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eb276d00814ac2bde2617b1f832c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/envs/sandbox1/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "mnist = MNISTDataModule()\n",
    "mnist.setup()\n",
    "\n",
    "# grab samples to log predictions on\n",
    "samples = next(iter(mnist.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NI1Bh8CGI-FG"
   },
   "outputs": [],
   "source": [
    "# connect to logging weights and biases, see documentation for more info https://docs.wandb.com/integrations/lightning\n",
    "wandb_logger = WandbLogger(project=\"lit-wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CxXtBfFrKYgA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "# create the trainer\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,    # W&B integration\n",
    "    log_every_n_steps=50,   # set the logging frequency\n",
    "    gpus=-1,                # use all GPUs\n",
    "    max_epochs=1,           # number of epochs\n",
    "    deterministic=True,     # keep it deterministic\n",
    "    callbacks=[ImagePredictionLogger(samples)] # see Callbacks section\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E7zB4ObdI8u8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphylliida\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">golden-surf-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/phylliida/lit-wandb\" target=\"_blank\">https://wandb.ai/phylliida/lit-wandb</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/phylliida/lit-wandb/runs/1y15noyr\" target=\"_blank\">https://wandb.ai/phylliida/lit-wandb/runs/1y15noyr</a><br/>\n",
       "                Run data is saved locally in <code>/home/azureuser/openai_learning/wandb/run-20201214_154002-1y15noyr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type     | Params\n",
      "---------------------------------------\n",
      "0 | layer_1   | Linear   | 100 K \n",
      "1 | layer_2   | Linear   | 33.0 K\n",
      "2 | layer_3   | Linear   | 2.6 K \n",
      "3 | train_acc | Accuracy | 0     \n",
      "4 | valid_acc | Accuracy | 0     \n",
      "5 | test_acc  | Accuracy | 0     \n",
      "---------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "/home/azureuser/miniconda3/envs/sandbox1/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953f2428c9c041e79cf3cf1bc0b3a935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/envs/sandbox1/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e9ce0532c04c288b4421822d204c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3064123f4884e65988f621c8007b001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/envs/sandbox1/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c8b2f03edf48f88d1ecdb14ac5091e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/acc_epoch': tensor(0.9096, device='cuda:0'),\n",
      " 'test/loss_epoch': tensor(0.3162, device='cuda:0'),\n",
      " 'valid/acc_epoch': tensor(0.9050, device='cuda:0'),\n",
      " 'valid/loss_epoch': tensor(0.3346, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18280<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceec52b40aa4b72a13e0d059edccd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1.07MB of 1.59MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.67260250199…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/azureuser/openai_learning/wandb/run-20201214_154002-1y15noyr/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/azureuser/openai_learning/wandb/run-20201214_154002-1y15noyr/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>global_step</td><td>429</td></tr><tr><td>_step</td><td>859</td></tr><tr><td>_runtime</td><td>25</td></tr><tr><td>_timestamp</td><td>1607960427</td></tr><tr><td>train/loss_step</td><td>0.39951</td></tr><tr><td>train/acc_step</td><td>0.86719</td></tr><tr><td>valid/loss_epoch</td><td>0.3346</td></tr><tr><td>valid/acc_epoch</td><td>0.905</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>train/loss_epoch</td><td>0.7545</td></tr><tr><td>train/acc_epoch</td><td>0.80895</td></tr><tr><td>test/loss_epoch</td><td>0.3162</td></tr><tr><td>test/acc_epoch</td><td>0.9096</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>global_step</td><td>▁▁██</td></tr><tr><td>_step</td><td>▁▁▁▂▂▃▃▃▄▄▄▄█</td></tr><tr><td>_runtime</td><td>▁▁▂▃▄▄▅▅▆▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▃▄▄▅▅▆▇▇██</td></tr><tr><td>train/loss_step</td><td>█▄▂▂▂▁▁▁</td></tr><tr><td>train/acc_step</td><td>▁▃▇▇▆██▇</td></tr><tr><td>valid/loss_epoch</td><td>▁</td></tr><tr><td>valid/acc_epoch</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁</td></tr><tr><td>train/loss_epoch</td><td>▁</td></tr><tr><td>train/acc_epoch</td><td>▁</td></tr><tr><td>test/loss_epoch</td><td>▁</td></tr><tr><td>test/acc_epoch</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 64 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">golden-surf-2</strong>: <a href=\"https://wandb.ai/phylliida/lit-wandb/runs/1y15noyr\" target=\"_blank\">https://wandb.ai/phylliida/lit-wandb/runs/1y15noyr</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# setup model\n",
    "model = LitMLP(in_dims=(1, 28, 28))\n",
    "\n",
    "# fit the model\n",
    "trainer.fit(model, mnist)\n",
    "\n",
    "# evaluate the model on a test set\n",
    "trainer.test(datamodule=mnist,\n",
    "             ckpt_path=None)  # uses last-saved model\n",
    "\n",
    "# Note: When visiting your run page, it is recommended to use global_step as x-axis to correctly superimpose metrics logged in different stages.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvhwTDtNX0z3"
   },
   "source": [
    "> _Note_: In notebooks, we need to call `wandb.finish()` when to indicate that we've finished our run. This isn't necessary in scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_mz7Evif77l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BaseTemplate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
