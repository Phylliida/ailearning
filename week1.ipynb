{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Valid/Test fundamentals:\n",
    "\n",
    "Train set is used for training the model\n",
    "\n",
    "Validation set is used for checking if the model is overfitting on the training set. Having training loss lower than validation loss is fine, you are overfitting once your validation loss starts getting worse.\n",
    "\n",
    "There's a trick called \"flooding\" where you start doing gradient ascent once your training loss gets too low. I think this makes more sense to start doing once your validation loss starts increasing. It's worth looking into how momentum opts and such should maybe be tweaked for when you do this ascent.\n",
    "\n",
    "Test set should only be used once, and is used to see if your model can generalize to real world data it hasn't seen before. If you are careful and use differential privacy you can actually use it more than once (about Sqrt(n) times iirc) if you are okay with not getting an accuracy, and instead only getting a bit saying whether accuracy is significantly different from validation performance, see https://arxiv.org/abs/1506.02629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "torch.min(torch.tensor([0.0]), torch.normal(0, 10, [10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelpfulModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._myHyperParams = {}\n",
    "        \n",
    "    def __setattr__(self, attr, val):\n",
    "        super().__setattr__(attr, val) # make sure to call super because torch.nn.Module also overrides this\n",
    "        simpleTypes = [int, str, float]\n",
    "        if type(val) in simpleTypes or (type(val) is list and (len(val) == 0 or type(val[0]) in simpleTypes)):\n",
    "            self._myHyperParams[attr] = val\n",
    "            \n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return \", \".join([(str(param) + \": \" + str(val)) for param, val in self._myHyperParams.items()])\n",
    "            \n",
    "\n",
    "class FeedforwardLayer(HelpfulModule):\n",
    "    def __init__(self, inSize, outSize):\n",
    "        super().__init__()\n",
    "        self.inSize = inSize\n",
    "        self.outSize = outSize\n",
    "        self.weights = nn.Parameter(torch.normal(0, 1, [inSize, outSize]))\n",
    "        self.bias = nn.Parameter(torch.normal(0, 1, [outSize]))\n",
    "    def forward(self, x):\n",
    "        res = x@self.weights+self.bias\n",
    "        return res\n",
    "    \n",
    "\n",
    "class SoftRELULayer(HelpfulModule):\n",
    "    def __init__(self, weightLess, offset):\n",
    "        super().__init__()\n",
    "        self.weightLess = weightLess\n",
    "        self.offset = offset\n",
    "    \n",
    "    def forward(self, x):\n",
    "        biggerThan = torch.max(torch.tensor([0.0]), x)\n",
    "        lessThan = torch.min(torch.tensor([0.0]), x)\n",
    "        return biggerThan + lessThan*self.weightLess - self.offset\n",
    "\n",
    "# Uses log-sum-exp trick.\n",
    "# see https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons section on softmax\n",
    "# This is nice for when you know each data point has exactly one label\n",
    "# Returns a value from 0-1, and guarantees sum of values is roughly 1.0\n",
    "class SoftmaxLayer(HelpfulModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        maxVal, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        logValDenominator = maxVal+(x-maxVal).exp().sum(axis=1, keepdim=True).log()\n",
    "        logValNumerator = x\n",
    "        return (logValNumerator - logValDenominator).exp()\n",
    "    \n",
    "class EmbeddingLayer(HelpfulModule):\n",
    "    def __init__(self, nClasses, embeddingDim):\n",
    "        super().__init__()\n",
    "        self.nClasses, self.embeddingDim = nClasses, embeddingDim\n",
    "        # Todo: what is good initialization for embeddings?\n",
    "        self.embeddings = nn.Parameter(torch.normal([nClasses, embeddingDim]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x # TODO: embedding\n",
    "    \n",
    "# This is nice for when your data might have more than one label\n",
    "# returns a value from 0-1, but sum of values might be anything\n",
    "class SigmoidLayer(HelpfulModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 1.0/(1.0+(-x).exp())\n",
    "        \n",
    "\n",
    "class FixupLayer(HelpfulModule):\n",
    "    def __init__(self, layer, fixupIters, fixupBatchSize, eps=0.01):\n",
    "        super().__init__()\n",
    "        assert fixupBatchSize>1, \"Fixup batch size needs to be greater than one to compute std\"\n",
    "        self.fixupIters, self.fixupBatchSize = fixupIters, fixupBatchSize\n",
    "        self.layer = layer\n",
    "        x = layer.generateInputData(fixupBatchSize)\n",
    "        layerOutput = layer(x)\n",
    "        layerOutputShape = list(layerOutput.shape)[1:]\n",
    "        self.avgStd = torch.ones(layerOutputShape)\n",
    "        self.avgMean = torch.zeros(layerOutputShape)\n",
    "        \n",
    "        for i in range(fixupIters):\n",
    "            x = layer.generateInputData(fixupBatchSize)\n",
    "            y = layer(x)\n",
    "            self.avgStd += y.std(axis=0)\n",
    "            self.avgMean += y.mean(axis=0)\n",
    "        \n",
    "        self.avgStd /= float(fixupIters)\n",
    "        self.avgMean /= float(fixupIters)\n",
    "        \n",
    "        self.avgStd = torch.clamp(self.avgStd, min=eps)\n",
    "        \n",
    "        \n",
    "        # This will ensure every activation has mean 0 std 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (self.layer(x)-self.avgMean)/self.avgStd\n",
    "            \n",
    "        \n",
    "        \n",
    "class DenseLayer(HelpfulModule):\n",
    "    def __init__(self, inSize, outSize, act):\n",
    "        super().__init__()\n",
    "        self.inSize, self.outSize, self.act = inSize, outSize, act\n",
    "        self.feedforward = FeedforwardLayer(inSize, outSize)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.feedforward(x))\n",
    "    \n",
    "    def generateInputData(self, bs):\n",
    "        return torch.normal(0, 1, [bs, self.inSize])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.normal(0, 1, [3, 4, 5, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple math for a single row (don't need to worry about axes)\n",
    "def softmaxSingleRow(x):\n",
    "    maxOfRow = torch.max(x)\n",
    "    denominator = maxOfRow + (x - maxOfRow).exp().sum().log()\n",
    "    numerator = x\n",
    "    return (numerator - denominator).exp()\n",
    "\n",
    "def softmaxTests(debug=False):\n",
    "    batchSize=5\n",
    "    def debugPrint(*args, **kwargs):\n",
    "        if debug: print(*args, **kwargs)\n",
    "    a = torch.normal(0, 1, [batchSize, 4])\n",
    "    debugPrint(a)\n",
    "    maxes = torch.max(a, dim=1, keepdim=True)[0]\n",
    "    debugPrint(maxes)\n",
    "    assert(torch.all(a-maxes-0.00001<=0))\n",
    "    debugPrint(a - maxes)\n",
    "    debugPrint((a - maxes - (a-maxes).exp().sum(axis=1, keepdim=True).log()).exp().sum(axis=1))\n",
    "    sm = SoftmaxLayer()\n",
    "    y = sm(a)\n",
    "    debugPrint(y)\n",
    "    debugPrint(y.sum(axis=0), y.sum(axis=0).shape)\n",
    "    debugPrint(y.sum(axis=1), y.sum(axis=1).shape)\n",
    "    debugPrint(y[0].sum(), softmaxSingleRow(a[0]).sum())\n",
    "    debugPrint(y[1].sum(), softmaxSingleRow(a[1]).sum())\n",
    "    debugPrint(softmaxSingleRow(a[0]), y[0])\n",
    "    debugPrint(softmaxSingleRow(a[1]), y[1])\n",
    "    # check to make sure that we are doing the right thing per batch\n",
    "    approx_equals(softmaxSingleRow(a[0]), y[0])\n",
    "    approx_equals(softmaxSingleRow(a[1]), y[1])\n",
    "    # check that each batch summed is roughly 1.0\n",
    "    approx_equals(y.sum(axis=1), torch.ones([batchSize]))\n",
    "\n",
    "softmaxTests(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialLayer(HelpfulModule):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, layer in enumerate(self.layers): self._modules[str(i)] = layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for layer in self.layers:\n",
    "            res = layer(res)\n",
    "        return res\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNet(HelpfulModule):\n",
    "    def __init__(self, inSize, hiddenSizes, outSize, act, finalAct, fixupIters, fixupBs):\n",
    "        super().__init__()\n",
    "        self.inSize, self.hiddenSizes, self.outSize, self.act, self.finalAct = inSize, hiddenSizes, outSize, act, finalAct\n",
    "        allSizes = [inSize] + hiddenSizes + [outSize]\n",
    "        self.layers = [DenseLayer(allSizes[i], allSizes[i+1], act) if i < len(allSizes)-2 else\n",
    "                          DenseLayer(allSizes[i], allSizes[i+1], finalAct)\n",
    "                       for i in range(len(allSizes)-1) ]\n",
    "        self.layers = SequentialLayer(*[FixupLayer(layer, fixupBs, fixupIters) for layer in self.layers])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchStats(x):\n",
    "    return x.mean(axis=0), x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixupLayer(\n",
      "  fixupIters: 1000, fixupBatchSize: 2\n",
      "  (layer): DenseLayer(\n",
      "    inSize: 12, outSize: 10\n",
      "    (act): SoftmaxLayer()\n",
      "    (feedforward): FeedforwardLayer(inSize: 12, outSize: 10)\n",
      "  )\n",
      ")\n",
      "Stats (tensor([ 0.0060,  0.0358, -0.0072,  0.0036,  0.0264, -0.0456,  0.0774, -0.0175,\n",
      "        -0.0044,  0.0612], grad_fn=<MeanBackward1>), tensor([2.1745, 1.9042, 2.0162, 1.6359, 1.9917, 1.4028, 2.6778, 1.9228, 1.5940,\n",
      "        3.2203], grad_fn=<StdBackward1>))\n",
      "Biases: tensor([0.0491, 0.0976, 0.0733, 0.1604, 0.0897, 0.2238, 0.0371, 0.0848, 0.1642,\n",
      "        0.0200], grad_fn=<DivBackward0>) tensor([0.0611, 0.1091, 0.0877, 0.1739, 0.1110, 0.2189, 0.0471, 0.0959, 0.1787,\n",
      "        0.0264], grad_fn=<ClampBackward>)\n"
     ]
    }
   ],
   "source": [
    "batchSize = 100000\n",
    "inputSize = 12\n",
    "hiddenSizes = [4,5,6,7]\n",
    "outputSize = 10\n",
    "fixupIters = 1000\n",
    "fixupBs = 2\n",
    "act = SoftRELULayer(weightLess=0.5, offset=0.5)\n",
    "finalAct = SoftmaxLayer()\n",
    "x = torch.normal(0, 1, [batchSize, inputSize])\n",
    "#net = FeedforwardNet(inputSize, hiddenSizes, outputSize, act, finalAct, fixupIters, fixupBs)\n",
    "dense = DenseLayer(inputSize, outputSize, finalAct)\n",
    "net = FixupLayer(dense, fixupIters, fixupBs)\n",
    "print(net)\n",
    "denseOutput = dense(x)\n",
    "y = net(x)\n",
    "#print(\"outputs:\", y[0], y[1])\n",
    "print(\"Stats\", batchStats(net(x)))\n",
    "print(\"Biases:\", net.avgMean, net.avgStd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9983)\n",
      "tensor([ 0.3440,  1.4775, -0.2170,  1.1752]) tensor(0.6949)\n",
      "torch.Size([1, 5]) tensor([[ 0.6949,  0.4711,  0.8089, -0.7772,  0.1219]])\n",
      "tensor([[ 0.3440,  0.7092,  0.6088, -0.6691,  1.2032],\n",
      "        [ 1.4775, -0.1609,  0.7756, -0.9842, -0.8903],\n",
      "        [-0.2170,  0.1969,  1.0542, -1.1420,  0.1661],\n",
      "        [ 1.1752,  1.1392,  0.7972, -0.3135,  0.0085]]) tensor([[-0.3509,  0.2381, -0.2002,  0.1081,  1.0813],\n",
      "        [ 0.7826, -0.6320, -0.0333, -0.2070, -1.0122],\n",
      "        [-0.9119, -0.2742,  0.2452, -0.3648,  0.0442],\n",
      "        [ 0.4803,  0.6681, -0.0118,  0.4637, -0.1133]]) tensor([[0.4495, 0.2444, 0.0254, 0.1007, 0.5522]]) tensor([[1.4154, 0.6883, 1.3341, 1.3088, 0.5819]]) hhh tensor([0.4495, 0.2444, 0.0254, 0.1007, 0.5522])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.0657), tensor(0.2744))"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variance(a, **kwargs):\n",
    "    return (a.pow(2)).mean(**kwargs) + a.mean(**kwargs).pow(2)\n",
    "\n",
    "def variance2(a, **kwargs):\n",
    "    return (a-a.mean(keepdim=True, **kwargs)).pow(2).mean(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(variance2(torch.normal(0, 1, [1000]), axis=0))\n",
    "\n",
    "a = torch.normal(0, 1, [4, 5])\n",
    "f = a.mean(axis=0, keepdim=True)\n",
    "f.pow(2).mean()\n",
    "print(a[:,0], a[:,0].mean())\n",
    "print(f.shape, f)\n",
    "print(a, a - a.mean(keepdim=True, axis=0), (a-a.mean(keepdim=True, axis=0)).pow(2).mean(keepdim=True, axis=0), variance(a, keepdim=True, axis=0), \"hhh\", variance2(a, axis=0))\n",
    "\n",
    "variance(a, axis=0).mean(), variance2(a, axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_equals(a, b):\n",
    "    assert torch.allclose(a, b, 0.0001), str(a) + \"!=\" + str(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFeedforward(debug=False):\n",
    "    def debugPrint(*args, **kwargs):\n",
    "        if debug: print(*args, **kwargs)\n",
    "    batchSize = 3\n",
    "    inputSize = 2\n",
    "    hiddenSize = 4\n",
    "    x = torch.normal(0, 1, [batchSize, inputSize])\n",
    "    debugPrint(x)\n",
    "    layer = FeedforwardLayer(inputSize, hiddenSize)\n",
    "    for p in layer.parameters():\n",
    "        debugPrint(p)\n",
    "    y = layer(x)\n",
    "    debugPrint(y)\n",
    "    weights = layer.weights\n",
    "    bias = layer.bias\n",
    "    firstOutputBatch1 = x[0]@layer.weights[:,0]+layer.bias[0]\n",
    "    secondOutputBatch1 = x[0]@layer.weights[:,1]+layer.bias[1]\n",
    "    firstOutputBatch2 = x[1]@layer.weights[:,0]+layer.bias[0]\n",
    "    secondOutputBatch2 = x[1]@layer.weights[:,1]+layer.bias[1]\n",
    "    debugPrint(x[0],\"*\",layer.weights[:,0],\"+\",layer.bias[0], \"=\", firstOutputBatch1) \n",
    "    debugPrint(x[0],\"*\",layer.weights[:,1],\"+\",layer.bias[1], \"=\", secondOutputBatch1)\n",
    "    debugPrint(x[1],\"*\",layer.weights[:,0],\"+\",layer.bias[0], \"=\", firstOutputBatch2) \n",
    "    debugPrint(x[1],\"*\",layer.weights[:,1],\"+\",layer.bias[1], \"=\", secondOutputBatch2) \n",
    "    approx_equals(firstOutputBatch1, y[0,0])\n",
    "    approx_equals(secondOutputBatch1, y[0,1])\n",
    "    approx_equals(firstOutputBatch2, y[1,0])\n",
    "    approx_equals(secondOutputBatch2, y[1,1])\n",
    "    \n",
    "testFeedforward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
