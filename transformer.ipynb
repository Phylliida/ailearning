{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "import numpy as np\n",
    "def approx_equals(a, b):\n",
    "    assert torch.allclose(a, b, 0.0001), str(a) + \"!=\" + str(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automataBattle\n",
    "import random\n",
    "from importlib import reload\n",
    "reload(automataBattle)\n",
    "from torch.utils.data import Dataset\n",
    "class FastLearnAutomataDataset(Dataset):\n",
    "    def __init__(self, nStates, nSymbols, split, sequenceLen, numSequences):\n",
    "        self.nStates = nStates\n",
    "        self.nSymbols = nSymbols\n",
    "        self.split = split # train/test\n",
    "        self.vocab_size = nSymbols*nSymbols\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = sequenceLen\n",
    "        \n",
    "        self.sequenceLen, self.numSequences = sequenceLen, numSequences\n",
    "        \n",
    "        '''\n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "        '''\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.numSequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        a = automataBattle.Automata(nStates=self.nStates, symbols=range(self.nSymbols), randomConnect=True)\n",
    "        a.minimize()\n",
    "        while a.complexity() != self.nStates:\n",
    "            a = automataBattle.Automata(nStates=self.nStates, symbols=range(self.nSymbols), randomConnect=True)\n",
    "            a.minimize()\n",
    "        X, Y = a.generate(self.sequenceLen+1, lambda: random.choice(range(self.nSymbols)))\n",
    "        x = torch.tensor(X)\n",
    "        y = torch.tensor(Y) # predict the output of the Automata\n",
    "        previous = y[:-1]\n",
    "        shiftedForwadInputsOne = x[1:]\n",
    "        outputs = y[1:] # Todo: look into encoding multiple things (\"tuple encodings\") instead of this gross thing\n",
    "        xOutput = shiftedForwadInputsOne+previous*self.nSymbols\n",
    "        yOutput = outputs\n",
    "        return xOutput, yOutput\n",
    "        \n",
    "        '''\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HelpfulModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._myHyperParams = {}\n",
    "        \n",
    "    def __setattr__(self, attr, val):\n",
    "        super().__setattr__(attr, val) # make sure to call super because torch.nn.Module also overrides this\n",
    "        simpleTypes = [int, str, float]\n",
    "        if type(val) in simpleTypes or (type(val) is list and (len(val) == 0 or type(val[0]) in simpleTypes)):\n",
    "            self._myHyperParams[attr] = val\n",
    "            \n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return \", \".join([(str(param) + \": \" + str(val)) for param, val in self._myHyperParams.items()])\n",
    "\n",
    "class SoftRELULayer(HelpfulModule):\n",
    "    def __init__(self, weightLess, offset):\n",
    "        super().__init__()\n",
    "        self.weightLess = weightLess\n",
    "        self.offset = offset\n",
    "    \n",
    "    def forward(self, x):\n",
    "        biggerThan = torch.max(torch.tensor([0.0]), x)\n",
    "        lessThan = torch.min(torch.tensor([0.0]), x)\n",
    "        return biggerThan + lessThan*self.weightLess - self.offset\n",
    "    \n",
    "\n",
    "# TODO: see if batch norm works for transformers\n",
    "\n",
    "\n",
    "class BatchedIndexCrossEntropyLoss(HelpfulModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y, target, rollupLosses=True):\n",
    "        '''\n",
    "        torch.gather(input, dim, index) does the following\n",
    "        out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "        out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "        out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
    "\n",
    "        y is [b,L,vocabSize]\n",
    "        goals is [b,L]\n",
    "        we want\n",
    "        out[bi,l] = y[bi,l,goals[bi,l]]\n",
    "        but that doesn't fit the above pattern.\n",
    "        To fix this, we can just do\n",
    "        out[bi,l,k] = y[bi,l,goals[bi,l,k]]\n",
    "        where k is only ever 0\n",
    "        so we need to add that axis to goals\n",
    "        '''\n",
    "        b,L = target.shape\n",
    "        values = torch.gather(y, 2, target.view((b,L,1)))\n",
    "        # Now make it look like b,L\n",
    "        values = values.view((b,L))\n",
    "        # Actual pr for those values is 1.0, so\n",
    "        # -target*x.log()-(1.0-target)*(1.0-x).log()\n",
    "        # turns into\n",
    "        res = -values.log()\n",
    "        # this gives us one loss per (batch, word), usually they just want a single loss value, so this can roll them up if you want\n",
    "        if rollupLosses: return res.mean()\n",
    "        else: return res\n",
    "\n",
    "class BatchedCrossEntropyLoss(HelpfulModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y, target, rollupLosses=True):\n",
    "        vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
    "        # sum along not batch axis\n",
    "        res = vals.sum(axis=2)\n",
    "        if rollupLosses: return res.mean()\n",
    "        else: return res\n",
    "        # -target[i]*log(x[i])-(1-target[i])*log(1-x[i])\n",
    "\n",
    "class LayerNorm(HelpfulModule):\n",
    "    def __init__(self, eps=0.01):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.multiplicitiveWeight = nn.Parameter(torch.tensor(1.0))\n",
    "        self.additiveWeight = nn.Parameter(torch.tensor(0.0))\n",
    "        self.nBatches = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu = x.mean((1,2,3), keepDim=True)\n",
    "        var = x.var((1,2,3), keepDim=True) # TODO: add correction based on batch size\n",
    "        normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps))\n",
    "        return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
    "\n",
    "class EmbeddingLayer(HelpfulModule):\n",
    "    def __init__(self, vocabSize, embeddingDim):\n",
    "        super().__init__()\n",
    "        self.vocabSize, self.embeddingDim = vocabSize, embeddingDim\n",
    "        # Todo: what is good initialization for embeddings?\n",
    "        self.embeddings = nn.Parameter(torch.normal(0, 1, [vocabSize, embeddingDim]))\n",
    "    # Inputs should be dimension [batchSize] and they should be integers\n",
    "    def forward(self, x):\n",
    "        return self.embeddings[x]\n",
    "    \n",
    "class Transformer(HelpfulModule):\n",
    "    def __init__(self, numHeads, vocabSize, embeddingDim, keyDim, valueDim, hiddenSize, numLayers, **kwargs):\n",
    "        super().__init__()\n",
    "        self.numHeads, self.vocabSize, self.embeddingDim, self.keyDim, self.valueDim, self.hiddenSize, self.numLayers = numHeads, vocabSize, embeddingDim, keyDim, valueDim, hiddenSize, numLayers\n",
    "        n, d, k, v, m = numHeads, embeddingDim, keyDim, valueDim, hiddenSize\n",
    "        self.n, self.d, self.k, self.v, self.m = n,d,k,v,m\n",
    "        self.embedding = EmbeddingLayer(vocabSize, embeddingDim)\n",
    "        self.encodingLayers = nn.Sequential(*[TransformerBlock(n,d,k,v,m,**kwargs) for _ in range(numLayers)])\n",
    "        self.finalProjection = nn.Parameter(torch.normal(0, 1, [n*d, vocabSize]))\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.batchedPrLoss = BatchedCrossEntropyLoss()\n",
    "        self.batchedIndexLoss = BatchedIndexCrossEntropyLoss()\n",
    "        # TODO: positional encodings\n",
    "    \n",
    "    def configure_optimizers(self, config):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, targets=None, rollupLosses=True):\n",
    "        # x is of size [b,L], word integer indices\n",
    "        if len(x.shape) == 1: # make everythingn work for batch size 1\n",
    "            x = x.view((1,x.shape[0]))\n",
    "        b, L = x.shape\n",
    "        \n",
    "        n,d = self.n, self.d\n",
    "        \n",
    "        embeddedOutputs = self.embedding(x)\n",
    "        # embeddedOutputs is of size [b,L,d]\n",
    "        # we need to make it [b,L,n,d], so we can just use expand to make it look that size\n",
    "        expandedEmbeddedOutputs = embeddedOutputs.view((b,L,1,d)).expand((b, L, n, d))\n",
    "        # now it's ready to go through the embeddings\n",
    "        # It's currently dim [b,L,n,d], we need to make it [b,L,vocabSize]\n",
    "        # For now I will just flatten and then project, so first make it [b,L,n*d]\n",
    "        flattenedOutputs = expandedEmbeddedOutputs.reshape((b,L,n*d))\n",
    "        # project to [b,L,vocabSize]\n",
    "        finalProj = torch.einsum(\"iv,bli->blv\", self.finalProjection, flattenedOutputs)\n",
    "        # Use softmax to convert to prs\n",
    "        wordPrs = self.softmax(finalProj)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            if targets.dtype == torch.int64: # fitting to desired word indices\n",
    "                if len(targets.shape) == 1: # if single batch, expand out\n",
    "                    targets = targets.view((1, targets.shape[0]))\n",
    "                loss = self.batchedIndexLoss(wordPrs, targets, rollupLosses=rollupLosses)\n",
    "            else: # fitting to word prs\n",
    "                if len(targets.shape) == 2: # if single batch, expand out\n",
    "                    targets = targets.view((1, targets.shape[0], targets.shape[1]))\n",
    "                loss = self.batchedPrLoss(wordPrs, targets, rollupLosses=rollupLosses)\n",
    "        \n",
    "        return wordPrs, loss\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "                else:\n",
    "                    decay.add(fpn)\n",
    "\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "        \n",
    "\n",
    "# Transformers work by processing each word in turn\n",
    "\n",
    "class MultiHeadSelfAttention(HelpfulModule):\n",
    "    def __init__(self, n, d, k, v):\n",
    "        super().__init__()\n",
    "        self.n, self.d, self.kDim, self.vDim = n,d,k,v\n",
    "        # Todo: compute initialization scaling factors\n",
    "        # TODO: What about more things than just QKV? Like four or five or something\n",
    "        self.Q = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.K = nn.Parameter(torch.normal(0, 1, [k, d]))\n",
    "        self.V = nn.Parameter(torch.normal(0, 1, [v, d]))\n",
    "        self.Wch = nn.Parameter(torch.normal(0, 1, [d, v]))\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        # x is [b,L,n,d]\n",
    "        # b is batch size\n",
    "        # L is sentence length\n",
    "        # n is num heads\n",
    "        # d is embedding dimension\n",
    "        # we need to use Q, K, V to make a qi, ki, vi for each word\n",
    "        # because we dot qi and kj, they need to be same dim, call this k\n",
    "        # vi can be any dim, call this v\n",
    "        # we need [b,L,n,d] -> [b,L,n,k] for qi and ki\n",
    "        # we need [b,L,n,d] -> [b,L,n,v] for vi\n",
    "        # [b,L,n,k]\n",
    "        \n",
    "        b,L,n,d,kDim,vDim = x.shape[0], x.shape[1], self.n, self.d, self.kDim, self.vDim\n",
    "        \n",
    "        q = torch.einsum(\"kd,blnd->blnk\", self.Q, x)\n",
    "        k = torch.einsum(\"kd,blnd->blnk\", self.K, x)\n",
    "        v = torch.einsum(\"vd,blnd->blnv\", self.V, x)\n",
    "        # Normally people just do a massive matrix, but that is quadratic in terms of L, and very wasteful with memory\n",
    "        # Instead, we will do a loop over each word and do this for each word.\n",
    "        # It's still quadratic in terms of L for time complexity (and slightly slower than giant matrix, because we are in python), but now linear in terms of space complexity, which is important for GPU space\n",
    "        inds = torch.tensor(range(L))\n",
    "        u = torch.zeros([b,L,n,vDim])\n",
    "        for i in range(L):\n",
    "            # q is [b,L,n,k]\n",
    "            # expand it so it looks as k so we can do dot product\n",
    "            qi = q[:,i,:,:].view((b,1,n,kDim)).expand((b,L,n,kDim))\n",
    "            # dot product is component wise product and then sum, so just do that\n",
    "            # scores is now [b,L,n]\n",
    "            scores = (qi*k).sum(axis=3)/math.sqrt(kDim) # also divide by sqrt(k)\n",
    "            scores[:,inds>i,:] = np.NINF # mask out words after current word\n",
    "            scores = self.softmax(scores)\n",
    "            # scores is [b,L,n], we need to make it look like [b,L,n,1] so we can expand it along last axis \n",
    "            scores = scores.view((b,L,n,1)).expand((b,L,n,vDim))\n",
    "            ui = (scores*v).sum(axis=1)\n",
    "            u[:,i,:,:] = ui\n",
    "        # u is [b,L,n,vDim]\n",
    "        # we want [b,L,n,d]\n",
    "        return torch.einsum(\"dv,blnv->blnd\", self.Wch, u)         \n",
    "\n",
    "    \n",
    "class TransformerBlock(HelpfulModule):\n",
    "    def __init__(self, n, d, k, v, m, **kwargs):\n",
    "        super().__init__()\n",
    "        # input x is [b,n,d]\n",
    "        # b is batchSize\n",
    "        # n is number of heads\n",
    "        # d is embedding dimension\n",
    "        # k is key size\n",
    "        # m is hidden layer size\n",
    "        self.n, self.d, self.k, self.m = n, d, k, m\n",
    "        self.W1 = nn.Parameter(torch.normal(0, 1, [m, d]))\n",
    "        self.W2 = nn.Parameter(torch.normal(0, 1, [d, m]))\n",
    "        self.attention = MultiHeadSelfAttention(n,d,k,v)\n",
    "        self.layerNorm1 = LayerNorm()\n",
    "        self.layerNorm2 = LayerNorm()\n",
    "        self.RELU = SoftRELULayer(**kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attentionOut = self.attention(x)\n",
    "        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
    "        # [d,m]x[b,n,d] -> [b,n,m]\n",
    "        denseOutput = self.RELU(torch.einsum(\"md,bnd->bnm\", self.W1, ui))\n",
    "        projectedBack = torch.einsum(\"dm,bnm->bnd\", self.W2, denseOutput)\n",
    "        return self.layerNorm2(ui+denseOutput)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 892 loss:tensor(27138.2090, grad_fn=<MeanBackward0>):   0%|          | 893/300000 [00:23<2:08:32, 38.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-290-97c62451315f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtryTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-290-97c62451315f>\u001b[0m in \u001b[0;36mtryTrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoaderSimple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# collapse all losses if they are scattered on multiple gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-290-97c62451315f>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcurBatchY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcurBatchX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-6058141e45fd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautomataBattle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnStates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnStates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnSymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomConnect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequenceLen\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnSymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict the output of the Automata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openai/openai_learning/automataBattle.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, maxTokens, inputGenerator)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0minputSymbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0minputSymbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mmyOutputSymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputSymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0minputSymbols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputSymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-6058141e45fd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautomataBattle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnStates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnStates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnSymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomConnect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequenceLen\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnSymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict the output of the Automata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;34m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sandbox1/lib/python3.8/random.py\u001b[0m in \u001b[0;36m_randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgetrandbits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrandbits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# don't use (n-1) here because n can be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# 0 <= r < 2**k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DataLoaderSimple(object):\n",
    "    def __init__(self, datas, batchSize):\n",
    "        self.datas = datas\n",
    "        self.batchSize = batchSize\n",
    "        self.index = 0\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        curBatchX = []\n",
    "        curBatchY = []\n",
    "        for i in range(self.batchSize):\n",
    "            x,y = self.datas[i]\n",
    "            i += 1\n",
    "            curBatchX.append(x)\n",
    "            curBatchY.append(y)\n",
    "        return torch.stack(curBatchX), torch.stack(curBatchY)\n",
    "import minGPT.mingpt.trainer\n",
    "reload(minGPT.mingpt.trainer)\n",
    "from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "from importlib import reload\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "def tryTrain():\n",
    "    batchSize = 64\n",
    "    seqLen = 100\n",
    "    numHeads = 4\n",
    "    embeddingDim = 16\n",
    "    keyDim = 16\n",
    "    valueDim = 16\n",
    "    vocabSize=4\n",
    "    hiddenSize=64\n",
    "    numLayers = 8\n",
    "    b,L,n,d,k,v = batchSize,seqLen,numHeads,embeddingDim,keyDim,valueDim\n",
    "    weightLess = 0.5\n",
    "    offset = 0.5\n",
    "    model = Transformer(numHeads=numHeads, vocabSize=vocabSize, embeddingDim=embeddingDim, keyDim=keyDim, valueDim=valueDim, hiddenSize=hiddenSize, numLayers=numLayers, weightLess=weightLess, offset=offset)\n",
    "    \n",
    "    train_dataset = FastLearnAutomataDataset(nStates=2, nSymbols=2, split='train', sequenceLen=seqLen, numSequences=300000)\n",
    "    test_dataset = FastLearnAutomataDataset(nStates=2, nSymbols=2, split='test', sequenceLen=seqLen, numSequences=1000)\n",
    "    \n",
    "    tconf = TrainerConfig(max_epochs=100, batch_size=batchSize, learning_rate=6e-5,\n",
    "                          lr_decay=True, warmup_tokens=2048, final_tokens=50*len(train_dataset)*(2+1),\n",
    "                          num_workers=0)\n",
    "    optimizer = model.configure_optimizers(tconf)\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "    loader = DataLoaderSimple(train_dataset, batchSize)\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, (x,y) in pbar:\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), tconf.grad_norm_clip)\n",
    "        optimizer.step()\n",
    "        pbar.set_description(\"epoch: \" + str(i) + \" loss:\" + str(loss))\n",
    "    x, y = train_dataset[0]\n",
    "    print(x,y)\n",
    "    print(trainer.device)\n",
    "    output, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    trainer.train()\n",
    "    \n",
    "\n",
    "tryTrain()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets tensor([[[0.1594, 0.0643, 0.3269, 0.1214, 0.1380, 0.0313, 0.0908, 0.0361,\n",
      "          0.0319],\n",
      "         [0.0869, 0.1340, 0.1009, 0.0286, 0.2093, 0.2145, 0.0711, 0.0465,\n",
      "          0.1082],\n",
      "         [0.1025, 0.1424, 0.1804, 0.1474, 0.0647, 0.1346, 0.0705, 0.1309,\n",
      "          0.0266]],\n",
      "\n",
      "        [[0.4072, 0.0696, 0.1560, 0.0229, 0.0523, 0.2354, 0.0202, 0.0146,\n",
      "          0.0217],\n",
      "         [0.0646, 0.1708, 0.0111, 0.0682, 0.0167, 0.0469, 0.5477, 0.0572,\n",
      "          0.0167],\n",
      "         [0.0370, 0.0320, 0.1552, 0.1518, 0.0717, 0.1811, 0.0378, 0.2584,\n",
      "          0.0751]]]) torch.Size([2, 3, 9]) tensor([0.1594, 0.0643, 0.3269, 0.1214, 0.1380, 0.0313, 0.0908, 0.0361, 0.0319]) tensor(1.)\n",
      "y tensor([[[3.5294e-02, 3.5906e-05, 2.7673e-07, 5.7191e-04, 1.4594e-05,\n",
      "          1.1280e-01, 1.2557e-07, 9.1254e-03, 8.4216e-01],\n",
      "         [4.7185e-02, 2.7636e-08, 5.4642e-05, 9.0173e-03, 8.6240e-04,\n",
      "          9.2805e-01, 1.3350e-02, 3.9484e-06, 1.4714e-03],\n",
      "         [1.8485e-10, 4.1149e-04, 6.1090e-07, 7.9660e-01, 1.2047e-07,\n",
      "          8.1938e-08, 7.9275e-08, 2.4302e-06, 2.0298e-01]],\n",
      "\n",
      "        [[1.8485e-10, 4.1149e-04, 6.1090e-07, 7.9660e-01, 1.2047e-07,\n",
      "          8.1938e-08, 7.9275e-08, 2.4302e-06, 2.0298e-01],\n",
      "         [1.9005e-04, 1.0504e-01, 4.4672e-03, 9.5232e-04, 7.1638e-01,\n",
      "          8.1508e-07, 1.5658e-01, 1.5610e-02, 7.7927e-04],\n",
      "         [1.8789e-01, 1.2859e-08, 2.3556e-03, 1.3719e-06, 1.0189e-03,\n",
      "          8.0286e-01, 1.2021e-04, 5.2928e-03, 4.5945e-04]]],\n",
      "       grad_fn=<SoftmaxBackward>) torch.Size([2, 3, 9]) (2, 3, 9)\n",
      "losses tensor(66.6813, grad_fn=<SumBackward0>) torch.Size([])\n",
      "dats: (tensor([1, 3, 2]), tensor([1, 1, 1])) 3\n",
      "datas: tensor([[3, 3, 1],\n",
      "        [0, 3, 3]]) tensor([[1, 0, 0],\n",
      "        [1, 1, 1]]) torch.Size([2, 3]) torch.Size([2, 3])\n",
      "y2, losses2 tensor([[[ 0.0389,  0.1500,  0.0716,  0.0134,  0.0820, -0.0084,  0.0004,\n",
      "          -0.0834,  0.0041],\n",
      "         [ 0.0246,  0.0442,  0.0340, -0.0107,  0.0750, -0.0325,  0.0033,\n",
      "          -0.0570,  0.0289],\n",
      "         [-0.0152,  0.0179, -0.0332, -0.0127,  0.0117,  0.0119,  0.0165,\n",
      "          -0.0454,  0.0061]],\n",
      "\n",
      "        [[ 0.0408,  0.0679,  0.0210,  0.0531, -0.0131,  0.0282, -0.0297,\n",
      "          -0.0565,  0.0030],\n",
      "         [ 0.0236,  0.0685,  0.0406, -0.0087,  0.0801, -0.0004, -0.0023,\n",
      "          -0.0640,  0.0349],\n",
      "         [ 0.0246,  0.0404,  0.0145, -0.0468,  0.0721,  0.0330, -0.0188,\n",
      "          -0.0250,  0.0436]]], grad_fn=<UnsafeViewBackward>) torch.Size([2, 3, 9]) tensor(2.1561, grad_fn=<NllLossBackward>) torch.Size([])\n",
      "y3, losses3 tensor([[[3.5294e-02, 3.5906e-05, 2.7673e-07, 5.7191e-04, 1.4594e-05,\n",
      "          1.1280e-01, 1.2557e-07, 9.1254e-03, 8.4216e-01],\n",
      "         [3.5294e-02, 3.5906e-05, 2.7673e-07, 5.7191e-04, 1.4594e-05,\n",
      "          1.1280e-01, 1.2557e-07, 9.1254e-03, 8.4216e-01],\n",
      "         [1.8485e-10, 4.1149e-04, 6.1090e-07, 7.9660e-01, 1.2047e-07,\n",
      "          8.1938e-08, 7.9275e-08, 2.4302e-06, 2.0298e-01]],\n",
      "\n",
      "        [[1.9005e-04, 1.0504e-01, 4.4672e-03, 9.5232e-04, 7.1638e-01,\n",
      "          8.1508e-07, 1.5658e-01, 1.5610e-02, 7.7927e-04],\n",
      "         [3.5294e-02, 3.5906e-05, 2.7673e-07, 5.7191e-04, 1.4594e-05,\n",
      "          1.1280e-01, 1.2557e-07, 9.1254e-03, 8.4216e-01],\n",
      "         [3.5294e-02, 3.5906e-05, 2.7673e-07, 5.7191e-04, 1.4594e-05,\n",
      "          1.1280e-01, 1.2557e-07, 9.1254e-03, 8.4216e-01]]],\n",
      "       grad_fn=<SoftmaxBackward>) torch.Size([2, 3, 9]) tensor(58.7127, grad_fn=<SumBackward0>) torch.Size([])\n",
      "tensor([[[ 0.0389,  0.1500,  0.0716,  0.0134,  0.0820, -0.0084,  0.0004,\n",
      "          -0.0834,  0.0041],\n",
      "         [ 0.0246,  0.0442,  0.0340, -0.0107,  0.0750, -0.0325,  0.0033,\n",
      "          -0.0570,  0.0289],\n",
      "         [-0.0152,  0.0179, -0.0332, -0.0127,  0.0117,  0.0119,  0.0165,\n",
      "          -0.0454,  0.0061]],\n",
      "\n",
      "        [[ 0.0408,  0.0679,  0.0210,  0.0531, -0.0131,  0.0282, -0.0297,\n",
      "          -0.0565,  0.0030],\n",
      "         [ 0.0236,  0.0685,  0.0406, -0.0087,  0.0801, -0.0004, -0.0023,\n",
      "          -0.0640,  0.0349],\n",
      "         [ 0.0246,  0.0404,  0.0145, -0.0468,  0.0721,  0.0330, -0.0188,\n",
      "          -0.0250,  0.0436]]], grad_fn=<UnsafeViewBackward>) torch.Size([2, 3, 9])\n",
      "spooked tensor([[ 0.0389,  0.1500,  0.0716,  0.0134,  0.0820, -0.0084,  0.0004, -0.0834,\n",
      "          0.0041],\n",
      "        [ 0.0246,  0.0442,  0.0340, -0.0107,  0.0750, -0.0325,  0.0033, -0.0570,\n",
      "          0.0289],\n",
      "        [-0.0152,  0.0179, -0.0332, -0.0127,  0.0117,  0.0119,  0.0165, -0.0454,\n",
      "          0.0061],\n",
      "        [ 0.0408,  0.0679,  0.0210,  0.0531, -0.0131,  0.0282, -0.0297, -0.0565,\n",
      "          0.0030],\n",
      "        [ 0.0236,  0.0685,  0.0406, -0.0087,  0.0801, -0.0004, -0.0023, -0.0640,\n",
      "          0.0349],\n",
      "        [ 0.0246,  0.0404,  0.0145, -0.0468,  0.0721,  0.0330, -0.0188, -0.0250,\n",
      "          0.0436]], grad_fn=<ViewBackward>) torch.Size([6, 9])\n"
     ]
    }
   ],
   "source": [
    "def testTransformer7():\n",
    "    b,L,n,d,k,v,vocabSize, hiddenSize, numLayers = 2,3,4,8,6,7,9, 10, 11\n",
    "    weightLess = 0.5\n",
    "    offset = 0.5\n",
    "    numHeads, vocabSize, embeddingDim, keyDim, valueDim, hiddenSize, numLayers = n, vocabSize, d, k, v, hiddenSize, numLayers \n",
    "    gpt = Transformer(numHeads=numHeads, vocabSize=vocabSize, embeddingDim=embeddingDim, keyDim=keyDim, valueDim=valueDim, hiddenSize=hiddenSize, numLayers=numLayers, weightLess=weightLess, offset=offset)\n",
    "    inputs = torch.tensor([[3,4,1],[1,0,2]])\n",
    "    targets = torch.normal(0, 1, [b,L, vocabSize])\n",
    "    sm = nn.Softmax(dim=2)\n",
    "    targets = sm(targets)\n",
    "    print(\"targets\", targets, targets.shape, targets[0,0], targets[0,0].sum())\n",
    "    y, losses = gpt(inputs, targets)\n",
    "    print(\"y\", y, y.shape, (b,L,vocabSize))\n",
    "    assert(y.shape == (b,L,vocabSize))\n",
    "    print(\"losses\", losses, losses.shape)\n",
    "    approx_equals(y.sum(axis=2), torch.ones([b,L]))\n",
    "    from minGPT.mingpt import model\n",
    "    from minGPT.mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "    mconf = GPTConfig(vocabSize, L, n_layer=numLayers, n_head=numHeads, n_embd=embeddingDim)\n",
    "    model = GPT(mconf)\n",
    "    from torch.utils.data.dataloader import DataLoader\n",
    "    data = FastLearnAutomataDataset(nStates=2, nSymbols=2, split='train', sequenceLen=L, numSequences=60)\n",
    "    print(\"dats:\", data[0], data.sequenceLen)\n",
    "    loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                        batch_size=b,\n",
    "                        num_workers=0)\n",
    "    for x,y in loader:\n",
    "        print(\"datas:\", x,y, x.shape, y.shape)\n",
    "        break\n",
    "    y2, losses2 = model(x, y)\n",
    "    y3, losses3 = gpt(x,y)\n",
    "    print(\"y2, losses2\", y2, y2.shape, losses2, losses2.shape)\n",
    "    print(\"y3, losses3\", y3, y3.shape, losses3, losses3.shape)\n",
    "    spooked = y2.view(-1, y2.size(-1))\n",
    "    print(y2, y2.shape)\n",
    "    print(\"spooked\", spooked, spooked.shape)\n",
    "testTransformer7()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[[7.1465e-02, 3.2889e-03, 7.0081e-03, 5.4360e-04, 9.1665e-01,\n",
      "          4.5948e-04, 4.4783e-07, 5.4686e-04, 3.4154e-05],\n",
      "         [3.5865e-05, 5.2502e-05, 9.9611e-01, 7.6170e-08, 1.2315e-03,\n",
      "          1.3414e-06, 2.7552e-05, 3.2243e-04, 2.2150e-03],\n",
      "         [6.8272e-01, 9.1474e-07, 7.7590e-08, 3.2535e-04, 2.5515e-07,\n",
      "          2.9180e-04, 3.0933e-01, 6.8850e-04, 6.6424e-03]],\n",
      "\n",
      "        [[6.8272e-01, 9.1474e-07, 7.7590e-08, 3.2535e-04, 2.5515e-07,\n",
      "          2.9180e-04, 3.0933e-01, 6.8850e-04, 6.6424e-03],\n",
      "         [1.7251e-06, 1.5701e-07, 1.6659e-08, 9.9988e-01, 8.1252e-08,\n",
      "          5.6448e-05, 3.0365e-06, 6.0190e-05, 2.8927e-06],\n",
      "         [1.0939e-05, 3.0040e-03, 1.4954e-04, 2.4149e-04, 1.6799e-04,\n",
      "          3.0257e-02, 9.6423e-01, 1.7110e-03, 2.2970e-04]]],\n",
      "       grad_fn=<SoftmaxBackward>) torch.Size([2, 3, 9])\n",
      "tensor([[[3.2889e-03],\n",
      "         [9.9611e-01],\n",
      "         [6.8272e-01]],\n",
      "\n",
      "        [[7.7590e-08],\n",
      "         [1.5701e-07],\n",
      "         [3.0040e-03]]], grad_fn=<GatherBackward>)\n"
     ]
    }
   ],
   "source": [
    "def testCrossEntropy():\n",
    "    inputs = torch.tensor([[3,4,1],[1,0,2]])\n",
    "    goals = torch.tensor([[1,2,0],[2,1,1]])\n",
    "    b,L,n,d,k,v,vocabSize, hiddenSize, numLayers = 2,3,4,8,6,7,9, 10, 11\n",
    "    weightLess = 0.5\n",
    "    offset = 0.5\n",
    "    numHeads, vocabSize, embeddingDim, keyDim, valueDim, hiddenSize, numLayers = n, vocabSize, d, k, v, hiddenSize, numLayers \n",
    "    gpt = Transformer(numHeads=numHeads, vocabSize=vocabSize, embeddingDim=embeddingDim, keyDim=keyDim, valueDim=valueDim, hiddenSize=hiddenSize, numLayers=numLayers, weightLess=weightLess, offset=offset)\n",
    "    y = gpt(inputs)[0]\n",
    "    print(\"y\", y, y.shape)\n",
    "    # for goals[b,i] we want to access the value at y[b,i,goals[b,i]]\n",
    "    # index for goals[b,i] is (b,i,goals[b,i])\n",
    "    # lets create that index lookup\n",
    "    \n",
    "    '''\n",
    "    torch.gather(input, dim, index) does the following\n",
    "    out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "    out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "    out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
    "    \n",
    "    y is [b,L,vocabSize]\n",
    "    goals is [b,L]\n",
    "    we want\n",
    "    out[bi,l] = y[bi,l,goals[bi,l]]\n",
    "    but that doesn't fit the above pattern.\n",
    "    To fix this, we can just do\n",
    "    out[bi,l,k] = y[bi,l,goals[bi,l,k]]\n",
    "    where k is only ever 0\n",
    "    so we need to add that axis to goals\n",
    "    '''\n",
    "    values = torch.gather(y, 2, goals.view((b,L,1)))\n",
    "    \n",
    "    \n",
    "    print(values)\n",
    "    for bi in range(b):\n",
    "        for l in range(L):\n",
    "            approx_equals(y[bi,l,goals[bi,l]],values[bi,l])\n",
    "testCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) tensor([[3, 4, 1],\n",
      "        [1, 0, 2]])\n",
      "Parameter containing:\n",
      "tensor([[ 1.1750,  1.5297, -0.8046,  0.9410, -0.4363],\n",
      "        [-0.6042,  0.6766, -0.9753,  0.7356, -1.0042],\n",
      "        [-1.1727,  0.5897, -0.2052, -2.0167,  0.1935],\n",
      "        [ 0.1514, -0.2831, -0.0529,  1.9061, -0.1231],\n",
      "        [-0.9733,  0.9495,  0.3668,  1.3234,  0.4505],\n",
      "        [-0.0030,  0.6066, -1.3297, -0.1344, -0.0678],\n",
      "        [ 1.6767, -0.2011,  0.4436,  0.5443,  0.8576],\n",
      "        [ 0.8300,  0.7594, -0.9010,  0.4384, -0.2681],\n",
      "        [-0.5617, -1.4651, -1.0412, -0.4914, -0.4653],\n",
      "        [-0.9644,  0.8375,  1.8821, -0.8017,  0.1264]], requires_grad=True)\n",
      "tensor([[[ 0.1514, -0.2831, -0.0529,  1.9061, -0.1231],\n",
      "         [-0.9733,  0.9495,  0.3668,  1.3234,  0.4505],\n",
      "         [-0.6042,  0.6766, -0.9753,  0.7356, -1.0042]],\n",
      "\n",
      "        [[-0.6042,  0.6766, -0.9753,  0.7356, -1.0042],\n",
      "         [ 1.1750,  1.5297, -0.8046,  0.9410, -0.4363],\n",
      "         [-1.1727,  0.5897, -0.2052, -2.0167,  0.1935]]],\n",
      "       grad_fn=<IndexBackward>) torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "def testTransformer6():\n",
    "    b,L,n,d,k,v = 2,3,4,5,6,7\n",
    "    vocabSize = 10\n",
    "    embeddingDim = d\n",
    "    emb = EmbeddingLayer(vocabSize, embeddingDim)\n",
    "    inputs = torch.tensor([[3,4,1],[1,0,2]])\n",
    "    print(inputs.shape, inputs)\n",
    "    print(emb.embeddings)\n",
    "    e = emb(inputs)\n",
    "    print(e, e.shape)\n",
    "testTransformer6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[[[ -3.6704,  -9.3205,   4.9568,   0.7842,   1.8210],\n",
      "          [ -1.8469,  -1.2511,   2.4165,  -8.4936,   2.3370],\n",
      "          [ -3.7076, -11.2480,  -2.2541,   0.1730,   3.9289],\n",
      "          [  4.1189,  12.6030,  -2.1638,   0.1501,  -3.8233]],\n",
      "\n",
      "         [[  1.0092,   3.1251,  -4.8048,  -7.0410,   0.3013],\n",
      "          [  3.1568,   8.8109,   3.1315,   1.8201,  -2.8351],\n",
      "          [  3.8105,   5.1023,  -8.9080,   7.9049,  -3.8480],\n",
      "          [  2.3823,   9.4847,   2.0629,  -4.3741,  -2.1458]],\n",
      "\n",
      "         [[  1.2555,   2.0134,  -4.1270,   4.3320,  -1.9510],\n",
      "          [  2.0929,   5.9423,   2.8129,   3.4904,  -2.7486],\n",
      "          [  3.3003,   4.9170,  -7.2156,   7.2499,  -3.3391],\n",
      "          [  0.2540,   5.4972,   6.9223,  -9.5532,  -0.1014]]],\n",
      "\n",
      "\n",
      "        [[[  1.1796,   5.4335,  -1.8405, -11.9477,   1.3111],\n",
      "          [ -0.0435,  -2.5657,  -1.9573,  -3.5210,  -0.3614],\n",
      "          [  3.9728,   3.3343,  -5.2243,  18.5582,  -5.8535],\n",
      "          [  4.3869,  10.8971,  -0.3297,  12.2588,  -4.5315]],\n",
      "\n",
      "         [[  1.1784,   5.4314,  -1.8388, -11.9490,   1.3120],\n",
      "          [ -1.7864,  -3.5855,   2.2497,  -6.1420,   2.2835],\n",
      "          [  3.1253,   3.1748,  -3.7324,  13.2690,  -4.4635],\n",
      "          [  4.3867,  10.8965,  -0.3298,  12.2585,  -4.5313]],\n",
      "\n",
      "         [[  1.0542,   4.9885,  -1.6194, -11.6354,   1.3204],\n",
      "          [ -0.1740,  -2.6535,  -1.6734,  -3.7043,  -0.1861],\n",
      "          [ -3.1349, -12.2116,   0.3304,   0.2915,   2.1177],\n",
      "          [  4.1344,   9.9346,  -0.7764,  12.0179,  -4.3521]]]],\n",
      "       grad_fn=<ViewBackward>) torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "def testTransformer5():\n",
    "    b,L,n,d,k,v = 2,3,4,5,6,7\n",
    "    attn = MultiHeadSelfAttention(n,d,k,v)\n",
    "    x = torch.normal(0, 1, [b,L,n,d])\n",
    "    y = attn(x)\n",
    "    print(\"y\", y, y.shape)\n",
    "testTransformer5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2370,  0.5129,  0.0639, -3.6355],\n",
      "         [ 0.8274,  1.5479,  0.1948,  0.0440],\n",
      "         [ 0.7549,  0.3560, -0.0806, -0.6300]],\n",
      "\n",
      "        [[ 0.9438,  0.6535,  0.1992, -1.4359],\n",
      "         [ 0.2629,  0.8505,  1.1858,  0.5235],\n",
      "         [ 1.4031,  0.4500,  1.0992, -1.3850]]])\n",
      "tensor([[[ 0.2370,  0.5129,  0.0639, -3.6355],\n",
      "         [ 0.8274,  1.5479,  0.1948,  0.0440],\n",
      "         [   -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "        [[ 0.9438,  0.6535,  0.1992, -1.4359],\n",
      "         [ 0.2629,  0.8505,  1.1858,  0.5235],\n",
      "         [   -inf,    -inf,    -inf,    -inf]]])\n",
      "tensor([[[0.3565, 0.2621, 0.4673, 0.0246],\n",
      "         [0.6435, 0.7379, 0.5327, 0.9754],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6639, 0.4509, 0.2716, 0.1235],\n",
      "         [0.3361, 0.5491, 0.7284, 0.8765],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "tensor([0.2621, 0.7379, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "def testTransformer4():\n",
    "    b,L,n,d,k,v = 2,3,4,5,6,7\n",
    "    inds = torch.tensor(range(L))\n",
    "    sm = torch.nn.Softmax(dim=1)\n",
    "    a = torch.normal(0, 1, [b,L,n])\n",
    "    print(a)\n",
    "    a[:,inds>1,:] = np.NINF\n",
    "    print(a)\n",
    "    print(sm(a))\n",
    "    print(sm(a)[0,:,1])\n",
    "    \n",
    "testTransformer4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5664, -0.5493,  1.2395,  0.8272],\n",
      "         [ 0.5664, -0.5493,  1.2395,  0.8272],\n",
      "         [ 0.5664, -0.5493,  1.2395,  0.8272]],\n",
      "\n",
      "        [[ 1.1796,  1.9616, -0.0884,  1.1684],\n",
      "         [ 1.1796,  1.9616, -0.0884,  1.1684],\n",
      "         [ 1.1796,  1.9616, -0.0884,  1.1684]]]) torch.Size([2, 3, 4]) (4, 0, 1)  0.5664487481117249\n",
      " -0.5492674112319946\n",
      " 1.2395411729812622\n",
      " 0.8271635174751282\n",
      " 1.1796386241912842\n",
      " 1.9616155624389648\n",
      " -0.0884392186999321\n",
      " 1.1684051752090454\n",
      "[torch.FloatStorage of size 8]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "normal(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d0854e941412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtestTransformer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-d0854e941412>\u001b[0m in \u001b[0;36mtestTransformer3\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: normal(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 1"
     ]
    }
   ],
   "source": [
    "def testTransformer3():\n",
    "    b, n, d, k, m = 2,3,4,5,6\n",
    "    set_seed(27)\n",
    "    a = torch.normal(0, 1, [b,d])\n",
    "    b = a.view((b,1,d)).expand((b,n,d))\n",
    "    print(b, b.shape, b.stride(), b.storage())\n",
    "    x = torch.normal(0, 1, [b,n,d])\n",
    "    \n",
    "    \n",
    "testTransformer3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1, 2, 1])\n",
      "torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "def testEmbeddings():\n",
    "    nWords, embeddingDim = 3, 4\n",
    "    embedding = EmbeddingLayer(nWords, embeddingDim)\n",
    "    inputs = torch.tensor([[0,2,1, 2, 1]]).reshape(5)\n",
    "    print(inputs)\n",
    "    print(embedding(inputs).shape)\n",
    "testEmbeddings()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qprs tensor([[[0.7313, 0.1338, 0.1350],\n",
      "         [0.1290, 0.5566, 0.3144],\n",
      "         [0.2761, 0.6204, 0.1035]],\n",
      "\n",
      "        [[0.1808, 0.0482, 0.7710],\n",
      "         [0.1420, 0.2864, 0.5716],\n",
      "         [0.5909, 0.2983, 0.1109]]])\n",
      "vh tensor([[[ 0.4601,  0.3644, -1.4775,  0.4753, -0.3383],\n",
      "         [-0.5367,  1.5008, -0.7286,  0.4594,  0.4356],\n",
      "         [-0.2073, -1.0252, -1.1372,  1.0307,  0.4656]],\n",
      "\n",
      "        [[-0.8964,  0.5814, -0.9950, -0.9881, -0.1613],\n",
      "         [ 0.1007,  0.9505,  0.9992, -0.8928,  1.6873],\n",
      "         [ 0.4901,  0.2179,  0.0329,  0.0506, -0.0541]]])\n",
      "summedRows tensor([[[ 0.2367,  0.3289, -1.3314,  0.5481, -0.1263],\n",
      "         [-0.3046,  0.5601, -0.9537,  0.6411,  0.3452],\n",
      "         [-0.2274,  0.9256, -0.9777,  0.5229,  0.2250]],\n",
      "\n",
      "        [[ 0.2207,  0.3189, -0.1064, -0.1826,  0.0104],\n",
      "         [ 0.1817,  0.4793,  0.1637, -0.3671,  0.4293],\n",
      "         [-0.4452,  0.6512, -0.2862, -0.8445,  0.4019]]])\n",
      "tensor(-0.9102) tensor(-0.9102)\n"
     ]
    }
   ],
   "source": [
    "def testTransformer2():\n",
    "    b, n, d, k = 2, 3, 4, 5\n",
    "    set_seed(27)\n",
    "    dotQueryKey = torch.normal(0, 1, [b,n,n])\n",
    "    softmax = torch.nn.Softmax(dim=2)\n",
    "    queryPrs = softmax(dotQueryKey)\n",
    "    vh = torch.normal(0, 1, [b,n,k])\n",
    "    print(\"qprs\", queryPrs)\n",
    "    print(\"vh\", vh)\n",
    "    # the ith output is taking sum over j of (queryPrs[b,i,j])*(vh[b,j])\n",
    "    #                            scalar            vector\n",
    "    # queryPrs is [b,n,n]\n",
    "    # vh       is [b,n,k]\n",
    "    # so         queryPrs[b,i] is of dim n\n",
    "    #            vh[b]         is of dim [n,k]\n",
    "    # so         j ranges from 0 to n-1\n",
    "    # fixing b and i and thinking of this as a small matrix, we do\n",
    "    # queryPrs = [0.4, 0.6] (n of these)\n",
    "    #              \n",
    "    # vh       = [1.2,     3.4,     5.2] (each row is of length k)\n",
    "    #          = [3.4,     2.3,     1.1] (there are n rows)\n",
    "    # we do\n",
    "    #            [0.4*1.2, 0.4*3.4, 0.4*5.2 ]\n",
    "    #            [0.6*3.4, 0.6*2.3, 0.6*1.1 ]\n",
    "    # and then we sum them:\n",
    "    #            [0.4*1.2+0.6*3.4, 0.4*3.4+0.6*2.3, 0.4*5.2+0.6*1.1]\n",
    "    # In other words, for a given i we dot the ith row of queryPrs[b] (dim n) by each column in vh[b] (vh[b] is [n,k], so each column is dim n)\n",
    "    # Thus, the output's [b,i,j] value is the ith row of queryPrs[b] dot the jth column of vh[b] \n",
    "    # for regular matrix multiplication of A and B, the [i,j]th value is ith row of A dot jth column of B, so this is just regular matrix multiplication.\n",
    "    # In einsum: torch.einsum(\"bij,bjk->bik\")\n",
    "    # which means that our output[b,i] is\n",
    "    summedRows = torch.einsum(\"bij,bjk->bik\", queryPrs, vh)\n",
    "    for bi in range(b):\n",
    "        approx_equals(summedRows[bi, 0,0], queryPrs[bi, 0]@vh[bi, :,0])\n",
    "        approx_equals(summedRows[bi, 0,1], queryPrs[bi, 0]@vh[bi, :,1])\n",
    "        approx_equals(summedRows[bi, 1,0], queryPrs[bi, 1]@vh[bi, :,0])\n",
    "        approx_equals(summedRows[bi, 1,1], queryPrs[bi, 1]@vh[bi, :,1])\n",
    "    print(\"summedRows\", summedRows)\n",
    "    \n",
    "    # we are currently [b,n,k],\n",
    "    # now we need to project res back to a [b,n,d] size\n",
    "    Wch = torch.normal(0, 1, [d,k])\n",
    "    \n",
    "    res = torch.einsum(\"dk,bnk->bnd\", Wch, summedRows)\n",
    "    for bi in range(b):\n",
    "        approx_equals(res[bi,0,0], Wch[0]@summedRows[bi, 0,0])\n",
    "        approx_equals(res[bi,0,1], Wch[0]@summedRows[bi, 0,0])\n",
    "        approx_equals(res[bi,0,0], Wch[0]@summedRows[bi, 0,0])\n",
    "        approx_equals(res[bi,0,0], Wch[0]@summedRows[bi, 0,0])\n",
    "    print(res[0,0,1], Wch[1]@summedRows[0,0])\n",
    "    \n",
    "    a = torch.normal(0, 1, [2, 3])\n",
    "    b = torch.normal(0, 1, [2, 3, 4])\n",
    "    \n",
    "testTransformer2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[ 1.7650,  0.0664, -0.0706, -0.1672],\n",
      "         [-0.4266,  1.5005, -0.2636, -1.0210],\n",
      "         [-1.7975, -0.3770,  0.6140,  0.5948]],\n",
      "\n",
      "        [[-0.8629, -0.9511, -0.9195, -0.7592],\n",
      "         [ 0.3197, -0.6699,  1.5661,  0.8074],\n",
      "         [-1.6036,  0.1696, -0.0308,  0.0434]]])\n",
      "Q: tensor([[ 1.5008, -0.7286, -0.5098,  0.4431],\n",
      "        [-0.9389,  1.5772,  1.6559, -0.4713],\n",
      "        [ 0.4656, -0.8964,  0.5814, -0.9950],\n",
      "        [ 0.6763,  0.1337,  0.0659,  0.5385],\n",
      "        [ 0.9992, -0.8928,  1.6873,  0.4901]])\n",
      "q: tensor([[[ 2.5625, -1.5905,  0.8876,  1.1078,  1.5033],\n",
      "         [-2.0517,  2.8120, -0.6810, -0.6551, -2.7111],\n",
      "         [-2.4725,  1.8294, -0.7338, -0.9053, -0.1320]],\n",
      "\n",
      "        [[-0.4698, -1.8548,  0.6716, -1.1802, -1.9367],\n",
      "         [ 0.5273,  0.8561,  0.8565,  0.6647,  3.9557],\n",
      "         [-2.4954,  1.7016, -0.9597, -1.0404, -1.7845]]])\n",
      "k: tensor([[[ 2.5625, -1.5905,  0.8876,  1.1078,  1.5033],\n",
      "         [-2.0517,  2.8120, -0.6810, -0.6551, -2.7111],\n",
      "         [-2.4725,  1.8294, -0.7338, -0.9053, -0.1320]],\n",
      "\n",
      "        [[-0.4698, -1.8548,  0.6716, -1.1802, -1.9367],\n",
      "         [ 0.5273,  0.8561,  0.8565,  0.6647,  3.9557],\n",
      "         [-2.4954,  1.7016, -0.9597, -1.0404, -1.7845]]])\n",
      "dq torch.Size([2, 3, 3]) tensor([[[ 13.3707, -15.1356, -11.0980],\n",
      "         [-15.1356,  20.3600,  11.6679],\n",
      "         [-11.0980,  11.6679,  10.8354]],\n",
      "\n",
      "        [[  9.2554,  -9.7056,   2.0555],\n",
      "         [ -9.7056,  17.8339,  -8.4316],\n",
      "         [  2.0555,  -8.4316,  14.3104]]])\n"
     ]
    }
   ],
   "source": [
    "def testTransformer():\n",
    "    b, n, d, k = 2, 3, 4, 5\n",
    "    from minGPT.mingpt.utils import set_seed\n",
    "    set_seed(27)\n",
    "    x = torch.normal(0, 1, [b,n,d])\n",
    "    print(\"x:\", x)\n",
    "    x\n",
    "    Q = torch.normal(0, 1, [k,d])\n",
    "    K = torch.normal(0, 1, [k,d])\n",
    "    V = torch.normal(0, 1, [k,d])\n",
    "    # In other words\n",
    "    print(\"Q:\", Q)\n",
    "    res = torch.einsum('kd,bnd->bnk', Q, x)\n",
    "    # Check that it's the same both ways\n",
    "    approx_equals(Q@(x[0,0]), res[0,0])\n",
    "    approx_equals(Q@(x[1,0]), res[1,0])\n",
    "    approx_equals(Q@(x[0,1]), res[0,1])\n",
    "    approx_equals(Q@(x[1,1]), res[1,1])\n",
    "    \n",
    "    q = torch.einsum(\"kd,bnd->bnk\", Q, x)\n",
    "    k = torch.einsum(\"kd,bnd->bnk\", Q, x)\n",
    "    v = torch.einsum(\"kd,bnd->bnk\", Q, x)\n",
    "    \n",
    "    print(\"q:\", q)\n",
    "    print(\"k:\", k)\n",
    "    dotQueryKey = torch.einsum(\"bij, bkj->bik\", q, k)\n",
    "    print(\"dq\", dotQueryKey.shape, dotQueryKey)\n",
    "    \n",
    "    # dotQueryKey[b,i,j] is q[b,i] dot k[b,j]\n",
    "    for ba in range(b):\n",
    "        approx_equals(q[ba,0]@k[ba,0], dotQueryKey[ba,0,0])\n",
    "        approx_equals(q[ba,0]@k[ba,1], dotQueryKey[ba,0,1])\n",
    "        approx_equals(q[ba,1]@k[ba,0], dotQueryKey[ba,1,0])\n",
    "        approx_equals(q[ba,1]@k[ba,1], dotQueryKey[ba,1,1])\n",
    "    \n",
    "testTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([[ 1.7650,  0.0664, -0.0706, -0.1672,  0.0756],\n",
      "        [-0.4957, -0.8165, -0.0069, -1.7975, -0.3770],\n",
      "        [ 0.6140,  0.5948, -0.1926,  0.5088,  1.2001],\n",
      "        [ 1.0033,  0.3197, -0.6699,  1.5661,  0.8074]])\n",
      "b tensor([[-1.4775,  0.4753, -0.3383, -0.5367],\n",
      "        [-0.8237, -0.4236,  0.3272, -1.9896],\n",
      "        [-0.9389,  1.5772,  1.6559, -0.4713],\n",
      "        [ 0.2374, -0.1400, -1.0862,  0.5188],\n",
      "        [ 0.6763,  0.1337,  0.0659,  0.5385]])\n",
      "tensor(-2.5848)\n",
      "later stuff\n",
      "tensor(-2.5248)\n",
      "tensor(-0.3093)\n",
      "tensor(1.6219)\n",
      "tensor(1.0495)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5248, -0.3093,  2.8159,  0.9808],\n",
       "        [ 1.6219,  1.0495,  0.2236, -1.1318],\n",
       "        [-1.8210,  1.7328, -0.6841,  1.3748],\n",
       "        [-2.6093,  0.8155,  0.2554,  1.1852]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(27)\n",
    "a = torch.normal(0, 1, [4, 5])\n",
    "b = torch.normal(0, 1, [5, 4])\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(a[0]@b[:,0])\n",
    "torch.einsum(\"ij,jk->ik\", a, b)\n",
    "set_seed(27)\n",
    "print(\"later stuff\")\n",
    "a = torch.normal(0, 1, [4, 5])\n",
    "b = torch.normal(0, 1, [4, 5])\n",
    "print(a[0]@b[0])\n",
    "print(a[0]@b[1])\n",
    "print(a[1]@b[0])\n",
    "print(a[1]@b[1])\n",
    "# we want to go from a [nxm],[nxm] to a [n,n]\n",
    "torch.einsum(\"ij,kj->ik\", a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.7650,  0.0664, -0.0706, -0.1672],\n",
      "         [-0.4266,  1.5005, -0.2636, -1.0210],\n",
      "         [-1.7975, -0.3770,  0.6140,  0.5948]],\n",
      "\n",
      "        [[-0.8629, -0.9511, -0.9195, -0.7592],\n",
      "         [ 0.3197, -0.6699,  1.5661,  0.8074],\n",
      "         [-1.6036,  0.1696, -0.0308,  0.0434]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(27)\n",
    "a = torch.normal(0, 1, [2, 3, 4])\n",
    "print(a)\n",
    "\n",
    "softmax(a)[0,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = LayerNorm(10)\n",
    "inputs = torch.normal(0, 1, [30, 10])\n",
    "ayy = bn(inputs)\n",
    "print(ayy.mean(), ayy.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
