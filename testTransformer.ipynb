{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "import numpy as np\n",
    "def approx_equals(a, b):\n",
    "    assert torch.allclose(a, b, 0.0001), str(a) + \"!=\" + str(b)\n",
    "\n",
    "class HelpfulModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._myHyperParams = {}\n",
    "        \n",
    "    def __setattr__(self, attr, val):\n",
    "        super().__setattr__(attr, val) # make sure to call super because torch.nn.Module also overrides this\n",
    "        simpleTypes = [int, str, float]\n",
    "        if type(val) in simpleTypes or (type(val) is list and (len(val) == 0 or type(val[0]) in simpleTypes)):\n",
    "            self._myHyperParams[attr] = val\n",
    "            \n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return \", \".join([(str(param) + \": \" + str(val)) for param, val in self._myHyperParams.items()])\n",
    "    \n",
    "\n",
    "\n",
    "class SoftRELULayer(HelpfulModule):\n",
    "    def __init__(self, weightLess, offset, maxMag=4.0,**kwargs):\n",
    "        super().__init__()\n",
    "        self.weightLess = weightLess\n",
    "        self.offset = offset\n",
    "        self.maxMag = maxMag\n",
    "    \n",
    "    def forward(self, x):\n",
    "        biggerThan = torch.max(torch.tensor([0.0]).to(device=x.device), x)\n",
    "        lessThan = torch.min(torch.tensor([0.0]).to(device=x.device), x)\n",
    "        res = biggerThan + lessThan*self.weightLess - self.offset\n",
    "        if self.maxMag is not None:\n",
    "            res.clamp_max_(self.maxMag)\n",
    "        return res\n",
    "    \n",
    "\n",
    "# TODO: see if batch norm works for transformers\n",
    "\n",
    "class AdaptableBatchedCrossEntropyLoss(HelpfulModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batchedPrLoss = BatchedCrossEntropyLoss()\n",
    "        self.batchedIndexLoss = BatchedIndexCrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, y, targets, rollupLosses=True):\n",
    "        if targets.dtype == torch.int64: # fitting to desired word indices\n",
    "            if len(targets.shape) == 1: # if single batch, expand out\n",
    "                targets = targets.view((1, targets.shape[0]))\n",
    "            loss = self.batchedIndexLoss(y, targets, rollupLosses=rollupLosses)\n",
    "        else: # fitting to word prs\n",
    "            if len(targets.shape) == 2: # if single batch, expand out\n",
    "                targets = targets.view((1, targets.shape[0], targets.shape[1]))\n",
    "            loss = self.batchedPrLoss(y, targets, rollupLosses=rollupLosses)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "class BatchedIndexCrossEntropyLoss(HelpfulModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y, target, rollupLosses=True):\n",
    "        '''\n",
    "        torch.gather(input, dim, index) does the following\n",
    "        out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "        out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "        out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
    "\n",
    "        y is [b,L,vocabSize]\n",
    "        goals is [b,L]\n",
    "        we want\n",
    "        out[bi,l] = y[bi,l,goals[bi,l]]\n",
    "        but that doesn't fit the above pattern.\n",
    "        To fix this, we can just do\n",
    "        out[bi,l,k] = y[bi,l,goals[bi,l,k]]\n",
    "        where k is only ever 0\n",
    "        so we need to add that axis to goals\n",
    "        '''\n",
    "        b,L = target.shape\n",
    "        values = torch.gather(y, 2, target.view((b,L,1)))\n",
    "        # Now make it look like b,L\n",
    "        values = values.view((b,L))\n",
    "        # Actual pr for those values is 1.0, so\n",
    "        # -target*x.log()-(1.0-target)*(1.0-x).log()\n",
    "        # turns into\n",
    "        res = -values.clamp(0.00001, 0.99999).log()\n",
    "        # this gives us one loss per (batch, word), usually they just want a single loss value, so this can roll them up if you want\n",
    "        if rollupLosses: return res.mean()\n",
    "        else: return res\n",
    "\n",
    "class BatchedCrossEntropyLoss(HelpfulModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y, target, rollupLosses=True):\n",
    "        vals = -target*y.log()-(1.0-target)*(1.0-y).log()\n",
    "        # sum along not batch axis\n",
    "        res = vals.sum(axis=2)\n",
    "        if rollupLosses: return res.mean()\n",
    "        else: return res\n",
    "        # -target[i]*log(x[i])-(1-target[i])*log(1-x[i])\n",
    "\n",
    "class LayerNorm(HelpfulModule):\n",
    "    def __init__(self, eps=0.01):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.multiplicitiveWeight = nn.Parameter(torch.tensor(1.0))\n",
    "        self.additiveWeight = nn.Parameter(torch.tensor(0.0))\n",
    "        self.nBatches = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu = x.mean((1,2,3), keepdim=True)\n",
    "        var = x.var((1,2,3), keepdim=True) # TODO: add correction based on batch size\n",
    "        normalizedOutput = (x-mu)/torch.max(var, torch.tensor(self.eps).to(device=x.device))\n",
    "        return normalizedOutput*self.multiplicitiveWeight+self.additiveWeight\n",
    "\n",
    "class SequentialDenseLayer(HelpfulModule):\n",
    "    def __init__(self, name, inputDim, hiddenDim, outputDim, nLayers, act, einsumStr=None):\n",
    "        super().__init__()\n",
    "        self.name, self.inputDim, self.hiddenDim, self.outputDim, self.nLayers, self.act = name, inputDim, hiddenDim, outputDim, nLayers, act\n",
    "        self.einsumStr = einsumStr\n",
    "        projectInto = DenseLayer(name + \"_\" + \"project\", inputDim, hiddenDim,einsumStr=einsumStr)\n",
    "        projectOut = DenseLayer(name + \"_\" + \"projectOut\", hiddenDim, outputDim,einsumStr=einsumStr)\n",
    "        allLayers = [projectInto] + [DenseLayer(name + \"_\" + str(i), inputDim=hiddenDim, outputDim=hiddenDim, act=act,einsumStr=einsumStr) for i in range(nLayers)] + [projectOut]\n",
    "        self.layers = nn.Sequential(*allLayers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class DenseLayer(HelpfulModule):\n",
    "    def __init__(self, name, inputDim, outputDim, act=None, einsumStr=None):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.inputDim, self.outputDim = inputDim, outputDim\n",
    "        self.weight = nn.Parameter(torch.normal(0, 1.0/math.sqrt(inputDim), [inputDim, outputDim])) # this is because dotting two vectors of mean zero std 1.0 gets output of mean zero std sqrt(inputDim), so we multiply to fix that\n",
    "        self.bias = nn.Parameter(torch.normal(0, 1.0, [outputDim]))\n",
    "        self.act = act\n",
    "        self.einsumStr = einsumStr\n",
    "    \n",
    "    def forward(self, x, einsumStr=None):\n",
    "        if einsumStr is None: einsumStr = self.einsumStr\n",
    "        #print(self.name, \"x\", x, \"weights\", self.weights, \"biases\", self.biases)\n",
    "        if einsumStr is None:\n",
    "            res = (x@self.weight + self.bias) \n",
    "        else:\n",
    "            res = (torch.einsum(einsumStr, x, self.weight)+self.bias)\n",
    "        res.div_(math.sqrt(2.0)) # adding two things of mean 0 std 1 requires dividing by math.sqrt(2.0) to make output mean 0.0 std 1.0\n",
    "        if self.act is None:\n",
    "            return res\n",
    "        else:\n",
    "            return self.act(res)\n",
    "    \n",
    "class EmbeddingLayer(HelpfulModule):\n",
    "    def __init__(self, vocabSize, embeddingDim):\n",
    "        super().__init__()\n",
    "        self.vocabSize, self.embeddingDim = vocabSize, embeddingDim\n",
    "        # Todo: what is good initialization for embeddings?\n",
    "        self.embeddings = nn.Parameter(torch.normal(0, 1, [vocabSize, embeddingDim]))\n",
    "    # Inputs should be dimension [batchSize] and they should be integers\n",
    "    def forward(self, x):\n",
    "        return self.embeddings[x]\n",
    "    \n",
    "class Transformer(HelpfulModule):\n",
    "    def __init__(self, numHeads, vocabSize, embeddingDim, posEmbeddingDim, keyDim, valueDim, hiddenSize, numLayers, **kwargs):\n",
    "        super().__init__()\n",
    "        self.numHeads, self.vocabSize, self.embeddingDim, self.posEmbeddingDim, self.keyDim, self.valueDim, self.hiddenSize, self.numLayers = numHeads, vocabSize, embeddingDim, posEmbeddingDim, keyDim, valueDim, hiddenSize, numLayers\n",
    "        n, k, v, m = numHeads, keyDim, valueDim, hiddenSize\n",
    "        d = embeddingDim+posEmbeddingDim\n",
    "        self.n, self.d, self.k, self.v, self.m = n,d,k,v,m\n",
    "        self.embedding = EmbeddingLayer(vocabSize, embeddingDim)\n",
    "        max_seq_len = 10000\n",
    "        self.posEmbeddings = nn.Parameter(torch.normal(0, 1, [max_seq_len, posEmbeddingDim]))\n",
    "        self.encodingLayers = nn.Sequential(*[TransformerBlock(n,d,k,v,m,layerNum=i,**kwargs) for i in range(numLayers)])\n",
    "        self.finalProjection = DenseLayer(\"FinalProj\", n*d, vocabSize)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.lossFunc = AdaptableBatchedCrossEntropyLoss()\n",
    "        # TODO: positional encodings\n",
    "    \n",
    "    def configure_optimizers(self, config):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, targets=None, rollupLosses=True):\n",
    "        # x is of size [b,L], word integer indices\n",
    "        if len(x.shape) == 1: # make everythingn work for batch size 1\n",
    "            x = x.view((1,x.shape[0]))\n",
    "        b, L = x.shape\n",
    "        \n",
    "        n,d = self.n, self.d\n",
    "        \n",
    "        # embeddings need to go from [b,L,embeddingDim] to [b,L,n,embeddingDim]\n",
    "        embs = self.embedding(x).view((b,L,1,self.embeddingDim)).expand((b, L, n, self.embeddingDim))\n",
    "        # positional embeddings are the same for every batch, so they need to go from [L,embeddingDim] to [b,L,n,embeddingDim]\n",
    "        posEmbs = self.posEmbeddings[torch.arange(L)].view((1,L,1,posEmbeddingDim)).expand((b,L,n,posEmbeddingDim))\n",
    "        embeddings = torch.cat([embs, posEmbs], axis=3)\n",
    "        # now it's ready to go through the embeddings\n",
    "        forwardPass = self.encodingLayers(embeddings)\n",
    "        # It's currently dim [b,L,n,d], we need to make it [b,L,vocabSize]\n",
    "        # For now I will just flatten and then project, so first make it [b,L,n*d]\n",
    "        flattenedOutputs = forwardPass.reshape((b,L,n*d))\n",
    "        # project to [b,L,vocabSize]\n",
    "        # this dots rows of dim n*d and columsn of dim n*d, so we need to divide by sqrt(n*d) to make mean 0 std 1\n",
    "        finalProj = self.finalProjection(flattenedOutputs, \"bli,iv->blv\")\n",
    "        # Use softmax to convert to prs\n",
    "        wordPrs = self.softmax(finalProj)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = self.lossFunc(wordPrs, targets, rollupLosses=rollupLosses)\n",
    "        \n",
    "        return wordPrs, loss\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                no_decay.add(fpn)\n",
    "                continue\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "        \n",
    "\n",
    "\n",
    "# Transformer variant where it creates a list of weights of dim contextSize, then uses those to average over keys made by the last contextSize tokens\n",
    "# each attention layer goes [b,L,n,d] -> [b,L-contextSize,n,d], which means that you need to input at least L>=contextSize*nlayers\n",
    "# complexity is L*contextSize, this removes the need for any positional embeddings, and allows information to trickle a distance of contextSize*nLayers (possibly further with some later strats I have where you take the layers and output from the current layer)\n",
    "class LookAroundTransformer(HelpfulModule):\n",
    "    def __init__(self, config, numHeads, vocabSize, embeddingDim, nLayers, hiddenDimBefore, lookaroundDim, hiddenDimAfter, nLayersBefore, nLayersAfter, contextSize, **kwargs):\n",
    "        super().__init__()\n",
    "        self.numHeads, self.vocabSize, self.embeddingDim = numHeads, vocabSize, embeddingDim\n",
    "        self.n, self.d, self.nLayers = numHeads,embeddingDim,nLayers\n",
    "        def makeLayer(i):\n",
    "            return LookAroundAttention(config=config, n=self.n, d=self.d, hiddenDimBefore=hiddenDimBefore, lookaroundDim=lookaroundDim,hiddenDimAfter=hiddenDimAfter,nLayersBefore=nLayersBefore,nLayersAfter=nLayersAfter,contextSize=contextSize,layerNum=i,**kwargs)\n",
    "        self.attentionLayers = torch.nn.Sequential(*[makeLayer(i) for i in range(nLayers)])\n",
    "        self.embedding = EmbeddingLayer(vocabSize=vocabSize, embeddingDim=embeddingDim) # TODO: Figure out how to do embeddings with any size of vocab\n",
    "        self.finalProj = DenseLayer(\"finalProj\", self.d*self.n, self.vocabSize)\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "        self.lossFunc = AdaptableBatchedCrossEntropyLoss()\n",
    "        self.embedDropout = nn.Dropout(config.embd_pdrop)\n",
    "        self.config = config\n",
    "    \n",
    "    def configure_optimizers(self, train_config):\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        no_decay = set()\n",
    "        decay = set()\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight'):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                else:\n",
    "                    no_decay.add(fpn) # embeddings should not be decayed\n",
    "        \n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        b,L = x.shape\n",
    "        n,d = self.n, self.d\n",
    "        embeddings = self.embedding(x) # [b,L] -> [b,L,d]\n",
    "        if self.config.train:\n",
    "            embeddings = self.embedDropout(embeddings)\n",
    "        inputsToAttention = embeddings.view((b,L,1,d)).expand((b,L,n,d)) # make it look like [b,L,n,d] for the attention heads\n",
    "        attentionOutputs = self.attentionLayers(inputsToAttention) # output will be [b,L-contextSize*nLayers,n,d]\n",
    "        b,Lnew,n,d = attentionOutputs.shape # Lnew = L - contextSize*nLayers\n",
    "        flattenedOutputs = attentionOutputs.reshape((b,Lnew,n*d))\n",
    "        wordPrs = self.softmax(self.finalProj(flattenedOutputs, \"blk,kv->blv\")) # [b,Lnew,n*d]x[n*d,vocabSize] -> [b,Lnew,vocabSize]\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            remainingTargets = targets[:,-Lnew:] # we can only measure the ones that had sufficient context\n",
    "            loss = self.lossFunc(wordPrs, remainingTargets)\n",
    "        return wordPrs, loss\n",
    "        \n",
    "\n",
    "\n",
    "class LookAroundAttention(HelpfulModule):\n",
    "    def __init__(self, config, n, d, hiddenDimBefore, lookaroundDim, hiddenDimAfter, nLayersBefore, nLayersAfter, contextSize, layerNum=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.act = SoftRELULayer(**kwargs)\n",
    "        self.n, self.d, self.hiddenDimBefore, self.lookaroundDim, self.hiddenDimAfter, self.nLayersBefore, self.nLayersAfter, self.contextSize, self.layerNum = n, d, hiddenDimBefore, lookaroundDim, hiddenDimAfter, nLayersBefore, nLayersAfter, contextSize, layerNum\n",
    "        self.projectToLookaround = SequentialDenseLayer(\"projectToLookaround_\" + str(layerNum), inputDim=d, hiddenDim=hiddenDimBefore, outputDim=hiddenDimBefore, nLayers=nLayersBefore, act=self.act, einsumStr=\"blnd,dh->blnh\")\n",
    "        self.projectToLookaroundKey = DenseLayer(\"projectToLookaroundKey_\" + str(layerNum), hiddenDimBefore, lookaroundDim, act=self.act)\n",
    "        self.projectToLookaroundWeights = DenseLayer(\"projectToLookaroundWeights_\" + str(layerNum), hiddenDimBefore, contextSize, act=self.act)\n",
    "        self.projectFromLookaround = SequentialDenseLayer(\"projectAfterLookaround_\" + str(layerNum), inputDim=lookaroundDim, hiddenDim=hiddenDimAfter, outputDim=d, nLayers=nLayersAfter, act=self.act, einsumStr=\"blnv,vd->blnd\")\n",
    "        self.softmax = torch.nn.Softmax(dim=3)\n",
    "        self.attentionDropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.config = config\n",
    "    \n",
    "    def forward(self, x, intermediateResults=False, efficientMethod=True):\n",
    "        # x is [b,L,n,d], output is [b,L-contextSize,n,d]\n",
    "        b,L,n,d = x.shape\n",
    "        LAfter = L-self.contextSize+1\n",
    "        # we are going to output [b,L-contextSize,n,d]\n",
    "        # first, project to lookaround vector. Each vector of size d is dotted with something of size d to be turned into something of size k\n",
    "        lookaroundVector = self.projectToLookaround(x) # [b,L,n,d] -> [b,L,n,hiddenDimBefore] # \"blnd,dh->blnh\"\n",
    "        lookaroundKeys = self.projectToLookaroundKey(lookaroundVector, \"blnh,hv->blnv\") # [b,L,n,hiddenDimBefore] -> [b,L,n,lookaroundDim]\n",
    "        # we can only apply lookbehind to things that have contextSize or more things to look at (we start from -LAfter which has a + 1 because one of the things they look at is themselves\n",
    "        lookaroundWeights = self.projectToLookaroundWeights(lookaroundVector[:,-LAfter:], \"blnh,hw->blnw\") # [b,LAfter,n,hiddenDimBefore] -> [b,L-contextSize,n,contextSize]\n",
    "        # apply softmax so they become weights from 0 to 1 that sum to 1\n",
    "        actualWeights = self.softmax(lookaroundWeights) # [b,L-contextSize,n,contextSize]\n",
    "        lookaroundValues = []\n",
    "        # to apply this in one large matrix, we need to somehow do:\n",
    "        # lookaroundKeys[:,i:i+self.contextSize]\n",
    "        # apply lookaround\n",
    "        \n",
    "        if efficientMethod:\n",
    "            sampleIndices = torch.stack([torch.arange(i,i+self.contextSize) for i in range(LAfter)]).long().to(x.device)\n",
    "            wordContexts = lookaroundKeys[:,sampleIndices] # [b,LAfter,contextSize,n,d]\n",
    "            lookaroundValue = torch.einsum(\"blcnd,blnc->blnd\", wordContexts, actualWeights) # [b,LAfter,contextSize,n,lookaroundDim]x[b,LAfter,n,contextSize]->[b,LAfter,n,lookaroundDim]\n",
    "            print(\"lookaroundValue1\", lookaroundValue, lookaroundValue.shape)\n",
    "        else:\n",
    "            for i in range(LAfter):\n",
    "                wordWeights = actualWeights[:,i] # [b,n,contextSize]\n",
    "                # we need to take [b,L,n,lookaroundDim] and get the relevant vectors we will be using\n",
    "                context = lookaroundKeys[:,i:i+self.contextSize] # [b,contextSize,n,lookaroundDim]\n",
    "                # dot the weights along the axis: this does a weighted sum of context vectors\n",
    "                lookaroundValues.append(torch.einsum(\"bcnd,bnc->bnd\", context, wordWeights).view((b,1,n,self.lookaroundDim))) # [b,contextSize,n,lookaroundDim] x [b,n,contextSize] -> [b,n,lookaroundDim]\n",
    "            # stack all the vectors we found\n",
    "            lookaroundValue = torch.cat(lookaroundValues, dim=1) # [b,L-contextSize,n,lookaroundDim]\n",
    "            print(\"lookaroundValue2\", lookaroundValue, lookaroundValue.shape)\n",
    "        res = self.projectFromLookaround(lookaroundValue) # \"blnv,vd->blnd\"\n",
    "        if self.config.train:\n",
    "            res = self.attentionDropout(res)\n",
    "        if intermediateResults:\n",
    "            return lookaroundVector, lookaroundKeys, lookaroundWeights, actualWeights, lookaroundValues, lookaroundValue, res\n",
    "        else:\n",
    "            return res\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "class MultiHeadSelfAttention(HelpfulModule):\n",
    "    def __init__(self, n, d, k, v, layerNum=0, doNormalize=True, efficientMethod=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n, self.d, self.kDim, self.vDim = n,d,k,v\n",
    "        self.doNormalize = doNormalize\n",
    "        self.efficientMethod = efficientMethod\n",
    "        # Todo: compute initialization scaling factors\n",
    "        # TODO: What about more things than just QKV? Like four or five or something\n",
    "        self.Q = DenseLayer(\"Q\" + str(layerNum), d, k)\n",
    "        self.K = DenseLayer(\"K\" + str(layerNum), d, k)\n",
    "        self.V = DenseLayer(\"V\" + str(layerNum), d, v)\n",
    "        self.Wch = DenseLayer(\"Wch\" + str(layerNum), v,d)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.softmaxAlltogether = torch.nn.Softmax(dim=3)\n",
    "    def forward(self, x):\n",
    "        # x is [b,L,n,d]\n",
    "        # b is batch size\n",
    "        # L is sentence length\n",
    "        # n is num heads\n",
    "        # d is embedding dimension\n",
    "        # we need to use Q, K, V to make a qi, ki, vi for each word\n",
    "        # because we dot qi and kj, they need to be same dim, call this k\n",
    "        # vi can be any dim, call this v\n",
    "        # we need [b,L,n,d] -> [b,L,n,k] for qi and ki\n",
    "        # we need [b,L,n,d] -> [b,L,n,v] for vi\n",
    "        # [b,L,n,k]\n",
    "        \n",
    "        b,L,n,d,kDim,vDim = x.shape[0], x.shape[1], self.n, self.d, self.kDim, self.vDim\n",
    "        \n",
    "        q = self.Q(x, \"blnd,dk->blnk\")\n",
    "        k = self.K(x, \"blnd,dk->blnk\")\n",
    "        v = self.V(x, \"blnd,dv->blnv\")\n",
    "        # each of these dot a row of dim d by a column of dim d, so we need to divide by sqrt(d) to ensure output is mean 0 std 1\n",
    "        #print(\"q\", q[:,0,0,0].mean(), q[:,0,0,0].std())\n",
    "        #print(\"k\", k[:,0,0,0].mean(), k[:,0,0,0].std())\n",
    "        #print(\"v\", v[:,0,0,0].mean(), v[:,0,0,0].std())\n",
    "        # Normally people just do a massive matrix, but that is quadratic in terms of L, and very wasteful with memory\n",
    "        # Instead, we will do a loop over each word and do this for each word.\n",
    "        # It's still quadratic in terms of L for time complexity (and slightly slower than giant matrix, because we are in python), but now linear in terms of space complexity, which is important for GPU space\n",
    "        \n",
    "        # simpler way:\n",
    "        if self.efficientMethod:\n",
    "            # need to dot each pair of q and k\n",
    "\n",
    "            # q[i,j] is a vector of size k\n",
    "            # k[i,j] is a vector of size k\n",
    "            # for every pair of (vector from q, vector from k), we need to get an output by taking their dot product\n",
    "            # normally if you have two matrices A and B of size NxM and MxK,\n",
    "            # when you multiply them, you can think of the output matrix's value in the (i,j)th position as the ith row in A dot jth column in B\n",
    "            # (thus it is every pair of row from first and column from second)\n",
    "            # in einsum, torch.einsum(\"ij,jk->ik\", A, B)\n",
    "            # If instead A and B are of size NxM and NxM and you want to do every pair of rows, you can just do\n",
    "            # torch.einsum(\"ij,kj->ik\") # transpose second matrices indices so it takes rows instead of columns\n",
    "            # we have an additional batch and head index at the front, so include that\n",
    "            # this will output something of dim [b,L,n,L]\n",
    "            # value [i,j,k,l] is batch i, head k, vector j dot with vector l \n",
    "            dotQueryKey = torch.einsum(\"binj, bknj->bink\", q, k)/math.sqrt(kDim)\n",
    "            # we need to softmax along axis 3, this will be [b,L,n,L] -> [n,L]\n",
    "            #print(\"scores 0 fast:\", dotQueryKey[0,0,:,:].shape, dotQueryKey[0,0,:,:])\n",
    "            queryPrs = self.softmaxAlltogether(dotQueryKey)\n",
    "            # now dot query weights with vectors to get [b,L,n,v] \n",
    "            u = torch.einsum(\"binj,bjnk->bink\", queryPrs, v)\n",
    "        else:\n",
    "        \n",
    "\n",
    "            inds = torch.tensor(range(L))\n",
    "            us = []\n",
    "            for i in range(L):\n",
    "                # q is [b,L,n,k]\n",
    "                # expand it so it looks as k so we can do dot product\n",
    "                qi = q[:,i,:,:].view((b,1,n,kDim)).expand((b,L,n,kDim))\n",
    "                # dot product is component wise product and then sum, so just do that\n",
    "                # scores is now [b,L,n]\n",
    "                scores = (qi*k).sum(axis=3) \n",
    "                #print(\"ayy\", scores.shape, (b,L,n))\n",
    "                #if i == 0: print(\"scores 0 slow\", scores[0].shape, scores[0])\n",
    "                #print(\"scores bad\", scores[:,0,0].mean(), scores[:,0,0].std())\n",
    "                if self.doNormalize:\n",
    "                    scores.div_(math.sqrt(kDim+0.0)) # also divide by sqrt(k), this ensures outputs are mean 0 std 1 if values of Q and K are mean 0 std 1\n",
    "                #print(\"scores good\", scores[:,0,0].mean(), scores[:,0,0].std())\n",
    "                scores[:,inds>i,:] = np.NINF # mask out words after current word\n",
    "                scores = self.softmax(scores)\n",
    "                # scores is [b,L,n], we need to make it look like [b,L,n,1] so we can expand it along last axis \n",
    "                scores = scores.view((b,L,n,1)).expand((b,L,n,vDim))\n",
    "                ui = (scores*v).sum(axis=1)\n",
    "                #print((scores*v).shape, (b,L,n,vDim), ui.shape, (b,n,vDim))\n",
    "                #print(\"ui bad\", ui[:,0,0].mean(), ui[:,0,0].std())\n",
    "                # in general, for a weighted sum of uncorrelated variables, we have\n",
    "                # var(sum_i s_i*x_i) = sum_i s_i^2*var(x_i)\n",
    "                # if we assume all x_i are initially std=1.0 (so var(x_i) = 1.0^2=1.0), we get\n",
    "                # var(sum_i s_i*x_i) = sum_i s_i^2\n",
    "                # Since we want var(sum_i s_i*x_i) = 1.0, we need to multiply by a constant, and if we do\n",
    "                # var((sum_i s_i*x_i)/sqrt(sum_i s_i^2))\n",
    "                # = var(sum_i s_i*x_i)/(sum_i s_i^2)\n",
    "                # = (sum_i s_i^2)/(sum_i s_i^2)\n",
    "                # = 1.0\n",
    "                if self.doNormalize:\n",
    "                    ui = ui/(scores.pow(2.0).sum(axis=1).sqrt())\n",
    "                #print(\"ui good\", ui[:,0,0].mean(), ui[:,0,0].std())\n",
    "                us.append(ui.view((b,1,n,vDim)))\n",
    "            u = torch.cat(us, dim=1)\n",
    "        # u is [b,L,n,vDim]\n",
    "        # we want [b,L,n,d]\n",
    "        return self.Wch(u, \"blnv,vd->blnd\") # this computation dots rows of dim v by columns of dim v, so we need to divide by sqrt(v) to ensure output is mean 0 std 1\n",
    "\n",
    "    \n",
    "class TransformerBlock(HelpfulModule):\n",
    "    def __init__(self, n, d, k, v, m, layerNum=0, doNormalize=True, **kwargs):\n",
    "        super().__init__()\n",
    "        # input x is [b,n,d]\n",
    "        # b is batchSize\n",
    "        # n is number of heads\n",
    "        # d is embedding dimension\n",
    "        # k is key size\n",
    "        # m is hidden layer size\n",
    "        self.n, self.d, self.k, self.m = n, d, k, m\n",
    "        self.W1 = DenseLayer(\"W1_\" + str(layerNum), d,m)\n",
    "        self.W2 = DenseLayer(\"W2_\" + str(layerNum), m,d)\n",
    "        self.attention = MultiHeadSelfAttention(n,d,k,v, doNormalize=doNormalize, **kwargs)\n",
    "        self.layerNorm1 = LayerNorm()\n",
    "        self.layerNorm2 = LayerNorm()\n",
    "        self.doNormalize = doNormalize\n",
    "        self.RELU = SoftRELULayer(**kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attentionOut = self.attention(x)\n",
    "        ui = self.layerNorm1(x+attentionOut) # todo: check to see if layer norm inside res net block is doing weird stuff, since we have a second res net thing below not attached\n",
    "        # [d,m]x[b,L,n,d] -> [b,L,n,m]\n",
    "        # this dot products rows of size d by columns of size d, so we need to divide by sqrt(d) to get mean 0 std 1\n",
    "        denseOutput = self.RELU(self.W1(ui, \"blnd,dm->blnm\"))\n",
    "        # this dot products rows of size m by columns of size m, so we need to divide by sqrt(m) to get mean 0 std 1\n",
    "        projectedBack = self.W2(denseOutput, \"blnm,md->blnd\")\n",
    "        return self.layerNorm2(ui+projectedBack)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookaroundValue1 tensor([[[[-0.8479, -0.7582, -0.9203, -0.6178, -1.4249],\n",
      "          [-0.7803, -0.8233, -0.9329, -0.7370, -1.3965],\n",
      "          [-0.7449, -0.7160, -1.0996, -0.7918, -1.5263]],\n",
      "\n",
      "         [[-0.8502, -0.7339, -0.9346, -0.5946, -1.4623],\n",
      "          [-0.8325, -0.7943, -0.8856, -0.6290, -1.4169],\n",
      "          [-0.6652, -0.7322, -1.1881, -0.8940, -1.5554]],\n",
      "\n",
      "         [[-0.8229, -0.7349, -0.9797, -0.6986, -1.4616],\n",
      "          [-0.8065, -0.7488, -0.9821, -0.6997, -1.4666],\n",
      "          [-0.7117, -0.7258, -1.1228, -0.7970, -1.5511]],\n",
      "\n",
      "         [[-0.8419, -0.7280, -0.9622, -0.6783, -1.4551],\n",
      "          [-0.7346, -0.7792, -1.0538, -0.8249, -1.4621],\n",
      "          [-0.7456, -0.7020, -1.1097, -0.7738, -1.5486]],\n",
      "\n",
      "         [[-0.7260, -0.8002, -1.0657, -0.9305, -1.4009],\n",
      "          [-0.6791, -0.8564, -1.0579, -0.9477, -1.3908],\n",
      "          [-0.7210, -0.7351, -1.1259, -0.8716, -1.5012]]]],\n",
      "       grad_fn=<ViewBackward>) torch.Size([1, 5, 3, 5])\n",
      "lookaroundValue2 tensor([[[[-0.8479, -0.7582, -0.9203, -0.6178, -1.4249],\n",
      "          [-0.7803, -0.8233, -0.9329, -0.7370, -1.3965],\n",
      "          [-0.7449, -0.7160, -1.0996, -0.7918, -1.5263]],\n",
      "\n",
      "         [[-0.8502, -0.7339, -0.9346, -0.5946, -1.4623],\n",
      "          [-0.8325, -0.7943, -0.8856, -0.6290, -1.4169],\n",
      "          [-0.6652, -0.7322, -1.1881, -0.8940, -1.5554]],\n",
      "\n",
      "         [[-0.8229, -0.7349, -0.9797, -0.6986, -1.4616],\n",
      "          [-0.8065, -0.7488, -0.9821, -0.6997, -1.4666],\n",
      "          [-0.7117, -0.7258, -1.1228, -0.7970, -1.5511]],\n",
      "\n",
      "         [[-0.8419, -0.7280, -0.9622, -0.6783, -1.4551],\n",
      "          [-0.7346, -0.7792, -1.0538, -0.8249, -1.4621],\n",
      "          [-0.7456, -0.7020, -1.1097, -0.7738, -1.5486]],\n",
      "\n",
      "         [[-0.7260, -0.8002, -1.0657, -0.9305, -1.4009],\n",
      "          [-0.6791, -0.8564, -1.0579, -0.9477, -1.3908],\n",
      "          [-0.7210, -0.7351, -1.1259, -0.8716, -1.5012]]]],\n",
      "       grad_fn=<CatBackward>) torch.Size([1, 5, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def funcsIntoArgs(**kwargs):\n",
    "    return kwargs\n",
    "\n",
    "class dummyConfig(object):\n",
    "    pass\n",
    "def toObject(dictThing):\n",
    "    a = dummyConfig()\n",
    "    for k,v in dictThing.items():\n",
    "        setattr(a, k, v)\n",
    "    return a\n",
    "def lookaroundAttentionTest():\n",
    "    set_seed(44)\n",
    "    n = 3\n",
    "    d = 4\n",
    "    hiddenDimBefore = 4\n",
    "    lookaroundDim = 5\n",
    "    hiddenDimAfter = 6\n",
    "    nLayersBefore = 0\n",
    "    nLayersAfter = 0\n",
    "    contextSize = 2\n",
    "    weightLess, offset, maxMag = 0.5, 0.5, 4.0\n",
    "    config = toObject({\"embd_pdrop\": 0.1, \"attn_pdrop\": 0.1, \"train\": False })\n",
    "    args = funcsIntoArgs(config=config,hiddenDimBefore=hiddenDimBefore,lookaroundDim=lookaroundDim,hiddenDimAfter=hiddenDimAfter,nLayersBefore=nLayersBefore,nLayersAfter=nLayersAfter,contextSize=contextSize,weightLess=weightLess,offset=offset,maxMag=maxMag)\n",
    "    attention = LookAroundAttention(n=n,d=d,**args)\n",
    "    L = 6\n",
    "    b = 1\n",
    "    LAfter = L-contextSize+1\n",
    "    x = torch.normal(0,1,[b,L,n,d])\n",
    "    \n",
    "    y1 = attention(x, efficientMethod=True)\n",
    "    y2 = attention(x, efficientMethod=False)\n",
    "    approx_equals(y1, y2)\n",
    "    '''\n",
    "    samples = torch.stack([torch.arange(i,i+contextSize) for i in range(LAfter)]).long()\n",
    "    print(samples)\n",
    "    print(\"x\", x)\n",
    "    oyy = x[:,samples]\n",
    "    print(\"oyy\", oyy,oyy.shape, (b,LAfter,contextSize,n,d))\n",
    "    W = DenseLayer(\"blah\", d, contextSize)\n",
    "    weights = W(x[:,-LAfter:], \"blnd,dc->blnc\")\n",
    "    sm = torch.nn.Softmax(dim=3)\n",
    "    weights = sm(weights)\n",
    "    print(\"weights\", weights, weights.shape, (b,LAfter, n, contextSize))\n",
    "    \n",
    "    res = torch.einsum(\"blcnd,blnc->blnd\", oyy, weights) # oyy is [b,LAfter,contextSize,n,d], weights is [b,LAfter,n,contextSize]\n",
    "    print(\"res\", res, res.shape, (b,LAfter,n,d))\n",
    "    \n",
    "    # we need to sample x to get something that is [b,L,n,d,contextWidth]\n",
    "    \n",
    "    \n",
    "    lookaroundVector, lookaroundKeys, lookaroundWeights, actualWeights, lookaroundValues, lookaroundValue, y = attention(x, intermediateResults=True)\n",
    "    print(y.shape, (b,L-contextSize,n,d))\n",
    "    nLayers, vocabSize, embeddingDim=2,4,5\n",
    "    transformer = LookAroundTransformer(nLayers=nLayers, numHeads=n, vocabSize=vocabSize, embeddingDim=embeddingDim, **args)\n",
    "    x = torch.tensor([[random.choice(range(vocabSize)) for _ in range(L)] for _ in range(b)]).long()\n",
    "    print(x)\n",
    "    out, loss = transformer(x, targets=x)\n",
    "    print(out.shape, (b,L-contextSize*nLayers, vocabSize), loss)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "lookaroundAttentionTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
